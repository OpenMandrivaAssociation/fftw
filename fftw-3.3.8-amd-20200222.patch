diff -up fftw-3.3.9/api/api.h.2~ fftw-3.3.9/api/api.h
--- fftw-3.3.9/api/api.h.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/api/api.h	2021-05-10 22:57:51.171039968 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -98,11 +99,17 @@ printer *X(mkprinter_cnt)(size_t *cnt);
 printer *X(mkprinter_str)(char *s);
 
 FFTW_EXTERN planner *X(the_planner)(void);
+#ifdef AMD_OPT_PREFER_256BIT_FPU
+FFTW_EXTERN planner *X(the_planner_ex)(int);
+#endif
 void X(configure_planner)(planner *plnr);
 
 void X(mapflags)(planner *, unsigned);
 
 apiplan *X(mkapiplan)(int sign, unsigned flags, problem *prb);
+#ifdef AMD_OPT_PREFER_256BIT_FPU
+apiplan *X(mkapiplan_ex)(int sign, unsigned flags, int n, problem *prb);
+#endif
 
 rdft_kind *X(map_r2r_kind)(int rank, const X(r2r_kind) * kind);
 
diff -up fftw-3.3.9/api/apiplan.c.2~ fftw-3.3.9/api/apiplan.c
--- fftw-3.3.9/api/apiplan.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/api/apiplan.c	2021-05-10 22:57:51.171039968 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -176,6 +177,101 @@ apiplan *X(mkapiplan)(int sign, unsigned
      return p;
 }
 
+#ifdef AMD_OPT_PREFER_256BIT_FPU
+apiplan *X(mkapiplan_ex)(int sign, unsigned flags, int n, problem *prb)
+{
+     apiplan *p = 0;
+     plan *pln;
+     unsigned flags_used_for_planning;
+     planner *plnr;
+     static const unsigned int pats[] = {FFTW_ESTIMATE, FFTW_MEASURE,
+                                         FFTW_PATIENT, FFTW_EXHAUSTIVE};
+     int pat, pat_max;
+     double pcost = 0;
+     
+     if (before_planner_hook)
+          before_planner_hook();
+     
+     plnr = X(the_planner_ex)(n);
+
+     if (flags & FFTW_WISDOM_ONLY) {
+	  /* Special mode that returns a plan only if wisdom is present,
+	     and returns 0 otherwise.  This is now documented in the manual,
+	     as a way to detect whether wisdom is available for a problem. */
+	  flags_used_for_planning = flags;
+	  pln = mkplan0(plnr, flags, prb, 0, WISDOM_ONLY);
+     } else {
+	  pat_max = flags & FFTW_ESTIMATE ? 0 :
+	       (flags & FFTW_EXHAUSTIVE ? 3 :
+		(flags & FFTW_PATIENT ? 2 : 1));
+	  pat = plnr->timelimit >= 0 ? 0 : pat_max;
+
+	  flags &= ~(FFTW_ESTIMATE | FFTW_MEASURE |
+		     FFTW_PATIENT | FFTW_EXHAUSTIVE);
+
+	  plnr->start_time = X(get_crude_time)();
+
+	  /* plan at incrementally increasing patience until we run
+	     out of time */
+	  for (pln = 0, flags_used_for_planning = 0; pat <= pat_max; ++pat) {
+	       plan *pln1;
+	       unsigned tmpflags = flags | pats[pat];
+	       pln1 = mkplan(plnr, tmpflags, prb, 0u);
+
+	       if (!pln1) {
+		    /* don't bother continuing if planner failed or timed out */
+		    A(!pln || plnr->timed_out);
+		    break;
+	       }
+
+	       X(plan_destroy_internal)(pln);
+	       pln = pln1;
+	       flags_used_for_planning = tmpflags;
+	       pcost = pln->pcost;
+	  }
+     }
+
+     if (pln) {
+	  /* build apiplan */
+	  p = (apiplan *) MALLOC(sizeof(apiplan), PLANS);
+	  p->prb = prb;
+	  p->sign = sign; /* cache for execute_dft */
+
+	  /* re-create plan from wisdom, adding blessing */
+	  p->pln = mkplan(plnr, flags_used_for_planning, prb, BLESSING);
+
+	  /* record pcost from most recent measurement for use in X(cost) */
+	  p->pln->pcost = pcost;
+
+	  if (sizeof(trigreal) > sizeof(R)) {
+	       /* this is probably faster, and we have enough trigreal
+		  bits to maintain accuracy */
+	       X(plan_awake)(p->pln, AWAKE_SQRTN_TABLE);
+	  } else {
+	       /* more accurate */
+	       X(plan_awake)(p->pln, AWAKE_SINCOS);
+	  }
+
+	  /* we don't use pln for p->pln, above, since by re-creating the
+	     plan we might use more patient wisdom from a timed-out mkplan */
+	  X(plan_destroy_internal)(pln);
+     } else
+	  X(problem_destroy)(prb);
+
+     /* discard all information not necessary to reconstruct the plan */
+     plnr->adt->forget(plnr, FORGET_ACCURSED);
+
+#ifdef FFTW_RANDOM_ESTIMATOR
+     X(random_estimate_seed)++; /* subsequent "random" plans are distinct */
+#endif
+
+     if (after_planner_hook)
+          after_planner_hook();
+     
+     return p;
+}
+#endif
+
 void X(destroy_plan)(X(plan) p)
 {
      if (p) {
diff -up fftw-3.3.9/api/plan-many-dft.c.2~ fftw-3.3.9/api/plan-many-dft.c
--- fftw-3.3.9/api/plan-many-dft.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/api/plan-many-dft.c	2021-05-10 22:57:51.171039968 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -37,8 +38,12 @@ X(plan) X(plan_many_dft)(int rank, const
      EXTRACT_REIM(sign, in, &ri, &ii);
      EXTRACT_REIM(sign, out, &ro, &io);
 
-     return 
+     return
+#ifdef AMD_OPT_PREFER_256BIT_FPU
+	  X(mkapiplan_ex)(sign, flags, *n,
+#else
 	  X(mkapiplan)(sign, flags,
+#endif
 		       X(mkproblem_dft_d)(
 			    X(mktensor_rowmajor)(rank, n, 
 						 N0(inembed), N0(onembed),
diff -up fftw-3.3.9/api/the-planner.c.2~ fftw-3.3.9/api/the-planner.c
--- fftw-3.3.9/api/the-planner.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/api/the-planner.c	2021-05-10 22:57:51.171039968 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -33,6 +34,20 @@ planner *X(the_planner)(void)
      return plnr;
 }
 
+#ifdef AMD_OPT_PREFER_256BIT_FPU
+/* create the planner for the rest of the API: this variant also saves the problem size */
+planner *X(the_planner_ex)(int n)
+{
+     if (!plnr) {
+          plnr = X(mkplanner)();
+	  plnr->size = n;
+          X(configure_planner)(plnr);
+     }
+
+     return plnr;
+}
+#endif
+
 void X(cleanup)(void)
 {
      if (plnr) {
diff -up fftw-3.3.9/config.h.in.2~ fftw-3.3.9/config.h.in
--- fftw-3.3.9/config.h.in.2~	2020-12-10 13:03:38.000000000 +0100
+++ fftw-3.3.9/config.h.in	2021-05-10 22:57:51.171039968 +0200
@@ -1,5 +1,11 @@
 /* config.h.in.  Generated from configure.ac by autoheader.  */
 
+/* Define to enable AMD cpu specific optimizations. */
+#undef AMD_OPT_ALL
+
+/* Define to enable AMD cpu optimized Transpose. */
+#undef AMD_OPT_TRANS
+
 /* Define if the machine architecture "naturally" prefers fused multiply-add
    instructions */
 #undef ARCH_PREFERS_FMA
diff -up fftw-3.3.9/configure.2~ fftw-3.3.9/configure
--- fftw-3.3.9/configure.2~	2020-12-10 13:03:38.000000000 +0100
+++ fftw-3.3.9/configure	2021-05-10 22:59:13.086260615 +0200
@@ -874,6 +874,8 @@ with_gnu_ld
 with_sysroot
 enable_libtool_lock
 enable_mpi
+enable_amd_opt
+enable_amd_trans
 enable_fortran
 with_g77_wrappers
 enable_openmp
@@ -1580,6 +1582,8 @@ Optional Features:
                           optimize for fast installation [default=yes]
   --disable-libtool-lock  avoid locking (might break parallel builds)
   --enable-mpi            compile FFTW MPI library
+  --enable-amd-opt        enable AMD cpu specific optimizations
+  --enable-amd-trans      enable AMD cpu optimized Transpose
   --disable-fortran       don't include Fortran-callable wrappers
   --enable-openmp         use OpenMP directives for parallelism
   --enable-threads        compile FFTW SMP threads library
@@ -17428,6 +17432,44 @@ if test "$ac_test_CFLAGS" != "set"; then
 fi
 fi
 
+# Check whether --enable-amd-opt was given.
+if test "${enable_amd_opt+set}" = set; then :
+  enableval=$enable_amd_opt; have_amd_opt=$enableval
+else
+  have_amd_opt=no
+fi
+
+if test "$have_amd_opt" = yes; then
+	if test "$ac_test_CFLAGS" != "set"; then
+		if test "$CC" = "clang"; then
+			CFLAGS="$CFLAGS -mavx2 -mfma"
+		else
+			GCCVERSION=$(expr `gcc -dumpversion | cut -f1 -d.` \>= 9)
+			AMDZEN2MODEL=$(expr `cat /proc/cpuinfo | grep -m1 model|cut -f2 -d:` \>= 48)
+			if test "$GCCVERSION" = "1"  && test "$AMDZEN2MODEL" = "1"; then
+				CFLAGS="$CFLAGS -march=znver2 -mavx2 -mno-avx256-split-unaligned-store -mno-avx256-split-unaligned-load -mno-prefer-avx128 -mfma"
+			else
+				CFLAGS="$CFLAGS -march=znver1 -mavx2 -mno-avx256-split-unaligned-store -mno-avx256-split-unaligned-load -mno-prefer-avx128 -mfma"
+			fi
+		fi
+	fi
+
+$as_echo "#define AMD_OPT_ALL 1" >>confdefs.h
+
+fi
+# Check whether --enable-amd-trans was given.
+if test "${enable_amd_trans+set}" = set; then :
+  enableval=$enable_amd_trans; have_amd_trans=$enableval
+else
+  have_amd_trans=no
+fi
+
+if test "$have_amd_trans" = yes; then
+
+$as_echo "#define AMD_OPT_TRANS 1" >>confdefs.h
+
+fi
+
 if test "$USE_MAINTAINER_MODE" = yes; then
     # Extract the first word of "indent", so it can be a program name with args.
 set dummy indent; ac_word=$2
diff -up fftw-3.3.9/configure.ac.2~ fftw-3.3.9/configure.ac
--- fftw-3.3.9/configure.ac.2~	2021-05-10 22:57:51.175040028 +0200
+++ fftw-3.3.9/configure.ac	2021-05-10 22:59:48.098782278 +0200
@@ -612,6 +612,30 @@ if test "$ac_test_CFLAGS" != "set"; then
 fi
 fi
 
+dnl amd optimization switch and CFLAGS setting based on config arg option --enable-amd-opt
+AC_ARG_ENABLE(amd-opt, [AC_HELP_STRING([--enable-amd-opt],[enable AMD cpu specific optimizations])], have_amd_opt=$enableval, have_amd_opt=no)
+if test "$have_amd_opt" = yes; then
+	if test "$ac_test_CFLAGS" != "set"; then
+		if test "$CC" = "clang"; then
+			CFLAGS="$CFLAGS -mavx2 -mfma"
+		else
+			GCCVERSION=$(expr `gcc -dumpversion | cut -f1 -d.` \>= 9)
+			AMDZEN2MODEL=$(expr `cat /proc/cpuinfo | grep -m1 model|cut -f2 -d:` \>= 48)
+			if test "$GCCVERSION" = "1"  && test "$AMDZEN2MODEL" = "1"; then
+				CFLAGS="$CFLAGS -march=znver2 -mavx2 -mno-avx256-split-unaligned-store -mno-avx256-split-unaligned-load -mno-prefer-avx128 -mfma"
+			else
+				CFLAGS="$CFLAGS -march=znver1 -mavx2 -mno-avx256-split-unaligned-store -mno-avx256-split-unaligned-load -mno-prefer-avx128 -mfma"
+			fi
+		fi
+	fi
+	AC_DEFINE(AMD_OPT_ALL,1,[Define to enable AMD cpu specific optimizations.])
+fi
+dnl amd optimization switch to enable amd cpu optimized transpose --enable-amd-trans
+AC_ARG_ENABLE(amd-trans, [AC_HELP_STRING([--enable-amd-trans],[enable AMD cpu optimized Transpose])], have_amd_trans=$enableval, have_amd_trans=no)
+if test "$have_amd_trans" = yes; then
+	AC_DEFINE(AMD_OPT_TRANS,1,[Define to enable AMD cpu optimized Transpose.])
+fi
+
 dnl check for a proper indent in maintainer mode
 if test "$USE_MAINTAINER_MODE" = yes; then
     AC_PATH_PROG(INDENT, indent, indent)
diff -up fftw-3.3.9/dft/conf.c.2~ fftw-3.3.9/dft/conf.c
--- fftw-3.3.9/dft/conf.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/dft/conf.c	2021-05-10 22:57:51.175040028 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -56,7 +57,11 @@ void X(dft_conf_standard)(planner *p)
 #if HAVE_AVX2
      if (X(have_simd_avx2)())
          X(solvtab_exec)(X(solvtab_dft_avx2), p);
+#ifdef AMD_OPT_PREFER_256BIT_FPU
+     if ((p->size > AMD_OPT_128BIT_KERNELS_THRESHOLD) && X(have_simd_avx2_128)())
+#else
      if (X(have_simd_avx2_128)())
+#endif
          X(solvtab_exec)(X(solvtab_dft_avx2_128), p);
 #endif
 #if HAVE_AVX512
@@ -85,4 +90,7 @@ void X(dft_conf_standard)(planner *p)
 #if HAVE_GENERIC_SIMD256
      X(solvtab_exec)(X(solvtab_dft_generic_simd256), p);
 #endif
+#ifdef AMD_OPT_AUTO_TUNED_TRANS_BLK_SIZE
+     X(enquire_L1DcacheSize)();
+#endif
 }
diff -up fftw-3.3.9/dft/simd/common/n2fv_8.c.2~ fftw-3.3.9/dft/simd/common/n2fv_8.c
--- fftw-3.3.9/dft/simd/common/n2fv_8.c.2~	2020-12-10 13:04:53.000000000 +0100
+++ fftw-3.3.9/dft/simd/common/n2fv_8.c	2021-05-10 22:57:51.175040028 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -162,6 +163,80 @@ static void n2fv_8(const R *ri, const R
 			 Tm = VADD(T4, T5);
 		    }
 	       }
+#if defined(AMD_OPT_KERNEL_REARRANGE_WRITE_V1)
+	       {
+		       V Tl, To, Tx, Ty, Tb, Tg, Tw;
+		       Tb = VADD(T3, Ta);
+		       Tg = VBYI(VSUB(Tc, Tf));
+
+		       /**/
+		       Tl = VADD(Tj, Tk);
+		       To = VADD(Tm, Tn);
+		       Ty = VADD(Tl, To);
+		       Ts = VADD(Tb, Tg);
+#if MEM_256 == 0
+               STM2(&(xo[0]), Ty, ovs, &(xo[0]));
+               STM2(&(xo[2]), Ts, ovs, &(xo[2]));
+               STN2(&(xo[0]), Ty, Ts, ovs);
+#else
+		       Tx = SHUF_CROSS_LANE_1(Ty, Ts);
+		       Ts = SHUF_CROSS_LANE_2(Ty, Ts);
+		       STA(&(xo[0]), Tx, ovs, &(xo[0]));
+		       STA(&(xo[0])+ovs, Ts, ovs, &(xo[0]));
+#endif
+		       Tx = VSUB(Tl, To);
+		       /**/
+
+		       Tl = VSUB(Tj, Tk);
+		       To = VBYI(VSUB(Tn, Tm));
+		       Tu = VADD(Tl, To);
+
+		       /**/
+		       T3 = VSUB(T3, Ta);
+		       Ta = VBYI(VADD(Tf, Tc));
+		       Tw = VADD(T3, Ta);
+#if MEM_256 == 0
+               STM2(&(xo[4]), Tu, ovs, &(xo[0]));
+               STM2(&(xo[6]), Tw, ovs, &(xo[2]));
+               STN2(&(xo[4]), Tu, Tw, ovs);
+#else
+		       Ty = SHUF_CROSS_LANE_1(Tu, Tw);
+		       Tw = SHUF_CROSS_LANE_2(Tu, Tw);
+		       STA(&(xo[4]), Ty, ovs, &(xo[0]));
+		       STA(&(xo[4])+ovs, Tw, ovs, &(xo[0]));
+#endif
+		       /**/
+
+		       /**/
+		       Ta = VSUB(T3, Ta);
+#if MEM_256 == 0
+               STM2(&(xo[8]), Tx, ovs, &(xo[0]));
+               STN2(&(xo[8]), Tx, Ta, ovs);
+               STM2(&(xo[10]), Ta, ovs, &(xo[2]));
+#else
+		       Ty = SHUF_CROSS_LANE_1(Tx, Ta);
+		       Ta = SHUF_CROSS_LANE_2(Tx, Ta);
+		       STA(&(xo[8]), Ty, ovs, &(xo[0]));
+		       STA(&(xo[8])+ovs, Ta, ovs, &(xo[0]));
+#endif
+		       /**/
+
+		       /**/
+		       Tl = VSUB(Tl, To);
+		       To = VSUB(Tb, Tg);
+#if MEM_256 == 0
+               STM2(&(xo[12]), Tl, ovs, &(xo[0]));
+               STM2(&(xo[14]), To, ovs, &(xo[2]));
+               STN2(&(xo[12]), Tl, To, ovs);
+#else
+		       Ty = SHUF_CROSS_LANE_1(Tl, To);
+		       To = SHUF_CROSS_LANE_2(Tl, To);
+		       STA(&(xo[12]), Ty, ovs, &(xo[0]));
+		       STA(&(xo[12])+ovs, To, ovs, &(xo[0]));
+#endif
+		       /**/
+	       }
+#else 
 	       {
 		    V Tr, Tb, Tg, Tp, Tq, Tt;
 		    Tb = VADD(T3, Ta);
@@ -199,6 +274,7 @@ static void n2fv_8(const R *ri, const R
 			 STN2(&(xo[0]), Ty, Ts, ovs);
 		    }
 	       }
+#endif
 	  }
      }
      VLEAVE();
diff -up fftw-3.3.9/dft/simd/common/q1fv_4.c.2~ fftw-3.3.9/dft/simd/common/q1fv_4.c
--- fftw-3.3.9/dft/simd/common/q1fv_4.c.2~	2020-12-10 13:05:40.000000000 +0100
+++ fftw-3.3.9/dft/simd/common/q1fv_4.c	2021-05-10 22:57:51.175040028 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -154,17 +155,21 @@ void XSIMD(codelet_q1fv_4) (planner *p)
 
 static void q1fv_4(R *ri, R *ii, const R *W, stride rs, stride vs, INT mb, INT me, INT ms)
 {
+
      {
 	  INT m;
 	  R *x;
 	  x = ri;
-	  for (m = mb, W = W + (mb * ((TWVL / VL) * 6)); m < me; m = m + VL, x = x + (VL * ms), W = W + (TWVL * 6), MAKE_VOLATILE_STRIDE(8, rs), MAKE_VOLATILE_STRIDE(8, vs)) {
-	       V T3, T9, TA, TG, TD, TH, T6, Ta, Te, Tk, Tp, Tv, Ts, Tw, Th;
+
+	  for (m = mb, W = W + (mb * ((TWVL / VL) * 6)); m < me; m = m + VL, x = x + (VL * ms), W = W + (TWVL * 6), MAKE_VOLATILE_STRIDE(8, rs), MAKE_VOLATILE_STRIDE(8, vs)) 
+	  {	        
+               V T3, T9, TA, TG, TD, TH, T6, Ta, Te, Tk, Tp, Tv, Ts, Tw, Th;
 	       V Tl;
 	       {
 		    V T1, T2, Ty, Tz;
 		    T1 = LD(&(x[0]), ms, &(x[0]));
 		    T2 = LD(&(x[WS(rs, 2)]), ms, &(x[0]));
+
 		    T3 = VSUB(T1, T2);
 		    T9 = VADD(T1, T2);
 		    Ty = LD(&(x[WS(vs, 3)]), ms, &(x[WS(vs, 3)]));
@@ -205,10 +210,56 @@ static void q1fv_4(R *ri, R *ii, const R
 		    Th = VBYI(VSUB(Tf, Tg));
 		    Tl = VADD(Tf, Tg);
 	       }
-	       ST(&(x[0]), VADD(T9, Ta), ms, &(x[0]));
-	       ST(&(x[WS(rs, 1)]), VADD(Tk, Tl), ms, &(x[WS(rs, 1)]));
-	       ST(&(x[WS(rs, 2)]), VADD(Tv, Tw), ms, &(x[0]));
-	       ST(&(x[WS(rs, 3)]), VADD(TG, TH), ms, &(x[WS(rs, 1)]));
+
+#if defined(AMD_OPT_KERNEL_REARRANGE_WRITE_V1)
+	       
+	       {
+	            V T7,T8,Tb;
+                    ST(&(x[0]), VADD(T9, Ta), ms, &(x[0]));
+	            T7 = BYTWJ(&(W[0]), VSUB(T3, T6));
+		    ST(&(x[WS(vs, 1)]), T7, ms, &(x[WS(vs, 1)]));
+		    Tb = BYTWJ(&(W[TWVL * 2]), VSUB(T9, Ta));
+		    ST(&(x[WS(vs, 2)]), Tb, ms, &(x[WS(vs, 2)]));
+		    T8 = BYTWJ(&(W[TWVL * 4]), VADD(T3, T6));
+		    ST(&(x[WS(vs, 3)]), T8, ms, &(x[WS(vs, 3)]));
+	       }
+	       {
+	            V Ti,Tj,Tm;
+	            ST(&(x[WS(rs, 1)]), VADD(Tk, Tl), ms, &(x[WS(rs, 1)]));
+	            Ti = BYTWJ(&(W[0]), VSUB(Te, Th));
+	            ST(&(x[WS(vs, 1) + WS(rs, 1)]), Ti, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+	            Tm = BYTWJ(&(W[TWVL * 2]), VSUB(Tk, Tl));
+	            ST(&(x[WS(vs, 2) + WS(rs, 1)]), Tm, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+	            Tj = BYTWJ(&(W[TWVL * 4]), VADD(Te, Th));
+	            ST(&(x[WS(vs, 3) + WS(rs, 1)]), Tj, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+	       }
+               {
+           	    V Tt,Tu,Tx;
+	            ST(&(x[WS(rs, 2)]), VADD(Tv, Tw), ms, &(x[0]));
+	            Tt = BYTWJ(&(W[0]), VSUB(Tp, Ts));
+	            ST(&(x[WS(vs, 1) + WS(rs, 2)]), Tt, ms, &(x[WS(vs, 1)]));
+	            Tx = BYTWJ(&(W[TWVL * 2]), VSUB(Tv, Tw));
+	            ST(&(x[WS(vs, 2) + WS(rs, 2)]), Tx, ms, &(x[WS(vs, 2)]));
+	            Tu = BYTWJ(&(W[TWVL * 4]), VADD(Tp, Ts));
+	            ST(&(x[WS(vs, 3) + WS(rs, 2)]), Tu, ms, &(x[WS(vs, 3)]));
+ 	       }
+ 	       {
+ 		    V TE,TF,TI;	
+	            ST(&(x[WS(rs, 3)]), VADD(TG, TH), ms, &(x[WS(rs, 1)]));
+	            TE = BYTWJ(&(W[0]), VSUB(TA, TD));
+	            ST(&(x[WS(vs, 1) + WS(rs, 3)]), TE, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+	            TI = BYTWJ(&(W[TWVL * 2]), VSUB(TG, TH));
+	            ST(&(x[WS(vs, 2) + WS(rs, 3)]), TI, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+	            TF = BYTWJ(&(W[TWVL * 4]), VADD(TA, TD));
+	            ST(&(x[WS(vs, 3) + WS(rs, 3)]), TF, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+	       }
+
+#else   
+			
+		    ST(&(x[0]), VADD(T9, Ta), ms, &(x[0]));
+	            ST(&(x[WS(rs, 1)]), VADD(Tk, Tl), ms, &(x[WS(rs, 1)]));
+	            ST(&(x[WS(rs, 2)]), VADD(Tv, Tw), ms, &(x[0]));
+	            ST(&(x[WS(rs, 3)]), VADD(TG, TH), ms, &(x[WS(rs, 1)]));
 	       {
 		    V T7, Ti, Tt, TE;
 		    T7 = BYTWJ(&(W[0]), VSUB(T3, T6));
@@ -242,11 +293,18 @@ static void q1fv_4(R *ri, R *ii, const R
 		    TI = BYTWJ(&(W[TWVL * 2]), VSUB(TG, TH));
 		    ST(&(x[WS(vs, 2) + WS(rs, 3)]), TI, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
 	       }
-	  }
+	       	    
+#endif      
+
+	 }
+
      }
      VLEAVE();
+
+
 }
 
+
 static const tw_instr twinstr[] = {
      VTW(0, 1),
      VTW(0, 2),
diff -up fftw-3.3.9/dft/simd/common/q1fv_8.c.2~ fftw-3.3.9/dft/simd/common/q1fv_8.c
--- fftw-3.3.9/dft/simd/common/q1fv_8.c.2~	2020-12-10 13:05:41.000000000 +0100
+++ fftw-3.3.9/dft/simd/common/q1fv_8.c	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -38,476 +39,477 @@ static void q1fv_8(R *ri, R *ii, const R
 {
      DVK(KP707106781, +0.707106781186547524400844362104849039284835938);
      {
-	  INT m;
-	  R *x;
-	  x = ri;
-	  for (m = mb, W = W + (mb * ((TWVL / VL) * 14)); m < me; m = m + VL, x = x + (VL * ms), W = W + (TWVL * 14), MAKE_VOLATILE_STRIDE(16, rs), MAKE_VOLATILE_STRIDE(16, vs)) {
-	       V T3, Tu, Te, Tp, T1E, T25, T1P, T20, T2b, T2C, T2m, T2x, T3M, T4d, T3X;
-	       V T48, TA, T11, TL, TW, T17, T1y, T1i, T1t, T2I, T39, T2T, T34, T3f, T3G;
-	       V T3q, T3B, Ta, Tv, Tf, Ts, T1L, T26, T1Q, T23, T2i, T2D, T2n, T2A, T3T;
-	       V T4e, T3Y, T4b, TH, T12, TM, TZ, T1e, T1z, T1j, T1w, T2P, T3a, T2U, T37;
-	       V T3m, T3H, T3r, T3E, T28, T14;
-	       {
-		    V T1, T2, Tn, Tc, Td, To;
-		    T1 = LD(&(x[0]), ms, &(x[0]));
-		    T2 = LD(&(x[WS(rs, 4)]), ms, &(x[0]));
-		    Tn = VADD(T1, T2);
-		    Tc = LD(&(x[WS(rs, 2)]), ms, &(x[0]));
-		    Td = LD(&(x[WS(rs, 6)]), ms, &(x[0]));
-		    To = VADD(Tc, Td);
-		    T3 = VSUB(T1, T2);
-		    Tu = VSUB(Tn, To);
-		    Te = VSUB(Tc, Td);
-		    Tp = VADD(Tn, To);
-	       }
-	       {
-		    V T1C, T1D, T1Y, T1N, T1O, T1Z;
-		    T1C = LD(&(x[WS(vs, 3)]), ms, &(x[WS(vs, 3)]));
-		    T1D = LD(&(x[WS(vs, 3) + WS(rs, 4)]), ms, &(x[WS(vs, 3)]));
-		    T1Y = VADD(T1C, T1D);
-		    T1N = LD(&(x[WS(vs, 3) + WS(rs, 2)]), ms, &(x[WS(vs, 3)]));
-		    T1O = LD(&(x[WS(vs, 3) + WS(rs, 6)]), ms, &(x[WS(vs, 3)]));
-		    T1Z = VADD(T1N, T1O);
-		    T1E = VSUB(T1C, T1D);
-		    T25 = VSUB(T1Y, T1Z);
-		    T1P = VSUB(T1N, T1O);
-		    T20 = VADD(T1Y, T1Z);
-	       }
-	       {
-		    V T29, T2a, T2v, T2k, T2l, T2w;
-		    T29 = LD(&(x[WS(vs, 4)]), ms, &(x[WS(vs, 4)]));
-		    T2a = LD(&(x[WS(vs, 4) + WS(rs, 4)]), ms, &(x[WS(vs, 4)]));
-		    T2v = VADD(T29, T2a);
-		    T2k = LD(&(x[WS(vs, 4) + WS(rs, 2)]), ms, &(x[WS(vs, 4)]));
-		    T2l = LD(&(x[WS(vs, 4) + WS(rs, 6)]), ms, &(x[WS(vs, 4)]));
-		    T2w = VADD(T2k, T2l);
-		    T2b = VSUB(T29, T2a);
-		    T2C = VSUB(T2v, T2w);
-		    T2m = VSUB(T2k, T2l);
-		    T2x = VADD(T2v, T2w);
-	       }
-	       {
-		    V T3K, T3L, T46, T3V, T3W, T47;
-		    T3K = LD(&(x[WS(vs, 7)]), ms, &(x[WS(vs, 7)]));
-		    T3L = LD(&(x[WS(vs, 7) + WS(rs, 4)]), ms, &(x[WS(vs, 7)]));
-		    T46 = VADD(T3K, T3L);
-		    T3V = LD(&(x[WS(vs, 7) + WS(rs, 2)]), ms, &(x[WS(vs, 7)]));
-		    T3W = LD(&(x[WS(vs, 7) + WS(rs, 6)]), ms, &(x[WS(vs, 7)]));
-		    T47 = VADD(T3V, T3W);
-		    T3M = VSUB(T3K, T3L);
-		    T4d = VSUB(T46, T47);
-		    T3X = VSUB(T3V, T3W);
-		    T48 = VADD(T46, T47);
-	       }
-	       {
-		    V Ty, Tz, TU, TJ, TK, TV;
-		    Ty = LD(&(x[WS(vs, 1)]), ms, &(x[WS(vs, 1)]));
-		    Tz = LD(&(x[WS(vs, 1) + WS(rs, 4)]), ms, &(x[WS(vs, 1)]));
-		    TU = VADD(Ty, Tz);
-		    TJ = LD(&(x[WS(vs, 1) + WS(rs, 2)]), ms, &(x[WS(vs, 1)]));
-		    TK = LD(&(x[WS(vs, 1) + WS(rs, 6)]), ms, &(x[WS(vs, 1)]));
-		    TV = VADD(TJ, TK);
-		    TA = VSUB(Ty, Tz);
-		    T11 = VSUB(TU, TV);
-		    TL = VSUB(TJ, TK);
-		    TW = VADD(TU, TV);
-	       }
-	       {
-		    V T15, T16, T1r, T1g, T1h, T1s;
-		    T15 = LD(&(x[WS(vs, 2)]), ms, &(x[WS(vs, 2)]));
-		    T16 = LD(&(x[WS(vs, 2) + WS(rs, 4)]), ms, &(x[WS(vs, 2)]));
-		    T1r = VADD(T15, T16);
-		    T1g = LD(&(x[WS(vs, 2) + WS(rs, 2)]), ms, &(x[WS(vs, 2)]));
-		    T1h = LD(&(x[WS(vs, 2) + WS(rs, 6)]), ms, &(x[WS(vs, 2)]));
-		    T1s = VADD(T1g, T1h);
-		    T17 = VSUB(T15, T16);
-		    T1y = VSUB(T1r, T1s);
-		    T1i = VSUB(T1g, T1h);
-		    T1t = VADD(T1r, T1s);
-	       }
-	       {
-		    V T2G, T2H, T32, T2R, T2S, T33;
-		    T2G = LD(&(x[WS(vs, 5)]), ms, &(x[WS(vs, 5)]));
-		    T2H = LD(&(x[WS(vs, 5) + WS(rs, 4)]), ms, &(x[WS(vs, 5)]));
-		    T32 = VADD(T2G, T2H);
-		    T2R = LD(&(x[WS(vs, 5) + WS(rs, 2)]), ms, &(x[WS(vs, 5)]));
-		    T2S = LD(&(x[WS(vs, 5) + WS(rs, 6)]), ms, &(x[WS(vs, 5)]));
-		    T33 = VADD(T2R, T2S);
-		    T2I = VSUB(T2G, T2H);
-		    T39 = VSUB(T32, T33);
-		    T2T = VSUB(T2R, T2S);
-		    T34 = VADD(T32, T33);
-	       }
-	       {
-		    V T3d, T3e, T3z, T3o, T3p, T3A;
-		    T3d = LD(&(x[WS(vs, 6)]), ms, &(x[WS(vs, 6)]));
-		    T3e = LD(&(x[WS(vs, 6) + WS(rs, 4)]), ms, &(x[WS(vs, 6)]));
-		    T3z = VADD(T3d, T3e);
-		    T3o = LD(&(x[WS(vs, 6) + WS(rs, 2)]), ms, &(x[WS(vs, 6)]));
-		    T3p = LD(&(x[WS(vs, 6) + WS(rs, 6)]), ms, &(x[WS(vs, 6)]));
-		    T3A = VADD(T3o, T3p);
-		    T3f = VSUB(T3d, T3e);
-		    T3G = VSUB(T3z, T3A);
-		    T3q = VSUB(T3o, T3p);
-		    T3B = VADD(T3z, T3A);
-	       }
-	       {
-		    V T6, Tq, T9, Tr;
-		    {
-			 V T4, T5, T7, T8;
-			 T4 = LD(&(x[WS(rs, 1)]), ms, &(x[WS(rs, 1)]));
-			 T5 = LD(&(x[WS(rs, 5)]), ms, &(x[WS(rs, 1)]));
-			 T6 = VSUB(T4, T5);
-			 Tq = VADD(T4, T5);
-			 T7 = LD(&(x[WS(rs, 7)]), ms, &(x[WS(rs, 1)]));
-			 T8 = LD(&(x[WS(rs, 3)]), ms, &(x[WS(rs, 1)]));
-			 T9 = VSUB(T7, T8);
-			 Tr = VADD(T7, T8);
-		    }
-		    Ta = VADD(T6, T9);
-		    Tv = VSUB(Tr, Tq);
-		    Tf = VSUB(T9, T6);
-		    Ts = VADD(Tq, Tr);
-	       }
-	       {
-		    V T1H, T21, T1K, T22;
-		    {
-			 V T1F, T1G, T1I, T1J;
-			 T1F = LD(&(x[WS(vs, 3) + WS(rs, 1)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1G = LD(&(x[WS(vs, 3) + WS(rs, 5)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1H = VSUB(T1F, T1G);
-			 T21 = VADD(T1F, T1G);
-			 T1I = LD(&(x[WS(vs, 3) + WS(rs, 7)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1J = LD(&(x[WS(vs, 3) + WS(rs, 3)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1K = VSUB(T1I, T1J);
-			 T22 = VADD(T1I, T1J);
-		    }
-		    T1L = VADD(T1H, T1K);
-		    T26 = VSUB(T22, T21);
-		    T1Q = VSUB(T1K, T1H);
-		    T23 = VADD(T21, T22);
-	       }
-	       {
-		    V T2e, T2y, T2h, T2z;
-		    {
-			 V T2c, T2d, T2f, T2g;
-			 T2c = LD(&(x[WS(vs, 4) + WS(rs, 1)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2d = LD(&(x[WS(vs, 4) + WS(rs, 5)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2e = VSUB(T2c, T2d);
-			 T2y = VADD(T2c, T2d);
-			 T2f = LD(&(x[WS(vs, 4) + WS(rs, 7)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2g = LD(&(x[WS(vs, 4) + WS(rs, 3)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2h = VSUB(T2f, T2g);
-			 T2z = VADD(T2f, T2g);
-		    }
-		    T2i = VADD(T2e, T2h);
-		    T2D = VSUB(T2z, T2y);
-		    T2n = VSUB(T2h, T2e);
-		    T2A = VADD(T2y, T2z);
-	       }
-	       {
-		    V T3P, T49, T3S, T4a;
-		    {
-			 V T3N, T3O, T3Q, T3R;
-			 T3N = LD(&(x[WS(vs, 7) + WS(rs, 1)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3O = LD(&(x[WS(vs, 7) + WS(rs, 5)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3P = VSUB(T3N, T3O);
-			 T49 = VADD(T3N, T3O);
-			 T3Q = LD(&(x[WS(vs, 7) + WS(rs, 7)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3R = LD(&(x[WS(vs, 7) + WS(rs, 3)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3S = VSUB(T3Q, T3R);
-			 T4a = VADD(T3Q, T3R);
-		    }
-		    T3T = VADD(T3P, T3S);
-		    T4e = VSUB(T4a, T49);
-		    T3Y = VSUB(T3S, T3P);
-		    T4b = VADD(T49, T4a);
-	       }
-	       {
-		    V TD, TX, TG, TY;
-		    {
-			 V TB, TC, TE, TF;
-			 TB = LD(&(x[WS(vs, 1) + WS(rs, 1)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TC = LD(&(x[WS(vs, 1) + WS(rs, 5)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TD = VSUB(TB, TC);
-			 TX = VADD(TB, TC);
-			 TE = LD(&(x[WS(vs, 1) + WS(rs, 7)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TF = LD(&(x[WS(vs, 1) + WS(rs, 3)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TG = VSUB(TE, TF);
-			 TY = VADD(TE, TF);
-		    }
-		    TH = VADD(TD, TG);
-		    T12 = VSUB(TY, TX);
-		    TM = VSUB(TG, TD);
-		    TZ = VADD(TX, TY);
-	       }
-	       {
-		    V T1a, T1u, T1d, T1v;
-		    {
-			 V T18, T19, T1b, T1c;
-			 T18 = LD(&(x[WS(vs, 2) + WS(rs, 1)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T19 = LD(&(x[WS(vs, 2) + WS(rs, 5)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T1a = VSUB(T18, T19);
-			 T1u = VADD(T18, T19);
-			 T1b = LD(&(x[WS(vs, 2) + WS(rs, 7)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T1c = LD(&(x[WS(vs, 2) + WS(rs, 3)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T1d = VSUB(T1b, T1c);
-			 T1v = VADD(T1b, T1c);
-		    }
-		    T1e = VADD(T1a, T1d);
-		    T1z = VSUB(T1v, T1u);
-		    T1j = VSUB(T1d, T1a);
-		    T1w = VADD(T1u, T1v);
-	       }
-	       {
-		    V T2L, T35, T2O, T36;
-		    {
-			 V T2J, T2K, T2M, T2N;
-			 T2J = LD(&(x[WS(vs, 5) + WS(rs, 1)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2K = LD(&(x[WS(vs, 5) + WS(rs, 5)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2L = VSUB(T2J, T2K);
-			 T35 = VADD(T2J, T2K);
-			 T2M = LD(&(x[WS(vs, 5) + WS(rs, 7)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2N = LD(&(x[WS(vs, 5) + WS(rs, 3)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2O = VSUB(T2M, T2N);
-			 T36 = VADD(T2M, T2N);
-		    }
-		    T2P = VADD(T2L, T2O);
-		    T3a = VSUB(T36, T35);
-		    T2U = VSUB(T2O, T2L);
-		    T37 = VADD(T35, T36);
-	       }
-	       {
-		    V T3i, T3C, T3l, T3D;
-		    {
-			 V T3g, T3h, T3j, T3k;
-			 T3g = LD(&(x[WS(vs, 6) + WS(rs, 1)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3h = LD(&(x[WS(vs, 6) + WS(rs, 5)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3i = VSUB(T3g, T3h);
-			 T3C = VADD(T3g, T3h);
-			 T3j = LD(&(x[WS(vs, 6) + WS(rs, 7)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3k = LD(&(x[WS(vs, 6) + WS(rs, 3)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3l = VSUB(T3j, T3k);
-			 T3D = VADD(T3j, T3k);
-		    }
-		    T3m = VADD(T3i, T3l);
-		    T3H = VSUB(T3D, T3C);
-		    T3r = VSUB(T3l, T3i);
-		    T3E = VADD(T3C, T3D);
-	       }
-	       ST(&(x[0]), VADD(Tp, Ts), ms, &(x[0]));
-	       ST(&(x[WS(rs, 2)]), VADD(T1t, T1w), ms, &(x[0]));
-	       ST(&(x[WS(rs, 5)]), VADD(T34, T37), ms, &(x[WS(rs, 1)]));
-	       ST(&(x[WS(rs, 7)]), VADD(T48, T4b), ms, &(x[WS(rs, 1)]));
-	       ST(&(x[WS(rs, 6)]), VADD(T3B, T3E), ms, &(x[0]));
-	       ST(&(x[WS(rs, 4)]), VADD(T2x, T2A), ms, &(x[0]));
-	       {
-		    V Tt, T4c, T2B, T24;
-		    ST(&(x[WS(rs, 3)]), VADD(T20, T23), ms, &(x[WS(rs, 1)]));
-		    ST(&(x[WS(rs, 1)]), VADD(TW, TZ), ms, &(x[WS(rs, 1)]));
-		    Tt = BYTWJ(&(W[TWVL * 6]), VSUB(Tp, Ts));
-		    ST(&(x[WS(vs, 4)]), Tt, ms, &(x[WS(vs, 4)]));
-		    T4c = BYTWJ(&(W[TWVL * 6]), VSUB(T48, T4b));
-		    ST(&(x[WS(vs, 4) + WS(rs, 7)]), T4c, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-		    T2B = BYTWJ(&(W[TWVL * 6]), VSUB(T2x, T2A));
-		    ST(&(x[WS(vs, 4) + WS(rs, 4)]), T2B, ms, &(x[WS(vs, 4)]));
-		    T24 = BYTWJ(&(W[TWVL * 6]), VSUB(T20, T23));
-		    ST(&(x[WS(vs, 4) + WS(rs, 3)]), T24, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-	       }
-	       {
-		    V T10, T1x, T3F, T38, T1A, Tw;
-		    T10 = BYTWJ(&(W[TWVL * 6]), VSUB(TW, TZ));
-		    ST(&(x[WS(vs, 4) + WS(rs, 1)]), T10, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-		    T1x = BYTWJ(&(W[TWVL * 6]), VSUB(T1t, T1w));
-		    ST(&(x[WS(vs, 4) + WS(rs, 2)]), T1x, ms, &(x[WS(vs, 4)]));
-		    T3F = BYTWJ(&(W[TWVL * 6]), VSUB(T3B, T3E));
-		    ST(&(x[WS(vs, 4) + WS(rs, 6)]), T3F, ms, &(x[WS(vs, 4)]));
-		    T38 = BYTWJ(&(W[TWVL * 6]), VSUB(T34, T37));
-		    ST(&(x[WS(vs, 4) + WS(rs, 5)]), T38, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-		    T1A = BYTWJ(&(W[TWVL * 10]), VFNMSI(T1z, T1y));
-		    ST(&(x[WS(vs, 6) + WS(rs, 2)]), T1A, ms, &(x[WS(vs, 6)]));
-		    Tw = BYTWJ(&(W[TWVL * 10]), VFNMSI(Tv, Tu));
-		    ST(&(x[WS(vs, 6)]), Tw, ms, &(x[WS(vs, 6)]));
-	       }
-	       {
-		    V T2E, T3I, T13, T27, T3b, T4f;
-		    T2E = BYTWJ(&(W[TWVL * 10]), VFNMSI(T2D, T2C));
-		    ST(&(x[WS(vs, 6) + WS(rs, 4)]), T2E, ms, &(x[WS(vs, 6)]));
-		    T3I = BYTWJ(&(W[TWVL * 10]), VFNMSI(T3H, T3G));
-		    ST(&(x[WS(vs, 6) + WS(rs, 6)]), T3I, ms, &(x[WS(vs, 6)]));
-		    T13 = BYTWJ(&(W[TWVL * 10]), VFNMSI(T12, T11));
-		    ST(&(x[WS(vs, 6) + WS(rs, 1)]), T13, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-		    T27 = BYTWJ(&(W[TWVL * 10]), VFNMSI(T26, T25));
-		    ST(&(x[WS(vs, 6) + WS(rs, 3)]), T27, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-		    T3b = BYTWJ(&(W[TWVL * 10]), VFNMSI(T3a, T39));
-		    ST(&(x[WS(vs, 6) + WS(rs, 5)]), T3b, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-		    T4f = BYTWJ(&(W[TWVL * 10]), VFNMSI(T4e, T4d));
-		    ST(&(x[WS(vs, 6) + WS(rs, 7)]), T4f, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-	       }
-	       {
-		    V Tx, T1B, T3c, T4g, T3J, T2F;
-		    Tx = BYTWJ(&(W[TWVL * 2]), VFMAI(Tv, Tu));
-		    ST(&(x[WS(vs, 2)]), Tx, ms, &(x[WS(vs, 2)]));
-		    T1B = BYTWJ(&(W[TWVL * 2]), VFMAI(T1z, T1y));
-		    ST(&(x[WS(vs, 2) + WS(rs, 2)]), T1B, ms, &(x[WS(vs, 2)]));
-		    T3c = BYTWJ(&(W[TWVL * 2]), VFMAI(T3a, T39));
-		    ST(&(x[WS(vs, 2) + WS(rs, 5)]), T3c, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-		    T4g = BYTWJ(&(W[TWVL * 2]), VFMAI(T4e, T4d));
-		    ST(&(x[WS(vs, 2) + WS(rs, 7)]), T4g, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-		    T3J = BYTWJ(&(W[TWVL * 2]), VFMAI(T3H, T3G));
-		    ST(&(x[WS(vs, 2) + WS(rs, 6)]), T3J, ms, &(x[WS(vs, 2)]));
-		    T2F = BYTWJ(&(W[TWVL * 2]), VFMAI(T2D, T2C));
-		    ST(&(x[WS(vs, 2) + WS(rs, 4)]), T2F, ms, &(x[WS(vs, 2)]));
-	       }
-	       T28 = BYTWJ(&(W[TWVL * 2]), VFMAI(T26, T25));
-	       ST(&(x[WS(vs, 2) + WS(rs, 3)]), T28, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-	       T14 = BYTWJ(&(W[TWVL * 2]), VFMAI(T12, T11));
-	       ST(&(x[WS(vs, 2) + WS(rs, 1)]), T14, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-	       {
-		    V Th, Ti, Tb, Tg;
-		    Tb = VFMA(LDK(KP707106781), Ta, T3);
-		    Tg = VFNMS(LDK(KP707106781), Tf, Te);
-		    Th = BYTWJ(&(W[0]), VFNMSI(Tg, Tb));
-		    Ti = BYTWJ(&(W[TWVL * 12]), VFMAI(Tg, Tb));
-		    ST(&(x[WS(vs, 1)]), Th, ms, &(x[WS(vs, 1)]));
-		    ST(&(x[WS(vs, 7)]), Ti, ms, &(x[WS(vs, 7)]));
-	       }
-	       {
-		    V T40, T41, T3U, T3Z;
-		    T3U = VFMA(LDK(KP707106781), T3T, T3M);
-		    T3Z = VFNMS(LDK(KP707106781), T3Y, T3X);
-		    T40 = BYTWJ(&(W[0]), VFNMSI(T3Z, T3U));
-		    T41 = BYTWJ(&(W[TWVL * 12]), VFMAI(T3Z, T3U));
-		    ST(&(x[WS(vs, 1) + WS(rs, 7)]), T40, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 7) + WS(rs, 7)]), T41, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-	       }
-	       {
-		    V T2p, T2q, T2j, T2o;
-		    T2j = VFMA(LDK(KP707106781), T2i, T2b);
-		    T2o = VFNMS(LDK(KP707106781), T2n, T2m);
-		    T2p = BYTWJ(&(W[0]), VFNMSI(T2o, T2j));
-		    T2q = BYTWJ(&(W[TWVL * 12]), VFMAI(T2o, T2j));
-		    ST(&(x[WS(vs, 1) + WS(rs, 4)]), T2p, ms, &(x[WS(vs, 1)]));
-		    ST(&(x[WS(vs, 7) + WS(rs, 4)]), T2q, ms, &(x[WS(vs, 7)]));
-	       }
-	       {
-		    V T1S, T1T, T1M, T1R;
-		    T1M = VFMA(LDK(KP707106781), T1L, T1E);
-		    T1R = VFNMS(LDK(KP707106781), T1Q, T1P);
-		    T1S = BYTWJ(&(W[0]), VFNMSI(T1R, T1M));
-		    T1T = BYTWJ(&(W[TWVL * 12]), VFMAI(T1R, T1M));
-		    ST(&(x[WS(vs, 1) + WS(rs, 3)]), T1S, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 7) + WS(rs, 3)]), T1T, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-	       }
-	       {
-		    V TO, TP, TI, TN;
-		    TI = VFMA(LDK(KP707106781), TH, TA);
-		    TN = VFNMS(LDK(KP707106781), TM, TL);
-		    TO = BYTWJ(&(W[0]), VFNMSI(TN, TI));
-		    TP = BYTWJ(&(W[TWVL * 12]), VFMAI(TN, TI));
-		    ST(&(x[WS(vs, 1) + WS(rs, 1)]), TO, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 7) + WS(rs, 1)]), TP, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-	       }
-	       {
-		    V T1l, T1m, T1f, T1k;
-		    T1f = VFMA(LDK(KP707106781), T1e, T17);
-		    T1k = VFNMS(LDK(KP707106781), T1j, T1i);
-		    T1l = BYTWJ(&(W[0]), VFNMSI(T1k, T1f));
-		    T1m = BYTWJ(&(W[TWVL * 12]), VFMAI(T1k, T1f));
-		    ST(&(x[WS(vs, 1) + WS(rs, 2)]), T1l, ms, &(x[WS(vs, 1)]));
-		    ST(&(x[WS(vs, 7) + WS(rs, 2)]), T1m, ms, &(x[WS(vs, 7)]));
-	       }
-	       {
-		    V T3t, T3u, T3n, T3s;
-		    T3n = VFMA(LDK(KP707106781), T3m, T3f);
-		    T3s = VFNMS(LDK(KP707106781), T3r, T3q);
-		    T3t = BYTWJ(&(W[0]), VFNMSI(T3s, T3n));
-		    T3u = BYTWJ(&(W[TWVL * 12]), VFMAI(T3s, T3n));
-		    ST(&(x[WS(vs, 1) + WS(rs, 6)]), T3t, ms, &(x[WS(vs, 1)]));
-		    ST(&(x[WS(vs, 7) + WS(rs, 6)]), T3u, ms, &(x[WS(vs, 7)]));
-	       }
-	       {
-		    V T2W, T2X, T2Q, T2V;
-		    T2Q = VFMA(LDK(KP707106781), T2P, T2I);
-		    T2V = VFNMS(LDK(KP707106781), T2U, T2T);
-		    T2W = BYTWJ(&(W[0]), VFNMSI(T2V, T2Q));
-		    T2X = BYTWJ(&(W[TWVL * 12]), VFMAI(T2V, T2Q));
-		    ST(&(x[WS(vs, 1) + WS(rs, 5)]), T2W, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 7) + WS(rs, 5)]), T2X, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-	       }
-	       {
-		    V T1p, T1q, T1n, T1o;
-		    T1n = VFNMS(LDK(KP707106781), T1e, T17);
-		    T1o = VFMA(LDK(KP707106781), T1j, T1i);
-		    T1p = BYTWJ(&(W[TWVL * 8]), VFNMSI(T1o, T1n));
-		    T1q = BYTWJ(&(W[TWVL * 4]), VFMAI(T1o, T1n));
-		    ST(&(x[WS(vs, 5) + WS(rs, 2)]), T1p, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 2)]), T1q, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V Tl, Tm, Tj, Tk;
-		    Tj = VFNMS(LDK(KP707106781), Ta, T3);
-		    Tk = VFMA(LDK(KP707106781), Tf, Te);
-		    Tl = BYTWJ(&(W[TWVL * 8]), VFNMSI(Tk, Tj));
-		    Tm = BYTWJ(&(W[TWVL * 4]), VFMAI(Tk, Tj));
-		    ST(&(x[WS(vs, 5)]), Tl, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3)]), Tm, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V T2t, T2u, T2r, T2s;
-		    T2r = VFNMS(LDK(KP707106781), T2i, T2b);
-		    T2s = VFMA(LDK(KP707106781), T2n, T2m);
-		    T2t = BYTWJ(&(W[TWVL * 8]), VFNMSI(T2s, T2r));
-		    T2u = BYTWJ(&(W[TWVL * 4]), VFMAI(T2s, T2r));
-		    ST(&(x[WS(vs, 5) + WS(rs, 4)]), T2t, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 4)]), T2u, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V T3x, T3y, T3v, T3w;
-		    T3v = VFNMS(LDK(KP707106781), T3m, T3f);
-		    T3w = VFMA(LDK(KP707106781), T3r, T3q);
-		    T3x = BYTWJ(&(W[TWVL * 8]), VFNMSI(T3w, T3v));
-		    T3y = BYTWJ(&(W[TWVL * 4]), VFMAI(T3w, T3v));
-		    ST(&(x[WS(vs, 5) + WS(rs, 6)]), T3x, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 6)]), T3y, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V TS, TT, TQ, TR;
-		    TQ = VFNMS(LDK(KP707106781), TH, TA);
-		    TR = VFMA(LDK(KP707106781), TM, TL);
-		    TS = BYTWJ(&(W[TWVL * 8]), VFNMSI(TR, TQ));
-		    TT = BYTWJ(&(W[TWVL * 4]), VFMAI(TR, TQ));
-		    ST(&(x[WS(vs, 5) + WS(rs, 1)]), TS, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 1)]), TT, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	       {
-		    V T1W, T1X, T1U, T1V;
-		    T1U = VFNMS(LDK(KP707106781), T1L, T1E);
-		    T1V = VFMA(LDK(KP707106781), T1Q, T1P);
-		    T1W = BYTWJ(&(W[TWVL * 8]), VFNMSI(T1V, T1U));
-		    T1X = BYTWJ(&(W[TWVL * 4]), VFMAI(T1V, T1U));
-		    ST(&(x[WS(vs, 5) + WS(rs, 3)]), T1W, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 3)]), T1X, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	       {
-		    V T30, T31, T2Y, T2Z;
-		    T2Y = VFNMS(LDK(KP707106781), T2P, T2I);
-		    T2Z = VFMA(LDK(KP707106781), T2U, T2T);
-		    T30 = BYTWJ(&(W[TWVL * 8]), VFNMSI(T2Z, T2Y));
-		    T31 = BYTWJ(&(W[TWVL * 4]), VFMAI(T2Z, T2Y));
-		    ST(&(x[WS(vs, 5) + WS(rs, 5)]), T30, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 5)]), T31, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	       {
-		    V T44, T45, T42, T43;
-		    T42 = VFNMS(LDK(KP707106781), T3T, T3M);
-		    T43 = VFMA(LDK(KP707106781), T3Y, T3X);
-		    T44 = BYTWJ(&(W[TWVL * 8]), VFNMSI(T43, T42));
-		    T45 = BYTWJ(&(W[TWVL * 4]), VFMAI(T43, T42));
-		    ST(&(x[WS(vs, 5) + WS(rs, 7)]), T44, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 7)]), T45, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	  }
+      INT m;
+      R *x;
+      x = ri;
+      for (m = mb, W = W + (mb * ((TWVL / VL) * 14)); m < me; m = m + VL, x = x + (VL * ms), W = W + (TWVL * 14), MAKE_VOLATILE_STRIDE(16, rs), MAKE_VOLATILE_STRIDE(16, vs)) {
+           V T3, Tu, Te, Tp, T1E, T25, T1P, T20, T2b, T2C, T2m, T2x, T3M, T4d, T3X;
+           V T48, TA, T11, TL, TW, T17, T1y, T1i, T1t, T2I, T39, T2T, T34, T3f, T3G;
+           V T3q, T3B, Ta, Tv, Tf, Ts, T1L, T26, T1Q, T23, T2i, T2D, T2n, T2A, T3T;
+           V T4e, T3Y, T4b, TH, T12, TM, TZ, T1e, T1z, T1j, T1w, T2P, T3a, T2U, T37;
+           V T3m, T3H, T3r, T3E, T28, T14;
+           {
+            V T1, T2, Tn, Tc, Td, To;
+            T1 = LD(&(x[0]), ms, &(x[0]));
+            T2 = LD(&(x[WS(rs, 4)]), ms, &(x[0]));
+            Tn = VADD(T1, T2);
+            Tc = LD(&(x[WS(rs, 2)]), ms, &(x[0]));
+            Td = LD(&(x[WS(rs, 6)]), ms, &(x[0]));
+            To = VADD(Tc, Td);
+            T3 = VSUB(T1, T2);
+            Tu = VSUB(Tn, To);
+            Te = VSUB(Tc, Td);
+            Tp = VADD(Tn, To);
+           }
+           {
+            V T1C, T1D, T1Y, T1N, T1O, T1Z;
+            T1C = LD(&(x[WS(vs, 3)]), ms, &(x[WS(vs, 3)]));
+            T1D = LD(&(x[WS(vs, 3) + WS(rs, 4)]), ms, &(x[WS(vs, 3)]));
+            T1Y = VADD(T1C, T1D);
+            T1N = LD(&(x[WS(vs, 3) + WS(rs, 2)]), ms, &(x[WS(vs, 3)]));
+            T1O = LD(&(x[WS(vs, 3) + WS(rs, 6)]), ms, &(x[WS(vs, 3)]));
+            T1Z = VADD(T1N, T1O);
+            T1E = VSUB(T1C, T1D);
+            T25 = VSUB(T1Y, T1Z);
+            T1P = VSUB(T1N, T1O);
+            T20 = VADD(T1Y, T1Z);
+           }
+           {
+            V T29, T2a, T2v, T2k, T2l, T2w;
+            T29 = LD(&(x[WS(vs, 4)]), ms, &(x[WS(vs, 4)]));
+            T2a = LD(&(x[WS(vs, 4) + WS(rs, 4)]), ms, &(x[WS(vs, 4)]));
+            T2v = VADD(T29, T2a);
+            T2k = LD(&(x[WS(vs, 4) + WS(rs, 2)]), ms, &(x[WS(vs, 4)]));
+            T2l = LD(&(x[WS(vs, 4) + WS(rs, 6)]), ms, &(x[WS(vs, 4)]));
+            T2w = VADD(T2k, T2l);
+            T2b = VSUB(T29, T2a);
+            T2C = VSUB(T2v, T2w);
+            T2m = VSUB(T2k, T2l);
+            T2x = VADD(T2v, T2w);
+           }
+           {
+            V T3K, T3L, T46, T3V, T3W, T47;
+            T3K = LD(&(x[WS(vs, 7)]), ms, &(x[WS(vs, 7)]));
+            T3L = LD(&(x[WS(vs, 7) + WS(rs, 4)]), ms, &(x[WS(vs, 7)]));
+            T46 = VADD(T3K, T3L);
+            T3V = LD(&(x[WS(vs, 7) + WS(rs, 2)]), ms, &(x[WS(vs, 7)]));
+            T3W = LD(&(x[WS(vs, 7) + WS(rs, 6)]), ms, &(x[WS(vs, 7)]));
+            T47 = VADD(T3V, T3W);
+            T3M = VSUB(T3K, T3L);
+            T4d = VSUB(T46, T47);
+            T3X = VSUB(T3V, T3W);
+            T48 = VADD(T46, T47);
+           }
+           {
+            V Ty, Tz, TU, TJ, TK, TV;
+            Ty = LD(&(x[WS(vs, 1)]), ms, &(x[WS(vs, 1)]));
+            Tz = LD(&(x[WS(vs, 1) + WS(rs, 4)]), ms, &(x[WS(vs, 1)]));
+            TU = VADD(Ty, Tz);
+            TJ = LD(&(x[WS(vs, 1) + WS(rs, 2)]), ms, &(x[WS(vs, 1)]));
+            TK = LD(&(x[WS(vs, 1) + WS(rs, 6)]), ms, &(x[WS(vs, 1)]));
+            TV = VADD(TJ, TK);
+            TA = VSUB(Ty, Tz);
+            T11 = VSUB(TU, TV);
+            TL = VSUB(TJ, TK);
+            TW = VADD(TU, TV);
+           }
+           {
+            V T15, T16, T1r, T1g, T1h, T1s;
+            T15 = LD(&(x[WS(vs, 2)]), ms, &(x[WS(vs, 2)]));
+            T16 = LD(&(x[WS(vs, 2) + WS(rs, 4)]), ms, &(x[WS(vs, 2)]));
+            T1r = VADD(T15, T16);
+            T1g = LD(&(x[WS(vs, 2) + WS(rs, 2)]), ms, &(x[WS(vs, 2)]));
+            T1h = LD(&(x[WS(vs, 2) + WS(rs, 6)]), ms, &(x[WS(vs, 2)]));
+            T1s = VADD(T1g, T1h);
+            T17 = VSUB(T15, T16);
+            T1y = VSUB(T1r, T1s);
+            T1i = VSUB(T1g, T1h);
+            T1t = VADD(T1r, T1s);
+           }
+           {
+            V T2G, T2H, T32, T2R, T2S, T33;
+            T2G = LD(&(x[WS(vs, 5)]), ms, &(x[WS(vs, 5)]));
+            T2H = LD(&(x[WS(vs, 5) + WS(rs, 4)]), ms, &(x[WS(vs, 5)]));
+            T32 = VADD(T2G, T2H);
+            T2R = LD(&(x[WS(vs, 5) + WS(rs, 2)]), ms, &(x[WS(vs, 5)]));
+            T2S = LD(&(x[WS(vs, 5) + WS(rs, 6)]), ms, &(x[WS(vs, 5)]));
+            T33 = VADD(T2R, T2S);
+            T2I = VSUB(T2G, T2H);
+            T39 = VSUB(T32, T33);
+            T2T = VSUB(T2R, T2S);
+            T34 = VADD(T32, T33);
+           }
+           {
+            V T3d, T3e, T3z, T3o, T3p, T3A;
+            T3d = LD(&(x[WS(vs, 6)]), ms, &(x[WS(vs, 6)]));
+            T3e = LD(&(x[WS(vs, 6) + WS(rs, 4)]), ms, &(x[WS(vs, 6)]));
+            T3z = VADD(T3d, T3e);
+            T3o = LD(&(x[WS(vs, 6) + WS(rs, 2)]), ms, &(x[WS(vs, 6)]));
+            T3p = LD(&(x[WS(vs, 6) + WS(rs, 6)]), ms, &(x[WS(vs, 6)]));
+            T3A = VADD(T3o, T3p);
+            T3f = VSUB(T3d, T3e);
+            T3G = VSUB(T3z, T3A);
+            T3q = VSUB(T3o, T3p);
+            T3B = VADD(T3z, T3A);
+           }
+           {
+            V T6, Tq, T9, Tr;
+            {
+             V T4, T5, T7, T8;
+             T4 = LD(&(x[WS(rs, 1)]), ms, &(x[WS(rs, 1)]));
+             T5 = LD(&(x[WS(rs, 5)]), ms, &(x[WS(rs, 1)]));
+             T6 = VSUB(T4, T5);
+             Tq = VADD(T4, T5);
+             T7 = LD(&(x[WS(rs, 7)]), ms, &(x[WS(rs, 1)]));
+             T8 = LD(&(x[WS(rs, 3)]), ms, &(x[WS(rs, 1)]));
+             T9 = VSUB(T7, T8);
+             Tr = VADD(T7, T8);
+            }
+            Ta = VADD(T6, T9);
+            Tv = VSUB(Tr, Tq);
+            Tf = VSUB(T9, T6);
+            Ts = VADD(Tq, Tr);
+           }
+           {
+            V T1H, T21, T1K, T22;
+            {
+             V T1F, T1G, T1I, T1J;
+             T1F = LD(&(x[WS(vs, 3) + WS(rs, 1)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1G = LD(&(x[WS(vs, 3) + WS(rs, 5)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1H = VSUB(T1F, T1G);
+             T21 = VADD(T1F, T1G);
+             T1I = LD(&(x[WS(vs, 3) + WS(rs, 7)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1J = LD(&(x[WS(vs, 3) + WS(rs, 3)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1K = VSUB(T1I, T1J);
+             T22 = VADD(T1I, T1J);
+            }
+            T1L = VADD(T1H, T1K);
+            T26 = VSUB(T22, T21);
+            T1Q = VSUB(T1K, T1H);
+            T23 = VADD(T21, T22);
+           }
+           {
+            V T2e, T2y, T2h, T2z;
+            {
+             V T2c, T2d, T2f, T2g;
+             T2c = LD(&(x[WS(vs, 4) + WS(rs, 1)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2d = LD(&(x[WS(vs, 4) + WS(rs, 5)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2e = VSUB(T2c, T2d);
+             T2y = VADD(T2c, T2d);
+             T2f = LD(&(x[WS(vs, 4) + WS(rs, 7)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2g = LD(&(x[WS(vs, 4) + WS(rs, 3)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2h = VSUB(T2f, T2g);
+             T2z = VADD(T2f, T2g);
+            }
+            T2i = VADD(T2e, T2h);
+            T2D = VSUB(T2z, T2y);
+            T2n = VSUB(T2h, T2e);
+            T2A = VADD(T2y, T2z);
+           }
+           {
+            V T3P, T49, T3S, T4a;
+            {
+             V T3N, T3O, T3Q, T3R;
+             T3N = LD(&(x[WS(vs, 7) + WS(rs, 1)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3O = LD(&(x[WS(vs, 7) + WS(rs, 5)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3P = VSUB(T3N, T3O);
+             T49 = VADD(T3N, T3O);
+             T3Q = LD(&(x[WS(vs, 7) + WS(rs, 7)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3R = LD(&(x[WS(vs, 7) + WS(rs, 3)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3S = VSUB(T3Q, T3R);
+             T4a = VADD(T3Q, T3R);
+            }
+            T3T = VADD(T3P, T3S);
+            T4e = VSUB(T4a, T49);
+            T3Y = VSUB(T3S, T3P);
+            T4b = VADD(T49, T4a);
+           }
+           {
+            V TD, TX, TG, TY;
+            {
+             V TB, TC, TE, TF;
+             TB = LD(&(x[WS(vs, 1) + WS(rs, 1)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TC = LD(&(x[WS(vs, 1) + WS(rs, 5)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TD = VSUB(TB, TC);
+             TX = VADD(TB, TC);
+             TE = LD(&(x[WS(vs, 1) + WS(rs, 7)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TF = LD(&(x[WS(vs, 1) + WS(rs, 3)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TG = VSUB(TE, TF);
+             TY = VADD(TE, TF);
+            }
+            TH = VADD(TD, TG);
+            T12 = VSUB(TY, TX);
+            TM = VSUB(TG, TD);
+            TZ = VADD(TX, TY);
+           }
+           {
+            V T1a, T1u, T1d, T1v;
+            {
+             V T18, T19, T1b, T1c;
+             T18 = LD(&(x[WS(vs, 2) + WS(rs, 1)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T19 = LD(&(x[WS(vs, 2) + WS(rs, 5)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T1a = VSUB(T18, T19);
+             T1u = VADD(T18, T19);
+             T1b = LD(&(x[WS(vs, 2) + WS(rs, 7)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T1c = LD(&(x[WS(vs, 2) + WS(rs, 3)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T1d = VSUB(T1b, T1c);
+             T1v = VADD(T1b, T1c);
+            }
+            T1e = VADD(T1a, T1d);
+            T1z = VSUB(T1v, T1u);
+            T1j = VSUB(T1d, T1a);
+            T1w = VADD(T1u, T1v);
+           }
+           {
+            V T2L, T35, T2O, T36;
+            {
+             V T2J, T2K, T2M, T2N;
+             T2J = LD(&(x[WS(vs, 5) + WS(rs, 1)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2K = LD(&(x[WS(vs, 5) + WS(rs, 5)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2L = VSUB(T2J, T2K);
+             T35 = VADD(T2J, T2K);
+             T2M = LD(&(x[WS(vs, 5) + WS(rs, 7)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2N = LD(&(x[WS(vs, 5) + WS(rs, 3)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2O = VSUB(T2M, T2N);
+             T36 = VADD(T2M, T2N);
+            }
+            T2P = VADD(T2L, T2O);
+            T3a = VSUB(T36, T35);
+            T2U = VSUB(T2O, T2L);
+            T37 = VADD(T35, T36);
+           }
+           {
+            V T3i, T3C, T3l, T3D;
+            {
+             V T3g, T3h, T3j, T3k;
+             T3g = LD(&(x[WS(vs, 6) + WS(rs, 1)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3h = LD(&(x[WS(vs, 6) + WS(rs, 5)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3i = VSUB(T3g, T3h);
+             T3C = VADD(T3g, T3h);
+             T3j = LD(&(x[WS(vs, 6) + WS(rs, 7)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3k = LD(&(x[WS(vs, 6) + WS(rs, 3)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3l = VSUB(T3j, T3k);
+             T3D = VADD(T3j, T3k);
+            }
+            T3m = VADD(T3i, T3l);
+            T3H = VSUB(T3D, T3C);
+            T3r = VSUB(T3l, T3i);
+            T3E = VADD(T3C, T3D);
+           }
+           ////////////////////////////////////////
+           ST(&(x[0]), VADD(Tp, Ts), ms, &(x[0]));
+           ST(&(x[WS(rs, 2)]), VADD(T1t, T1w), ms, &(x[0]));
+           ST(&(x[WS(rs, 5)]), VADD(T34, T37), ms, &(x[WS(rs, 1)]));
+           ST(&(x[WS(rs, 7)]), VADD(T48, T4b), ms, &(x[WS(rs, 1)]));
+           ST(&(x[WS(rs, 6)]), VADD(T3B, T3E), ms, &(x[0]));
+           ST(&(x[WS(rs, 4)]), VADD(T2x, T2A), ms, &(x[0]));
+           {
+            V Tt, T4c, T2B, T24;
+            ST(&(x[WS(rs, 3)]), VADD(T20, T23), ms, &(x[WS(rs, 1)]));
+            ST(&(x[WS(rs, 1)]), VADD(TW, TZ), ms, &(x[WS(rs, 1)]));
+            Tt = BYTWJ(&(W[TWVL * 6]), VSUB(Tp, Ts));
+            ST(&(x[WS(vs, 4)]), Tt, ms, &(x[WS(vs, 4)]));
+            T4c = BYTWJ(&(W[TWVL * 6]), VSUB(T48, T4b));
+            ST(&(x[WS(vs, 4) + WS(rs, 7)]), T4c, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            T2B = BYTWJ(&(W[TWVL * 6]), VSUB(T2x, T2A));
+            ST(&(x[WS(vs, 4) + WS(rs, 4)]), T2B, ms, &(x[WS(vs, 4)]));
+            T24 = BYTWJ(&(W[TWVL * 6]), VSUB(T20, T23));
+            ST(&(x[WS(vs, 4) + WS(rs, 3)]), T24, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+           }
+           {
+            V T10, T1x, T3F, T38, T1A, Tw;
+            T10 = BYTWJ(&(W[TWVL * 6]), VSUB(TW, TZ));
+            ST(&(x[WS(vs, 4) + WS(rs, 1)]), T10, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            T1x = BYTWJ(&(W[TWVL * 6]), VSUB(T1t, T1w));
+            ST(&(x[WS(vs, 4) + WS(rs, 2)]), T1x, ms, &(x[WS(vs, 4)]));
+            T3F = BYTWJ(&(W[TWVL * 6]), VSUB(T3B, T3E));
+            ST(&(x[WS(vs, 4) + WS(rs, 6)]), T3F, ms, &(x[WS(vs, 4)]));
+            T38 = BYTWJ(&(W[TWVL * 6]), VSUB(T34, T37));
+            ST(&(x[WS(vs, 4) + WS(rs, 5)]), T38, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            T1A = BYTWJ(&(W[TWVL * 10]), VFNMSI(T1z, T1y));
+            ST(&(x[WS(vs, 6) + WS(rs, 2)]), T1A, ms, &(x[WS(vs, 6)]));
+            Tw = BYTWJ(&(W[TWVL * 10]), VFNMSI(Tv, Tu));
+            ST(&(x[WS(vs, 6)]), Tw, ms, &(x[WS(vs, 6)]));
+           }
+           {
+            V T2E, T3I, T13, T27, T3b, T4f;
+            T2E = BYTWJ(&(W[TWVL * 10]), VFNMSI(T2D, T2C));
+            ST(&(x[WS(vs, 6) + WS(rs, 4)]), T2E, ms, &(x[WS(vs, 6)]));
+            T3I = BYTWJ(&(W[TWVL * 10]), VFNMSI(T3H, T3G));
+            ST(&(x[WS(vs, 6) + WS(rs, 6)]), T3I, ms, &(x[WS(vs, 6)]));
+            T13 = BYTWJ(&(W[TWVL * 10]), VFNMSI(T12, T11));
+            ST(&(x[WS(vs, 6) + WS(rs, 1)]), T13, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            T27 = BYTWJ(&(W[TWVL * 10]), VFNMSI(T26, T25));
+            ST(&(x[WS(vs, 6) + WS(rs, 3)]), T27, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            T3b = BYTWJ(&(W[TWVL * 10]), VFNMSI(T3a, T39));
+            ST(&(x[WS(vs, 6) + WS(rs, 5)]), T3b, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            T4f = BYTWJ(&(W[TWVL * 10]), VFNMSI(T4e, T4d));
+            ST(&(x[WS(vs, 6) + WS(rs, 7)]), T4f, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+           }
+           {
+            V Tx, T1B, T3c, T4g, T3J, T2F;
+            Tx = BYTWJ(&(W[TWVL * 2]), VFMAI(Tv, Tu));
+            ST(&(x[WS(vs, 2)]), Tx, ms, &(x[WS(vs, 2)]));
+            T1B = BYTWJ(&(W[TWVL * 2]), VFMAI(T1z, T1y));
+            ST(&(x[WS(vs, 2) + WS(rs, 2)]), T1B, ms, &(x[WS(vs, 2)]));
+            T3c = BYTWJ(&(W[TWVL * 2]), VFMAI(T3a, T39));
+            ST(&(x[WS(vs, 2) + WS(rs, 5)]), T3c, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            T4g = BYTWJ(&(W[TWVL * 2]), VFMAI(T4e, T4d));
+            ST(&(x[WS(vs, 2) + WS(rs, 7)]), T4g, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            T3J = BYTWJ(&(W[TWVL * 2]), VFMAI(T3H, T3G));
+            ST(&(x[WS(vs, 2) + WS(rs, 6)]), T3J, ms, &(x[WS(vs, 2)]));
+            T2F = BYTWJ(&(W[TWVL * 2]), VFMAI(T2D, T2C));
+            ST(&(x[WS(vs, 2) + WS(rs, 4)]), T2F, ms, &(x[WS(vs, 2)]));
+           }
+           T28 = BYTWJ(&(W[TWVL * 2]), VFMAI(T26, T25));
+           ST(&(x[WS(vs, 2) + WS(rs, 3)]), T28, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+           T14 = BYTWJ(&(W[TWVL * 2]), VFMAI(T12, T11));
+           ST(&(x[WS(vs, 2) + WS(rs, 1)]), T14, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+           {
+            V Th, Ti, Tb, Tg;
+            Tb = VFMA(LDK(KP707106781), Ta, T3);
+            Tg = VFNMS(LDK(KP707106781), Tf, Te);
+            Th = BYTWJ(&(W[0]), VFNMSI(Tg, Tb));
+            Ti = BYTWJ(&(W[TWVL * 12]), VFMAI(Tg, Tb));
+            ST(&(x[WS(vs, 1)]), Th, ms, &(x[WS(vs, 1)]));
+            ST(&(x[WS(vs, 7)]), Ti, ms, &(x[WS(vs, 7)]));
+           }
+           {
+            V T40, T41, T3U, T3Z;
+            T3U = VFMA(LDK(KP707106781), T3T, T3M);
+            T3Z = VFNMS(LDK(KP707106781), T3Y, T3X);
+            T40 = BYTWJ(&(W[0]), VFNMSI(T3Z, T3U));
+            T41 = BYTWJ(&(W[TWVL * 12]), VFMAI(T3Z, T3U));
+            ST(&(x[WS(vs, 1) + WS(rs, 7)]), T40, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 7)]), T41, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+           }
+           {
+            V T2p, T2q, T2j, T2o;
+            T2j = VFMA(LDK(KP707106781), T2i, T2b);
+            T2o = VFNMS(LDK(KP707106781), T2n, T2m);
+            T2p = BYTWJ(&(W[0]), VFNMSI(T2o, T2j));
+            T2q = BYTWJ(&(W[TWVL * 12]), VFMAI(T2o, T2j));
+            ST(&(x[WS(vs, 1) + WS(rs, 4)]), T2p, ms, &(x[WS(vs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 4)]), T2q, ms, &(x[WS(vs, 7)]));
+           }
+           {
+            V T1S, T1T, T1M, T1R;
+            T1M = VFMA(LDK(KP707106781), T1L, T1E);
+            T1R = VFNMS(LDK(KP707106781), T1Q, T1P);
+            T1S = BYTWJ(&(W[0]), VFNMSI(T1R, T1M));
+            T1T = BYTWJ(&(W[TWVL * 12]), VFMAI(T1R, T1M));
+            ST(&(x[WS(vs, 1) + WS(rs, 3)]), T1S, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 3)]), T1T, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+           }
+           {
+            V TO, TP, TI, TN;
+            TI = VFMA(LDK(KP707106781), TH, TA);
+            TN = VFNMS(LDK(KP707106781), TM, TL);
+            TO = BYTWJ(&(W[0]), VFNMSI(TN, TI));
+            TP = BYTWJ(&(W[TWVL * 12]), VFMAI(TN, TI));
+            ST(&(x[WS(vs, 1) + WS(rs, 1)]), TO, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 1)]), TP, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+           }
+           {
+            V T1l, T1m, T1f, T1k;
+            T1f = VFMA(LDK(KP707106781), T1e, T17);
+            T1k = VFNMS(LDK(KP707106781), T1j, T1i);
+            T1l = BYTWJ(&(W[0]), VFNMSI(T1k, T1f));
+            T1m = BYTWJ(&(W[TWVL * 12]), VFMAI(T1k, T1f));
+            ST(&(x[WS(vs, 1) + WS(rs, 2)]), T1l, ms, &(x[WS(vs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 2)]), T1m, ms, &(x[WS(vs, 7)]));
+           }
+           {
+            V T3t, T3u, T3n, T3s;
+            T3n = VFMA(LDK(KP707106781), T3m, T3f);
+            T3s = VFNMS(LDK(KP707106781), T3r, T3q);
+            T3t = BYTWJ(&(W[0]), VFNMSI(T3s, T3n));
+            T3u = BYTWJ(&(W[TWVL * 12]), VFMAI(T3s, T3n));
+            ST(&(x[WS(vs, 1) + WS(rs, 6)]), T3t, ms, &(x[WS(vs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 6)]), T3u, ms, &(x[WS(vs, 7)]));
+           }
+           {
+            V T2W, T2X, T2Q, T2V;
+            T2Q = VFMA(LDK(KP707106781), T2P, T2I);
+            T2V = VFNMS(LDK(KP707106781), T2U, T2T);
+            T2W = BYTWJ(&(W[0]), VFNMSI(T2V, T2Q));
+            T2X = BYTWJ(&(W[TWVL * 12]), VFMAI(T2V, T2Q));
+            ST(&(x[WS(vs, 1) + WS(rs, 5)]), T2W, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 5)]), T2X, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+           }
+           {
+            V T1p, T1q, T1n, T1o;
+            T1n = VFNMS(LDK(KP707106781), T1e, T17);
+            T1o = VFMA(LDK(KP707106781), T1j, T1i);
+            T1p = BYTWJ(&(W[TWVL * 8]), VFNMSI(T1o, T1n));
+            T1q = BYTWJ(&(W[TWVL * 4]), VFMAI(T1o, T1n));
+            ST(&(x[WS(vs, 5) + WS(rs, 2)]), T1p, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 2)]), T1q, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V Tl, Tm, Tj, Tk;
+            Tj = VFNMS(LDK(KP707106781), Ta, T3);
+            Tk = VFMA(LDK(KP707106781), Tf, Te);
+            Tl = BYTWJ(&(W[TWVL * 8]), VFNMSI(Tk, Tj));
+            Tm = BYTWJ(&(W[TWVL * 4]), VFMAI(Tk, Tj));
+            ST(&(x[WS(vs, 5)]), Tl, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3)]), Tm, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V T2t, T2u, T2r, T2s;
+            T2r = VFNMS(LDK(KP707106781), T2i, T2b);
+            T2s = VFMA(LDK(KP707106781), T2n, T2m);
+            T2t = BYTWJ(&(W[TWVL * 8]), VFNMSI(T2s, T2r));
+            T2u = BYTWJ(&(W[TWVL * 4]), VFMAI(T2s, T2r));
+            ST(&(x[WS(vs, 5) + WS(rs, 4)]), T2t, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 4)]), T2u, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V T3x, T3y, T3v, T3w;
+            T3v = VFNMS(LDK(KP707106781), T3m, T3f);
+            T3w = VFMA(LDK(KP707106781), T3r, T3q);
+            T3x = BYTWJ(&(W[TWVL * 8]), VFNMSI(T3w, T3v));
+            T3y = BYTWJ(&(W[TWVL * 4]), VFMAI(T3w, T3v));
+            ST(&(x[WS(vs, 5) + WS(rs, 6)]), T3x, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 6)]), T3y, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V TS, TT, TQ, TR;
+            TQ = VFNMS(LDK(KP707106781), TH, TA);
+            TR = VFMA(LDK(KP707106781), TM, TL);
+            TS = BYTWJ(&(W[TWVL * 8]), VFNMSI(TR, TQ));
+            TT = BYTWJ(&(W[TWVL * 4]), VFMAI(TR, TQ));
+            ST(&(x[WS(vs, 5) + WS(rs, 1)]), TS, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 1)]), TT, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+           {
+            V T1W, T1X, T1U, T1V;
+            T1U = VFNMS(LDK(KP707106781), T1L, T1E);
+            T1V = VFMA(LDK(KP707106781), T1Q, T1P);
+            T1W = BYTWJ(&(W[TWVL * 8]), VFNMSI(T1V, T1U));
+            T1X = BYTWJ(&(W[TWVL * 4]), VFMAI(T1V, T1U));
+            ST(&(x[WS(vs, 5) + WS(rs, 3)]), T1W, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 3)]), T1X, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+           {
+            V T30, T31, T2Y, T2Z;
+            T2Y = VFNMS(LDK(KP707106781), T2P, T2I);
+            T2Z = VFMA(LDK(KP707106781), T2U, T2T);
+            T30 = BYTWJ(&(W[TWVL * 8]), VFNMSI(T2Z, T2Y));
+            T31 = BYTWJ(&(W[TWVL * 4]), VFMAI(T2Z, T2Y));
+            ST(&(x[WS(vs, 5) + WS(rs, 5)]), T30, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 5)]), T31, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+           {
+            V T44, T45, T42, T43;
+            T42 = VFNMS(LDK(KP707106781), T3T, T3M);
+            T43 = VFMA(LDK(KP707106781), T3Y, T3X);
+            T44 = BYTWJ(&(W[TWVL * 8]), VFNMSI(T43, T42));
+            T45 = BYTWJ(&(W[TWVL * 4]), VFMAI(T43, T42));
+            ST(&(x[WS(vs, 5) + WS(rs, 7)]), T44, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 7)]), T45, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+      }
      }
      VLEAVE();
 }
@@ -539,483 +541,1337 @@ void XSIMD(codelet_q1fv_8) (planner *p)
  */
 #include "dft/simd/q1f.h"
 
+#if !defined(FFTW_SINGLE) && defined(AMD_OPT_KERNEL_NEW_IMPLEMENTATION)
+static void q1fv_8(R *ri, R *ii, const R *W, stride rs, stride vs, INT mb, INT me, INT ms)
+{
+    V W1_2_8K;
+    R W1_2_8 = 0.70710678118654752440084436210485;
+    INT mn = WS(vs, 2);//should be 4 complex apart for double and 8 complex apart for single
+    INT mm, mt, w_ms, mn4, mn6;
+    R *x;
+    x = ri;
+
+    W1_2_8K = VBROADCASTS((const double *)&W1_2_8);
+    mn4 = (mn<<1);
+    mn6 = (mn * 3);
+
+    for (mm = mb, W = W + (mb * ((TWVL / VL) * 14)); mm < me; mm = mm + VL, x = x + (VL * ms), W = W + (TWVL * 14),MAKE_VOLATILE_STRIDE(16, rs), MAKE_VOLATILE_STRIDE(16, vs))
+    {
+        V a, c, d, e;
+        V f, g, h, i;
+        V j, k, l, m;
+        V a2, c2, d2, e2;
+        V f2, g2, h2, i2;
+        V a3, c3, d3, e3;
+        V f3, g3, h3, i3;
+        V a4, c4, d4, e4;
+        V f4, g4, h4, i4;
+        V a5, c5, d5, e5;
+        
+        w_ms = 0;   
+        for (mt = 0; mt <= ms; mt += ms)
+        {
+            //Stage-1
+            {
+                //0,4,2,6
+                f = LDA(x + mt, 0, 0);                  //0
+                g = LDA(x + WS(rs, 2) + mt, 0, 0);      //2
+                h = LDA(x + WS(rs, 4) + mt, 0, 0);      //4
+                i = LDA(x + WS(rs, 6) + mt, 0, 0);      //6
+
+                f2 = LDA(x + mt + mn, 0, 0);                //0+m
+                g2 = LDA(x + WS(rs, 2) + mt + mn, 0, 0);    //2+m
+                h2 = LDA(x + WS(rs, 4) + mt + mn, 0, 0);    //4+m
+                i2 = LDA(x + WS(rs, 6) + mt + mn, 0, 0);    //6+m
+
+                //first set
+                a = VADD(f, h);                     //0+4
+                c = VSUB(f, h);                     //0-4
+                d = VADD(g, i);                     //2+6
+                e = VSUB(g, i);                     //2-6
+
+                //second set
+                a2 = VADD(f2, h2);                  //0+4
+                c2 = VSUB(f2, h2);                  //0-4
+                d2 = VADD(g2, i2);                  //2+6
+                e2 = VSUB(g2, i2);                  //2-6
+
+                //1,5,3,7
+                j = LDA(x + WS(rs, 1) + mt, 0, 0);      //1
+                k = LDA(x + WS(rs, 3) + mt, 0, 0);      //3
+                l = LDA(x + WS(rs, 5) + mt, 0, 0);      //5
+                m = LDA(x + WS(rs, 7) + mt, 0, 0);      //7
+
+                f2 = LDA(x + WS(rs, 1) + mt + mn, 0, 0);    //1
+                g2 = LDA(x + WS(rs, 3) + mt + mn, 0, 0);    //3
+                h2 = LDA(x + WS(rs, 5) + mt + mn, 0, 0);    //5
+                i2 = LDA(x + WS(rs, 7) + mt + mn, 0, 0);    //7
+
+                //first set
+                f = VADD(j, l);                     //1+5
+                g = VSUB(j, l);                     //1-5
+                h = VADD(k, m);                     //3+7
+                i = VSUB(k, m);                     //3-7
+
+                //second set
+                j = VADD(f2, h2);                   //1+5
+                k = VSUB(f2, h2);                   //1-5
+                l = VADD(g2, i2);                   //3+7
+                m = VSUB(g2, i2);                   //3-7
+            }
+
+            //Stage-2
+            {
+                V tw1, tw2;
+
+                //second set
+                e2 = VCONJ(FLIP_RI(e2));
+
+                tw1 = VSHUF(k, k, 0x0);         //5th r
+                k = VSHUF(k, k, 0xF);           //5th i
+                tw1 = VCONJ(tw1);               //Towards Real=(r+i)W , Img=(i-r)*W
+                tw2 = VSHUF(m, m, 0xF);         //7th i
+                m = VSHUF(m, m, 0x0);           //7th r
+                tw2 = VCONJ(tw2);               //Towards Real=(i-r)W, Img=(-i-r)W
+                k = VADD(k, tw1);               //Towards Real=(r+i)W , Img=(i-r)*W
+                m = VSUB(tw2, m);               //Towards Real=(i-r)W, Img=(-i-r)W
+                k = VMUL(k, W1_2_8K);           //Towards Real=(r+i)W , Img=(i-r)*W
+                m = VMUL(m, W1_2_8K);           //Towards Real=(i-r)W, Img=(-i-r)W
+
+                //first set
+                e = VCONJ(FLIP_RI(e));
+
+                tw1 = VSHUF(g, g, 0x0);         //5th r
+                g = VSHUF(g, g, 0xF);           //5th i
+                tw1 = VCONJ(tw1);               //Towards Real=(r+i)W , Img=(i-r)*W
+                tw2 = VSHUF(i, i, 0xF);         //7th i
+                i = VSHUF(i, i, 0x0);           //7th r
+                tw2 = VCONJ(tw2);               //Towards Real=(i-r)W, Img=(-i-r)W
+                g = VADD(g, tw1);               //Towards Real=(r+i)W , Img=(i-r)*W
+                i = VSUB(tw2, i);               //Towards Real=(i-r)W, Img=(-i-r)W
+                g = VMUL(g, W1_2_8K);           //Towards Real=(r+i)W , Img=(i-r)*W
+                i = VMUL(i, W1_2_8K);           //Towards Real=(i-r)W, Img=(-i-r)W
+
+                ////////// Next 8 - 15 (Stage-1) starts //////////
+                //0,4,2,6
+                f3 = LDA(x + mt + mn4, 0, 0);                       //0
+                g3 = LDA(x + WS(rs, 2) + mt + mn4, 0, 0);       //2
+                h3 = LDA(x + WS(rs, 4) + mt + mn4, 0, 0);       //4
+                i3 = LDA(x + WS(rs, 6) + mt + mn4, 0, 0);       //6
+
+                f4 = LDA(x + mt + mn6, 0, 0);                   //0+m
+                g4 = LDA(x + WS(rs, 2) + mt + mn6, 0, 0);       //2+m
+                h4 = LDA(x + WS(rs, 4) + mt + mn6, 0, 0);       //4+m
+                i4 = LDA(x + WS(rs, 6) + mt + mn6, 0, 0);       //6+m
+                ///////// Next 8 - 15 ends ///////////
+
+                //second set
+                f2 = VADD(j, l);                    //1+5
+                h2 = VSUB(j, l);                    //3+7
+                g2 = VADD(k, m);                    //1-5
+                i2 = VSUB(k, m);                    //3-7
+
+                //first set
+                j = VADD(a, d);                 //0+4
+                l = VSUB(a, d);                 //2+6
+                k = VADD(c, e);                 //0-4
+                m = VSUB(c, e);                 //2-6
+
+                ////////// Next 8 - 15 (Stage-1) starts //////////
+                //first set
+                a3 = VADD(f3, h3);                      //0+4
+                c3 = VSUB(f3, h3);                      //0-4
+                d3 = VADD(g3, i3);                      //2+6
+                e3 = VSUB(g3, i3);                      //2-6
+
+                //second set
+                a4 = VADD(f4, h4);                  //0+4
+                c4 = VSUB(f4, h4);                  //0-4
+                d4 = VADD(g4, i4);                  //2+6
+                e4 = VSUB(g4, i4);                  //2-6
+
+                //1,5,3,7
+                f3 = LDA(x + WS(rs, 1) + mt + mn4, 0, 0);           //1
+                g3 = LDA(x + WS(rs, 3) + mt + mn4, 0, 0);   //3
+                h3 = LDA(x + WS(rs, 5) + mt + mn4, 0, 0);   //5
+                i3 = LDA(x + WS(rs, 7) + mt + mn4, 0, 0);   //7
+
+                f4 = LDA(x + WS(rs, 1) + mt + mn6, 0, 0);           //1
+                g4 = LDA(x + WS(rs, 3) + mt + mn6, 0, 0);   //3
+                h4 = LDA(x + WS(rs, 5) + mt + mn6, 0, 0);   //5
+                i4 = LDA(x + WS(rs, 7) + mt + mn6, 0, 0);   //7
+                ///////// Next 8 - 15 ends ///////////
+
+                //first set
+                a = VADD(f, h);                 //1+5
+                d = VSUB(f, h);                 //3+7
+                c = VADD(g, i);                 //1-5
+                e = VSUB(g, i);                 //3-7
+
+                //second set
+                f = VADD(a2, d2);                   //0+4
+                h = VSUB(a2, d2);                   //2+6
+                g = VADD(c2, e2);                   //0-4
+                i = VSUB(c2, e2);                   //2-6
+
+                ////////// Next 8 - 15 (Stage-1) starts //////////
+                //first set
+                a2 = VADD(f3, h3);                  //1+5
+                c2 = VSUB(f3, h3);                  //1-5
+                d2 = VADD(g3, i3);                  //3+7
+                e2 = VSUB(g3, i3);                  //3-7
+
+                //second set
+                f3 = VADD(f4, h4);                  //1+5
+                g3 = VSUB(f4, h4);                  //1-5
+                h3 = VADD(g4, i4);                  //3+7
+                i3 = VSUB(g4, i4);                  //3-7
+                ///////// Next 8 - 15 ends ///////////
+            }
+
+            //Stage-3
+            {
+                V tw1, tw2;
+
+                d = VCONJ(FLIP_RI(d));
+                e = VCONJ(FLIP_RI(e));
+                h2 = VCONJ(FLIP_RI(h2));
+                i2 = VCONJ(FLIP_RI(i2));
+
+                ////////// Next 8 - 15 (Stage-2) starts //////////
+                //second set
+                e4 = VCONJ(FLIP_RI(e4));
+
+                tw1 = VSHUF(g3, g3, 0x0);           //5th r
+                g3 = VSHUF(g3, g3, 0xF);            //5th i
+                tw1 = VCONJ(tw1);               //Towards Real=(r+i)W , Img=(i-r)*W
+                tw2 = VSHUF(i3, i3, 0xF);           //7th i
+                i3 = VSHUF(i3, i3, 0x0);            //7th r
+                tw2 = VCONJ(tw2);               //Towards Real=(i-r)W, Img=(-i-r)W
+                g3 = VADD(g3, tw1);             //Towards Real=(r+i)W , Img=(i-r)*W
+                i3 = VSUB(tw2, i3);             //Towards Real=(i-r)W, Img=(-i-r)W
+                g3 = VMUL(g3, W1_2_8K);         //Towards Real=(r+i)W , Img=(i-r)*W
+                i3 = VMUL(i3, W1_2_8K);         //Towards Real=(i-r)W, Img=(-i-r)W
+
+                //first set
+                e3 = VCONJ(FLIP_RI(e3));
+
+                tw1 = VSHUF(c2, c2, 0x0);           //5th r
+                c2 = VSHUF(c2, c2, 0xF);            //5th i
+                tw1 = VCONJ(tw1);               //Towards Real=(r+i)W , Img=(i-r)*W
+                tw2 = VSHUF(e2, e2, 0xF);           //7th i
+                e2 = VSHUF(e2, e2, 0x0);            //7th r
+                tw2 = VCONJ(tw2);               //Towards Real=(i-r)W, Img=(-i-r)W
+                c2 = VADD(c2, tw1);             //Towards Real=(r+i)W , Img=(i-r)*W
+                e2 = VSUB(tw2, e2);             //Towards Real=(i-r)W, Img=(-i-r)W
+                c2 = VMUL(c2, W1_2_8K);         //Towards Real=(r+i)W , Img=(i-r)*W
+                e2 = VMUL(e2, W1_2_8K);         //Towards Real=(i-r)W, Img=(-i-r)W
+                ///////// Next 8 - 15 ends ///////////
+
+                ///// Shuffle operations before stores : Start /////
+                //vs:0,1
+                f4 = VADD(j, a);    //vs:0,1 ; rs:0
+                g4 = VADD(k, c);    //vs:0,1 ; rs:1
+                h4 = VADD(l, d);    //vs:0,1 ; rs:2
+                i4 = VADD(m, e);    //vs:0,1 ; rs:3
+                j = VSUB(j, a);     //vs:0,1 ; rs:4
+                k = VSUB(k, c);     //vs:0,1 ; rs:5
+                l = VSUB(l, d);     //vs:0,1 ; rs:6
+                m = VSUB(m, e);     //vs:0,1 ; rs:7
+
+                //Multiply additional next stage Twiddle Factors for vs:0,1
+                g4 = BYTWJB(&(W[0+w_ms]), g4);
+                h4 = BYTWJB(&(W[(TWVL * 2)+w_ms]), h4);
+                i4 = BYTWJB(&(W[(TWVL * 4)+w_ms]), i4);
+                j = BYTWJB(&(W[(TWVL * 6)+w_ms]), j);
+                k = BYTWJB(&(W[(TWVL * 8)+w_ms]), k);
+                l = BYTWJB(&(W[(TWVL * 10)+w_ms]), l);
+                m = BYTWJB(&(W[(TWVL * 12)+w_ms]), m);
+
+                ///// Shuffle operations before stores : Start /////
+
+                ////////// Next 8 - 15 (Stage-2) starts //////////
+                //second set
+                a = VADD(a4, d4);                   //0+4
+                d = VSUB(a4, d4);                   //2+6
+                c = VADD(c4, e4);                   //0-4
+                e = VSUB(c4, e4);                   //2-6
+
+                //second set
+                a4 = VADD(f3, h3);                  //1+5
+                d4 = VSUB(f3, h3);                  //3+7
+                c4 = VADD(g3, i3);                  //1-5
+                e4 = VSUB(g3, i3);                  //3-7
+
+                a5 = SHUF_CROSS_LANE_1(f4, g4);     //vs:0,0 ; rs:0,1
+                c5 = SHUF_CROSS_LANE_2(f4, g4);     //vs:1,1 ; rs:0,1
+                d5 = SHUF_CROSS_LANE_1(h4, i4);     //vs:0,0 ; rs:2,3
+                e5 = SHUF_CROSS_LANE_2(h4, i4);     //vs:1,1 ; rs:2,3
+                f4 = SHUF_CROSS_LANE_1(j, k);       //vs:0,0 ; rs:4,5
+                g4 = SHUF_CROSS_LANE_2(j, k);       //vs:1,1 ; rs:4,5
+                h4 = SHUF_CROSS_LANE_1(l, m);       //vs:0,0 ; rs:6,7
+                i4 = SHUF_CROSS_LANE_2(l, m);       //vs:1,1 ; rs:6,7
+
+                //first set
+                f3 = VADD(a3, d3);                  //0+4
+                h3 = VSUB(a3, d3);                  //2+6
+                g3 = VADD(c3, e3);                  //0-4
+                i3 = VSUB(c3, e3);                  //2-6
+
+                //first set
+                a3 = VADD(a2, d2);                  //1+5
+                d3 = VSUB(a2, d2);                  //3+7
+                c3 = VADD(c2, e2);                  //1-5
+                e3 = VSUB(c2, e2);                  //3-7
+
+                ///////// Next 8 - 15 ends ///////////
+            }
+
+            //Stage-3 ////////// Next 8 - 15 (Stage-1) starts //////////
+            {
+
+                d3 = VCONJ(FLIP_RI(d3));
+                e3 = VCONJ(FLIP_RI(e3));
+                d4 = VCONJ(FLIP_RI(d4));
+                e4 = VCONJ(FLIP_RI(e4));
+
+                ///// Shuffle operations before stores : Start /////
+
+                //vs:0,0 ; rs:0->7
+                STA((x + mt), a5, 0, 0);
+                STA((x + mt + mn), d5, 0, 0);
+                STA((x + mt + mn4), f4, 0, 0);
+                STA((x + mt + mn6), h4, 0, 0);
+
+                //vs:2,3
+                a2 = VADD(f, f2);   //vs:2,3 ; rs:0
+                c2 = VADD(g, g2);   //vs:2,3 ; rs:1
+                d2 = VADD(h, h2);   //vs:2,3 ; rs:2
+                e2 = VADD(i, i2);   //vs:2,3 ; rs:3
+                f2 = VSUB(f, f2);   //vs:2,3 ; rs:4
+                g2 = VSUB(g, g2);   //vs:2,3 ; rs:5
+                h2 = VSUB(h, h2);   //vs:2,3 ; rs:6
+                i2 = VSUB(i, i2);   //vs:2,3 ; rs:7
+
+                //Multiply additional next stage Twiddle Factors for vs:2,3
+                c2 = BYTWJB(&(W[0+w_ms]), c2);
+                d2 = BYTWJB(&(W[(TWVL * 2)+w_ms]), d2);
+                e2 = BYTWJB(&(W[(TWVL * 4)+w_ms]), e2);
+                f2 = BYTWJB(&(W[(TWVL * 6)+w_ms]), f2);
+                g2 = BYTWJB(&(W[(TWVL * 8)+w_ms]), g2);
+                h2 = BYTWJB(&(W[(TWVL * 10)+w_ms]), h2);
+                i2 = BYTWJB(&(W[(TWVL * 12)+w_ms]), i2);
+
+                //vs:1,1 ; rs:0->7
+                STA((x + WS(rs, 1) + mt), c5, 0, 0);
+                STA((x + WS(rs, 1) + mt + mn), e5, 0, 0);
+                STA((x + WS(rs, 1) + mt + mn4), g4, 0, 0);
+                STA((x + WS(rs, 1) + mt + mn6), i4, 0, 0);
+
+                //vs:4,5
+                f = VADD(f3, a3);   //vs:4,5 ; rs:0
+                g = VADD(g3, c3);   //vs:4,5 ; rs:1
+                h = VADD(h3, d3);   //vs:4,5 ; rs:2
+                i = VADD(i3, e3);   //vs:4,5 ; rs:3
+                f3 = VSUB(f3, a3);  //vs:4,5 ; rs:4
+                g3 = VSUB(g3, c3);  //vs:4,5 ; rs:5
+                h3 = VSUB(h3, d3);  //vs:4,5 ; rs:6
+                i3 = VSUB(i3, e3);  //vs:4,5 ; rs:7
+
+                j = SHUF_CROSS_LANE_1(a2, c2);      //vs:2,2 ; rs:0,1
+                k = SHUF_CROSS_LANE_2(a2, c2);      //vs:3,3 ; rs:0,1
+                l = SHUF_CROSS_LANE_1(d2, e2);      //vs:2,2 ; rs:2,3
+                m = SHUF_CROSS_LANE_2(d2, e2);      //vs:3,3 ; rs:2,3
+                a2 = SHUF_CROSS_LANE_1(f2, g2);     //vs:2,2 ; rs:4,5
+                c2 = SHUF_CROSS_LANE_2(f2, g2);     //vs:3,3 ; rs:4,5
+                d2 = SHUF_CROSS_LANE_1(h2, i2);     //vs:2,2 ; rs:6,7
+                e2 = SHUF_CROSS_LANE_2(h2, i2);     //vs:3,3 ; rs:6,7
+
+                //vs:6,7
+                a3 = VADD(a, a4);   //vs:6,7 ; rs:0
+                c3 = VADD(c, c4);   //vs:6,7 ; rs:1
+                d3 = VADD(d, d4);   //vs:6,7 ; rs:2
+                e3 = VADD(e, e4);   //vs:6,7 ; rs:3
+                a4 = VSUB(a, a4);   //vs:6,7 ; rs:4
+                c4 = VSUB(c, c4);   //vs:6,7 ; rs:5
+                d4 = VSUB(d, d4);   //vs:6,7 ; rs:6
+                e4 = VSUB(e, e4);   //vs:6,7 ; rs:7
+
+                //Multiply additional next stage Twiddle Factors for vs:4,5
+                g = BYTWJB(&(W[0+w_ms]), g);
+                h = BYTWJB(&(W[(TWVL * 2)+w_ms]), h);
+                i = BYTWJB(&(W[(TWVL * 4)+w_ms]), i);
+                f3 = BYTWJB(&(W[(TWVL * 6)+w_ms]), f3);
+                g3 = BYTWJB(&(W[(TWVL * 8)+w_ms]), g3);
+                h3 = BYTWJB(&(W[(TWVL * 10)+w_ms]), h3);
+                i3 = BYTWJB(&(W[(TWVL * 12)+w_ms]), i3);
+
+                //vs:2,2 ; rs:0->7
+                STA((x + WS(rs, 2) + mt), j, 0, 0);
+                STA((x + WS(rs, 2) + mt + mn), l, 0, 0);
+                STA((x + WS(rs, 2) + mt + mn4), a2, 0, 0);
+                STA((x + WS(rs, 2) + mt + mn6), d2, 0, 0);
+
+                //vs:3,3 ; rs:0->7
+                STA((x + WS(rs, 3) + mt), k, 0, 0);
+                STA((x + WS(rs, 3) + mt + mn), m, 0, 0);
+                STA((x + WS(rs, 3) + mt + mn4), c2, 0, 0);
+                STA((x + WS(rs, 3) + mt + mn6), e2, 0, 0);
+
+                //Multiply additional next stage Twiddle Factors for vs:6,7
+                c3 = BYTWJB(&(W[0+w_ms]), c3);
+                d3 = BYTWJB(&(W[(TWVL * 2)+w_ms]), d3);
+                e3 = BYTWJB(&(W[(TWVL * 4)+w_ms]), e3);
+                a4 = BYTWJB(&(W[(TWVL * 6)+w_ms]), a4);
+                c4 = BYTWJB(&(W[(TWVL * 8)+w_ms]), c4);
+                d4 = BYTWJB(&(W[(TWVL * 10)+w_ms]), d4);
+                e4 = BYTWJB(&(W[(TWVL * 12)+w_ms]), e4);
+
+                f2 = SHUF_CROSS_LANE_1(f, g);      //vs:4,4 ; rs:0,1
+                g2 = SHUF_CROSS_LANE_2(f, g);      //vs:5,5 ; rs:0,1
+                h2 = SHUF_CROSS_LANE_1(h, i);      //vs:4,4 ; rs:2,3
+                i2 = SHUF_CROSS_LANE_2(h, i);      //vs:5,5 ; rs:2,3
+                f = SHUF_CROSS_LANE_1(f3, g3);     //vs:4,4 ; rs:4,5
+                g = SHUF_CROSS_LANE_2(f3, g3);     //vs:5,5 ; rs:4,5
+                h = SHUF_CROSS_LANE_1(h3, i3);     //vs:4,4 ; rs:6,7
+                i = SHUF_CROSS_LANE_2(h3, i3);     //vs:5,5 ; rs:6,7
+
+                f3 = SHUF_CROSS_LANE_1(a3, c3);    //vs:6,6 ; rs:0,1
+                g3 = SHUF_CROSS_LANE_2(a3, c3);    //vs:7,7 ; rs:0,1
+                h3 = SHUF_CROSS_LANE_1(d3, e3);    //vs:6,6 ; rs:2,3
+                i3 = SHUF_CROSS_LANE_2(d3, e3);    //vs:7,7 ; rs:2,3
+                a3 = SHUF_CROSS_LANE_1(a4, c4);    //vs:6,6 ; rs:4,5
+                c3 = SHUF_CROSS_LANE_2(a4, c4);    //vs:7,7 ; rs:4,5
+                d3 = SHUF_CROSS_LANE_1(d4, e4);    //vs:6,6 ; rs:6,7
+                e3 = SHUF_CROSS_LANE_2(d4, e4);    //vs:7,7 ; rs:6,7
+
+                ///// Shuffle operations before stores : end /////
+
+                //vs:4,4 ; rs:0->7
+                STA((x + WS(rs, 4) + mt), f2, 0, 0);
+                STA((x + WS(rs, 4) + mt + mn), h2, 0, 0);
+                STA((x + WS(rs, 4) + mt + mn4), f, 0, 0);
+                STA((x + WS(rs, 4) + mt + mn6), h, 0, 0);
+
+                //vs:5,5 ; rs:0->7
+                STA((x + WS(rs, 5) + mt), g2, 0, 0);
+                STA((x + WS(rs, 5) + mt + mn), i2, 0, 0);
+                STA((x + WS(rs, 5) + mt + mn4), g, 0, 0);
+                STA((x + WS(rs, 5) + mt + mn6), i, 0, 0);
+
+                //vs:6,6 ; rs:0->7
+                STA((x + WS(rs, 6) + mt), f3, 0, 0);
+                STA((x + WS(rs, 6) + mt + mn), h3, 0, 0);
+                STA((x + WS(rs, 6) + mt + mn4), a3, 0, 0);
+                STA((x + WS(rs, 6) + mt + mn6), d3, 0, 0);
+
+                //vs:7,7 ; rs:0->7
+                STA((x + WS(rs, 7) + mt), g3, 0, 0);
+                STA((x + WS(rs, 7) + mt + mn), i3, 0, 0);
+                STA((x + WS(rs, 7) + mt + mn4), c3, 0, 0);
+                STA((x + WS(rs, 7) + mt + mn6), e3, 0, 0);
+            }
+            w_ms=TWVL;
+        }
+    }
+    VLEAVE();
+}
+#endif
+
+#if (!defined(FFTW_SINGLE) && !defined(AMD_OPT_KERNEL_NEW_IMPLEMENTATION)) || defined(FFTW_SINGLE)
 static void q1fv_8(R *ri, R *ii, const R *W, stride rs, stride vs, INT mb, INT me, INT ms)
 {
      DVK(KP707106781, +0.707106781186547524400844362104849039284835938);
      {
-	  INT m;
-	  R *x;
-	  x = ri;
-	  for (m = mb, W = W + (mb * ((TWVL / VL) * 14)); m < me; m = m + VL, x = x + (VL * ms), W = W + (TWVL * 14), MAKE_VOLATILE_STRIDE(16, rs), MAKE_VOLATILE_STRIDE(16, vs)) {
-	       V T3, Tu, Tf, Tp, T1E, T25, T1Q, T20, T2b, T2C, T2n, T2x, T3M, T4d, T3Y;
-	       V T48, TA, T11, TM, TW, T17, T1y, T1j, T1t, T2I, T39, T2U, T34, T3f, T3G;
-	       V T3r, T3B, Ta, Tv, Tc, Ts, T1L, T26, T1N, T23, T2i, T2D, T2k, T2A, T3T;
-	       V T4e, T3V, T4b, TH, T12, TJ, TZ, T1e, T1z, T1g, T1w, T2P, T3a, T2R, T37;
-	       V T3m, T3H, T3o, T3E, T28, T14;
-	       {
-		    V T1, T2, Tn, Td, Te, To;
-		    T1 = LD(&(x[0]), ms, &(x[0]));
-		    T2 = LD(&(x[WS(rs, 4)]), ms, &(x[0]));
-		    Tn = VADD(T1, T2);
-		    Td = LD(&(x[WS(rs, 2)]), ms, &(x[0]));
-		    Te = LD(&(x[WS(rs, 6)]), ms, &(x[0]));
-		    To = VADD(Td, Te);
-		    T3 = VSUB(T1, T2);
-		    Tu = VSUB(Tn, To);
-		    Tf = VSUB(Td, Te);
-		    Tp = VADD(Tn, To);
-	       }
-	       {
-		    V T1C, T1D, T1Y, T1O, T1P, T1Z;
-		    T1C = LD(&(x[WS(vs, 3)]), ms, &(x[WS(vs, 3)]));
-		    T1D = LD(&(x[WS(vs, 3) + WS(rs, 4)]), ms, &(x[WS(vs, 3)]));
-		    T1Y = VADD(T1C, T1D);
-		    T1O = LD(&(x[WS(vs, 3) + WS(rs, 2)]), ms, &(x[WS(vs, 3)]));
-		    T1P = LD(&(x[WS(vs, 3) + WS(rs, 6)]), ms, &(x[WS(vs, 3)]));
-		    T1Z = VADD(T1O, T1P);
-		    T1E = VSUB(T1C, T1D);
-		    T25 = VSUB(T1Y, T1Z);
-		    T1Q = VSUB(T1O, T1P);
-		    T20 = VADD(T1Y, T1Z);
-	       }
-	       {
-		    V T29, T2a, T2v, T2l, T2m, T2w;
-		    T29 = LD(&(x[WS(vs, 4)]), ms, &(x[WS(vs, 4)]));
-		    T2a = LD(&(x[WS(vs, 4) + WS(rs, 4)]), ms, &(x[WS(vs, 4)]));
-		    T2v = VADD(T29, T2a);
-		    T2l = LD(&(x[WS(vs, 4) + WS(rs, 2)]), ms, &(x[WS(vs, 4)]));
-		    T2m = LD(&(x[WS(vs, 4) + WS(rs, 6)]), ms, &(x[WS(vs, 4)]));
-		    T2w = VADD(T2l, T2m);
-		    T2b = VSUB(T29, T2a);
-		    T2C = VSUB(T2v, T2w);
-		    T2n = VSUB(T2l, T2m);
-		    T2x = VADD(T2v, T2w);
-	       }
-	       {
-		    V T3K, T3L, T46, T3W, T3X, T47;
-		    T3K = LD(&(x[WS(vs, 7)]), ms, &(x[WS(vs, 7)]));
-		    T3L = LD(&(x[WS(vs, 7) + WS(rs, 4)]), ms, &(x[WS(vs, 7)]));
-		    T46 = VADD(T3K, T3L);
-		    T3W = LD(&(x[WS(vs, 7) + WS(rs, 2)]), ms, &(x[WS(vs, 7)]));
-		    T3X = LD(&(x[WS(vs, 7) + WS(rs, 6)]), ms, &(x[WS(vs, 7)]));
-		    T47 = VADD(T3W, T3X);
-		    T3M = VSUB(T3K, T3L);
-		    T4d = VSUB(T46, T47);
-		    T3Y = VSUB(T3W, T3X);
-		    T48 = VADD(T46, T47);
-	       }
-	       {
-		    V Ty, Tz, TU, TK, TL, TV;
-		    Ty = LD(&(x[WS(vs, 1)]), ms, &(x[WS(vs, 1)]));
-		    Tz = LD(&(x[WS(vs, 1) + WS(rs, 4)]), ms, &(x[WS(vs, 1)]));
-		    TU = VADD(Ty, Tz);
-		    TK = LD(&(x[WS(vs, 1) + WS(rs, 2)]), ms, &(x[WS(vs, 1)]));
-		    TL = LD(&(x[WS(vs, 1) + WS(rs, 6)]), ms, &(x[WS(vs, 1)]));
-		    TV = VADD(TK, TL);
-		    TA = VSUB(Ty, Tz);
-		    T11 = VSUB(TU, TV);
-		    TM = VSUB(TK, TL);
-		    TW = VADD(TU, TV);
-	       }
-	       {
-		    V T15, T16, T1r, T1h, T1i, T1s;
-		    T15 = LD(&(x[WS(vs, 2)]), ms, &(x[WS(vs, 2)]));
-		    T16 = LD(&(x[WS(vs, 2) + WS(rs, 4)]), ms, &(x[WS(vs, 2)]));
-		    T1r = VADD(T15, T16);
-		    T1h = LD(&(x[WS(vs, 2) + WS(rs, 2)]), ms, &(x[WS(vs, 2)]));
-		    T1i = LD(&(x[WS(vs, 2) + WS(rs, 6)]), ms, &(x[WS(vs, 2)]));
-		    T1s = VADD(T1h, T1i);
-		    T17 = VSUB(T15, T16);
-		    T1y = VSUB(T1r, T1s);
-		    T1j = VSUB(T1h, T1i);
-		    T1t = VADD(T1r, T1s);
-	       }
-	       {
-		    V T2G, T2H, T32, T2S, T2T, T33;
-		    T2G = LD(&(x[WS(vs, 5)]), ms, &(x[WS(vs, 5)]));
-		    T2H = LD(&(x[WS(vs, 5) + WS(rs, 4)]), ms, &(x[WS(vs, 5)]));
-		    T32 = VADD(T2G, T2H);
-		    T2S = LD(&(x[WS(vs, 5) + WS(rs, 2)]), ms, &(x[WS(vs, 5)]));
-		    T2T = LD(&(x[WS(vs, 5) + WS(rs, 6)]), ms, &(x[WS(vs, 5)]));
-		    T33 = VADD(T2S, T2T);
-		    T2I = VSUB(T2G, T2H);
-		    T39 = VSUB(T32, T33);
-		    T2U = VSUB(T2S, T2T);
-		    T34 = VADD(T32, T33);
-	       }
-	       {
-		    V T3d, T3e, T3z, T3p, T3q, T3A;
-		    T3d = LD(&(x[WS(vs, 6)]), ms, &(x[WS(vs, 6)]));
-		    T3e = LD(&(x[WS(vs, 6) + WS(rs, 4)]), ms, &(x[WS(vs, 6)]));
-		    T3z = VADD(T3d, T3e);
-		    T3p = LD(&(x[WS(vs, 6) + WS(rs, 2)]), ms, &(x[WS(vs, 6)]));
-		    T3q = LD(&(x[WS(vs, 6) + WS(rs, 6)]), ms, &(x[WS(vs, 6)]));
-		    T3A = VADD(T3p, T3q);
-		    T3f = VSUB(T3d, T3e);
-		    T3G = VSUB(T3z, T3A);
-		    T3r = VSUB(T3p, T3q);
-		    T3B = VADD(T3z, T3A);
-	       }
-	       {
-		    V T6, Tq, T9, Tr;
-		    {
-			 V T4, T5, T7, T8;
-			 T4 = LD(&(x[WS(rs, 1)]), ms, &(x[WS(rs, 1)]));
-			 T5 = LD(&(x[WS(rs, 5)]), ms, &(x[WS(rs, 1)]));
-			 T6 = VSUB(T4, T5);
-			 Tq = VADD(T4, T5);
-			 T7 = LD(&(x[WS(rs, 7)]), ms, &(x[WS(rs, 1)]));
-			 T8 = LD(&(x[WS(rs, 3)]), ms, &(x[WS(rs, 1)]));
-			 T9 = VSUB(T7, T8);
-			 Tr = VADD(T7, T8);
-		    }
-		    Ta = VMUL(LDK(KP707106781), VADD(T6, T9));
-		    Tv = VBYI(VSUB(Tr, Tq));
-		    Tc = VMUL(LDK(KP707106781), VSUB(T9, T6));
-		    Ts = VADD(Tq, Tr);
-	       }
-	       {
-		    V T1H, T21, T1K, T22;
-		    {
-			 V T1F, T1G, T1I, T1J;
-			 T1F = LD(&(x[WS(vs, 3) + WS(rs, 1)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1G = LD(&(x[WS(vs, 3) + WS(rs, 5)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1H = VSUB(T1F, T1G);
-			 T21 = VADD(T1F, T1G);
-			 T1I = LD(&(x[WS(vs, 3) + WS(rs, 7)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1J = LD(&(x[WS(vs, 3) + WS(rs, 3)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-			 T1K = VSUB(T1I, T1J);
-			 T22 = VADD(T1I, T1J);
-		    }
-		    T1L = VMUL(LDK(KP707106781), VADD(T1H, T1K));
-		    T26 = VBYI(VSUB(T22, T21));
-		    T1N = VMUL(LDK(KP707106781), VSUB(T1K, T1H));
-		    T23 = VADD(T21, T22);
-	       }
-	       {
-		    V T2e, T2y, T2h, T2z;
-		    {
-			 V T2c, T2d, T2f, T2g;
-			 T2c = LD(&(x[WS(vs, 4) + WS(rs, 1)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2d = LD(&(x[WS(vs, 4) + WS(rs, 5)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2e = VSUB(T2c, T2d);
-			 T2y = VADD(T2c, T2d);
-			 T2f = LD(&(x[WS(vs, 4) + WS(rs, 7)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2g = LD(&(x[WS(vs, 4) + WS(rs, 3)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-			 T2h = VSUB(T2f, T2g);
-			 T2z = VADD(T2f, T2g);
-		    }
-		    T2i = VMUL(LDK(KP707106781), VADD(T2e, T2h));
-		    T2D = VBYI(VSUB(T2z, T2y));
-		    T2k = VMUL(LDK(KP707106781), VSUB(T2h, T2e));
-		    T2A = VADD(T2y, T2z);
-	       }
-	       {
-		    V T3P, T49, T3S, T4a;
-		    {
-			 V T3N, T3O, T3Q, T3R;
-			 T3N = LD(&(x[WS(vs, 7) + WS(rs, 1)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3O = LD(&(x[WS(vs, 7) + WS(rs, 5)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3P = VSUB(T3N, T3O);
-			 T49 = VADD(T3N, T3O);
-			 T3Q = LD(&(x[WS(vs, 7) + WS(rs, 7)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3R = LD(&(x[WS(vs, 7) + WS(rs, 3)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-			 T3S = VSUB(T3Q, T3R);
-			 T4a = VADD(T3Q, T3R);
-		    }
-		    T3T = VMUL(LDK(KP707106781), VADD(T3P, T3S));
-		    T4e = VBYI(VSUB(T4a, T49));
-		    T3V = VMUL(LDK(KP707106781), VSUB(T3S, T3P));
-		    T4b = VADD(T49, T4a);
-	       }
-	       {
-		    V TD, TX, TG, TY;
-		    {
-			 V TB, TC, TE, TF;
-			 TB = LD(&(x[WS(vs, 1) + WS(rs, 1)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TC = LD(&(x[WS(vs, 1) + WS(rs, 5)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TD = VSUB(TB, TC);
-			 TX = VADD(TB, TC);
-			 TE = LD(&(x[WS(vs, 1) + WS(rs, 7)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TF = LD(&(x[WS(vs, 1) + WS(rs, 3)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-			 TG = VSUB(TE, TF);
-			 TY = VADD(TE, TF);
-		    }
-		    TH = VMUL(LDK(KP707106781), VADD(TD, TG));
-		    T12 = VBYI(VSUB(TY, TX));
-		    TJ = VMUL(LDK(KP707106781), VSUB(TG, TD));
-		    TZ = VADD(TX, TY);
-	       }
-	       {
-		    V T1a, T1u, T1d, T1v;
-		    {
-			 V T18, T19, T1b, T1c;
-			 T18 = LD(&(x[WS(vs, 2) + WS(rs, 1)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T19 = LD(&(x[WS(vs, 2) + WS(rs, 5)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T1a = VSUB(T18, T19);
-			 T1u = VADD(T18, T19);
-			 T1b = LD(&(x[WS(vs, 2) + WS(rs, 7)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T1c = LD(&(x[WS(vs, 2) + WS(rs, 3)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-			 T1d = VSUB(T1b, T1c);
-			 T1v = VADD(T1b, T1c);
-		    }
-		    T1e = VMUL(LDK(KP707106781), VADD(T1a, T1d));
-		    T1z = VBYI(VSUB(T1v, T1u));
-		    T1g = VMUL(LDK(KP707106781), VSUB(T1d, T1a));
-		    T1w = VADD(T1u, T1v);
-	       }
-	       {
-		    V T2L, T35, T2O, T36;
-		    {
-			 V T2J, T2K, T2M, T2N;
-			 T2J = LD(&(x[WS(vs, 5) + WS(rs, 1)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2K = LD(&(x[WS(vs, 5) + WS(rs, 5)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2L = VSUB(T2J, T2K);
-			 T35 = VADD(T2J, T2K);
-			 T2M = LD(&(x[WS(vs, 5) + WS(rs, 7)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2N = LD(&(x[WS(vs, 5) + WS(rs, 3)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-			 T2O = VSUB(T2M, T2N);
-			 T36 = VADD(T2M, T2N);
-		    }
-		    T2P = VMUL(LDK(KP707106781), VADD(T2L, T2O));
-		    T3a = VBYI(VSUB(T36, T35));
-		    T2R = VMUL(LDK(KP707106781), VSUB(T2O, T2L));
-		    T37 = VADD(T35, T36);
-	       }
-	       {
-		    V T3i, T3C, T3l, T3D;
-		    {
-			 V T3g, T3h, T3j, T3k;
-			 T3g = LD(&(x[WS(vs, 6) + WS(rs, 1)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3h = LD(&(x[WS(vs, 6) + WS(rs, 5)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3i = VSUB(T3g, T3h);
-			 T3C = VADD(T3g, T3h);
-			 T3j = LD(&(x[WS(vs, 6) + WS(rs, 7)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3k = LD(&(x[WS(vs, 6) + WS(rs, 3)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-			 T3l = VSUB(T3j, T3k);
-			 T3D = VADD(T3j, T3k);
-		    }
-		    T3m = VMUL(LDK(KP707106781), VADD(T3i, T3l));
-		    T3H = VBYI(VSUB(T3D, T3C));
-		    T3o = VMUL(LDK(KP707106781), VSUB(T3l, T3i));
-		    T3E = VADD(T3C, T3D);
-	       }
-	       ST(&(x[0]), VADD(Tp, Ts), ms, &(x[0]));
-	       ST(&(x[WS(rs, 2)]), VADD(T1t, T1w), ms, &(x[0]));
-	       ST(&(x[WS(rs, 5)]), VADD(T34, T37), ms, &(x[WS(rs, 1)]));
-	       ST(&(x[WS(rs, 7)]), VADD(T48, T4b), ms, &(x[WS(rs, 1)]));
-	       ST(&(x[WS(rs, 6)]), VADD(T3B, T3E), ms, &(x[0]));
-	       ST(&(x[WS(rs, 4)]), VADD(T2x, T2A), ms, &(x[0]));
-	       {
-		    V Tt, T4c, T2B, T24;
-		    ST(&(x[WS(rs, 3)]), VADD(T20, T23), ms, &(x[WS(rs, 1)]));
-		    ST(&(x[WS(rs, 1)]), VADD(TW, TZ), ms, &(x[WS(rs, 1)]));
-		    Tt = BYTWJ(&(W[TWVL * 6]), VSUB(Tp, Ts));
-		    ST(&(x[WS(vs, 4)]), Tt, ms, &(x[WS(vs, 4)]));
-		    T4c = BYTWJ(&(W[TWVL * 6]), VSUB(T48, T4b));
-		    ST(&(x[WS(vs, 4) + WS(rs, 7)]), T4c, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-		    T2B = BYTWJ(&(W[TWVL * 6]), VSUB(T2x, T2A));
-		    ST(&(x[WS(vs, 4) + WS(rs, 4)]), T2B, ms, &(x[WS(vs, 4)]));
-		    T24 = BYTWJ(&(W[TWVL * 6]), VSUB(T20, T23));
-		    ST(&(x[WS(vs, 4) + WS(rs, 3)]), T24, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-	       }
-	       {
-		    V T10, T1x, T3F, T38, T1A, Tw;
-		    T10 = BYTWJ(&(W[TWVL * 6]), VSUB(TW, TZ));
-		    ST(&(x[WS(vs, 4) + WS(rs, 1)]), T10, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-		    T1x = BYTWJ(&(W[TWVL * 6]), VSUB(T1t, T1w));
-		    ST(&(x[WS(vs, 4) + WS(rs, 2)]), T1x, ms, &(x[WS(vs, 4)]));
-		    T3F = BYTWJ(&(W[TWVL * 6]), VSUB(T3B, T3E));
-		    ST(&(x[WS(vs, 4) + WS(rs, 6)]), T3F, ms, &(x[WS(vs, 4)]));
-		    T38 = BYTWJ(&(W[TWVL * 6]), VSUB(T34, T37));
-		    ST(&(x[WS(vs, 4) + WS(rs, 5)]), T38, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
-		    T1A = BYTWJ(&(W[TWVL * 10]), VSUB(T1y, T1z));
-		    ST(&(x[WS(vs, 6) + WS(rs, 2)]), T1A, ms, &(x[WS(vs, 6)]));
-		    Tw = BYTWJ(&(W[TWVL * 10]), VSUB(Tu, Tv));
-		    ST(&(x[WS(vs, 6)]), Tw, ms, &(x[WS(vs, 6)]));
-	       }
-	       {
-		    V T2E, T3I, T13, T27, T3b, T4f;
-		    T2E = BYTWJ(&(W[TWVL * 10]), VSUB(T2C, T2D));
-		    ST(&(x[WS(vs, 6) + WS(rs, 4)]), T2E, ms, &(x[WS(vs, 6)]));
-		    T3I = BYTWJ(&(W[TWVL * 10]), VSUB(T3G, T3H));
-		    ST(&(x[WS(vs, 6) + WS(rs, 6)]), T3I, ms, &(x[WS(vs, 6)]));
-		    T13 = BYTWJ(&(W[TWVL * 10]), VSUB(T11, T12));
-		    ST(&(x[WS(vs, 6) + WS(rs, 1)]), T13, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-		    T27 = BYTWJ(&(W[TWVL * 10]), VSUB(T25, T26));
-		    ST(&(x[WS(vs, 6) + WS(rs, 3)]), T27, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-		    T3b = BYTWJ(&(W[TWVL * 10]), VSUB(T39, T3a));
-		    ST(&(x[WS(vs, 6) + WS(rs, 5)]), T3b, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-		    T4f = BYTWJ(&(W[TWVL * 10]), VSUB(T4d, T4e));
-		    ST(&(x[WS(vs, 6) + WS(rs, 7)]), T4f, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
-	       }
-	       {
-		    V Tx, T1B, T3c, T4g, T3J, T2F;
-		    Tx = BYTWJ(&(W[TWVL * 2]), VADD(Tu, Tv));
-		    ST(&(x[WS(vs, 2)]), Tx, ms, &(x[WS(vs, 2)]));
-		    T1B = BYTWJ(&(W[TWVL * 2]), VADD(T1y, T1z));
-		    ST(&(x[WS(vs, 2) + WS(rs, 2)]), T1B, ms, &(x[WS(vs, 2)]));
-		    T3c = BYTWJ(&(W[TWVL * 2]), VADD(T39, T3a));
-		    ST(&(x[WS(vs, 2) + WS(rs, 5)]), T3c, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-		    T4g = BYTWJ(&(W[TWVL * 2]), VADD(T4d, T4e));
-		    ST(&(x[WS(vs, 2) + WS(rs, 7)]), T4g, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-		    T3J = BYTWJ(&(W[TWVL * 2]), VADD(T3G, T3H));
-		    ST(&(x[WS(vs, 2) + WS(rs, 6)]), T3J, ms, &(x[WS(vs, 2)]));
-		    T2F = BYTWJ(&(W[TWVL * 2]), VADD(T2C, T2D));
-		    ST(&(x[WS(vs, 2) + WS(rs, 4)]), T2F, ms, &(x[WS(vs, 2)]));
-	       }
-	       T28 = BYTWJ(&(W[TWVL * 2]), VADD(T25, T26));
-	       ST(&(x[WS(vs, 2) + WS(rs, 3)]), T28, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-	       T14 = BYTWJ(&(W[TWVL * 2]), VADD(T11, T12));
-	       ST(&(x[WS(vs, 2) + WS(rs, 1)]), T14, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
-	       {
-		    V Th, Ti, Tb, Tg;
-		    Tb = VADD(T3, Ta);
-		    Tg = VBYI(VSUB(Tc, Tf));
-		    Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
-		    Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
-		    ST(&(x[WS(vs, 7)]), Th, ms, &(x[WS(vs, 7)]));
-		    ST(&(x[WS(vs, 1)]), Ti, ms, &(x[WS(vs, 1)]));
-	       }
-	       {
-		    V T40, T41, T3U, T3Z;
-		    T3U = VADD(T3M, T3T);
-		    T3Z = VBYI(VSUB(T3V, T3Y));
-		    T40 = BYTWJ(&(W[TWVL * 12]), VSUB(T3U, T3Z));
-		    T41 = BYTWJ(&(W[0]), VADD(T3U, T3Z));
-		    ST(&(x[WS(vs, 7) + WS(rs, 7)]), T40, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 1) + WS(rs, 7)]), T41, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-	       }
-	       {
-		    V T2p, T2q, T2j, T2o;
-		    T2j = VADD(T2b, T2i);
-		    T2o = VBYI(VSUB(T2k, T2n));
-		    T2p = BYTWJ(&(W[TWVL * 12]), VSUB(T2j, T2o));
-		    T2q = BYTWJ(&(W[0]), VADD(T2j, T2o));
-		    ST(&(x[WS(vs, 7) + WS(rs, 4)]), T2p, ms, &(x[WS(vs, 7)]));
-		    ST(&(x[WS(vs, 1) + WS(rs, 4)]), T2q, ms, &(x[WS(vs, 1)]));
-	       }
-	       {
-		    V T1S, T1T, T1M, T1R;
-		    T1M = VADD(T1E, T1L);
-		    T1R = VBYI(VSUB(T1N, T1Q));
-		    T1S = BYTWJ(&(W[TWVL * 12]), VSUB(T1M, T1R));
-		    T1T = BYTWJ(&(W[0]), VADD(T1M, T1R));
-		    ST(&(x[WS(vs, 7) + WS(rs, 3)]), T1S, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 1) + WS(rs, 3)]), T1T, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-	       }
-	       {
-		    V TO, TP, TI, TN;
-		    TI = VADD(TA, TH);
-		    TN = VBYI(VSUB(TJ, TM));
-		    TO = BYTWJ(&(W[TWVL * 12]), VSUB(TI, TN));
-		    TP = BYTWJ(&(W[0]), VADD(TI, TN));
-		    ST(&(x[WS(vs, 7) + WS(rs, 1)]), TO, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 1) + WS(rs, 1)]), TP, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-	       }
-	       {
-		    V T1l, T1m, T1f, T1k;
-		    T1f = VADD(T17, T1e);
-		    T1k = VBYI(VSUB(T1g, T1j));
-		    T1l = BYTWJ(&(W[TWVL * 12]), VSUB(T1f, T1k));
-		    T1m = BYTWJ(&(W[0]), VADD(T1f, T1k));
-		    ST(&(x[WS(vs, 7) + WS(rs, 2)]), T1l, ms, &(x[WS(vs, 7)]));
-		    ST(&(x[WS(vs, 1) + WS(rs, 2)]), T1m, ms, &(x[WS(vs, 1)]));
-	       }
-	       {
-		    V T3t, T3u, T3n, T3s;
-		    T3n = VADD(T3f, T3m);
-		    T3s = VBYI(VSUB(T3o, T3r));
-		    T3t = BYTWJ(&(W[TWVL * 12]), VSUB(T3n, T3s));
-		    T3u = BYTWJ(&(W[0]), VADD(T3n, T3s));
-		    ST(&(x[WS(vs, 7) + WS(rs, 6)]), T3t, ms, &(x[WS(vs, 7)]));
-		    ST(&(x[WS(vs, 1) + WS(rs, 6)]), T3u, ms, &(x[WS(vs, 1)]));
-	       }
-	       {
-		    V T2W, T2X, T2Q, T2V;
-		    T2Q = VADD(T2I, T2P);
-		    T2V = VBYI(VSUB(T2R, T2U));
-		    T2W = BYTWJ(&(W[TWVL * 12]), VSUB(T2Q, T2V));
-		    T2X = BYTWJ(&(W[0]), VADD(T2Q, T2V));
-		    ST(&(x[WS(vs, 7) + WS(rs, 5)]), T2W, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 1) + WS(rs, 5)]), T2X, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
-	       }
-	       {
-		    V T1p, T1q, T1n, T1o;
-		    T1n = VSUB(T17, T1e);
-		    T1o = VBYI(VADD(T1j, T1g));
-		    T1p = BYTWJ(&(W[TWVL * 8]), VSUB(T1n, T1o));
-		    T1q = BYTWJ(&(W[TWVL * 4]), VADD(T1n, T1o));
-		    ST(&(x[WS(vs, 5) + WS(rs, 2)]), T1p, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 2)]), T1q, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V Tl, Tm, Tj, Tk;
-		    Tj = VSUB(T3, Ta);
-		    Tk = VBYI(VADD(Tf, Tc));
-		    Tl = BYTWJ(&(W[TWVL * 8]), VSUB(Tj, Tk));
-		    Tm = BYTWJ(&(W[TWVL * 4]), VADD(Tj, Tk));
-		    ST(&(x[WS(vs, 5)]), Tl, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3)]), Tm, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V T2t, T2u, T2r, T2s;
-		    T2r = VSUB(T2b, T2i);
-		    T2s = VBYI(VADD(T2n, T2k));
-		    T2t = BYTWJ(&(W[TWVL * 8]), VSUB(T2r, T2s));
-		    T2u = BYTWJ(&(W[TWVL * 4]), VADD(T2r, T2s));
-		    ST(&(x[WS(vs, 5) + WS(rs, 4)]), T2t, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 4)]), T2u, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V T3x, T3y, T3v, T3w;
-		    T3v = VSUB(T3f, T3m);
-		    T3w = VBYI(VADD(T3r, T3o));
-		    T3x = BYTWJ(&(W[TWVL * 8]), VSUB(T3v, T3w));
-		    T3y = BYTWJ(&(W[TWVL * 4]), VADD(T3v, T3w));
-		    ST(&(x[WS(vs, 5) + WS(rs, 6)]), T3x, ms, &(x[WS(vs, 5)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 6)]), T3y, ms, &(x[WS(vs, 3)]));
-	       }
-	       {
-		    V TS, TT, TQ, TR;
-		    TQ = VSUB(TA, TH);
-		    TR = VBYI(VADD(TM, TJ));
-		    TS = BYTWJ(&(W[TWVL * 8]), VSUB(TQ, TR));
-		    TT = BYTWJ(&(W[TWVL * 4]), VADD(TQ, TR));
-		    ST(&(x[WS(vs, 5) + WS(rs, 1)]), TS, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 1)]), TT, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	       {
-		    V T1W, T1X, T1U, T1V;
-		    T1U = VSUB(T1E, T1L);
-		    T1V = VBYI(VADD(T1Q, T1N));
-		    T1W = BYTWJ(&(W[TWVL * 8]), VSUB(T1U, T1V));
-		    T1X = BYTWJ(&(W[TWVL * 4]), VADD(T1U, T1V));
-		    ST(&(x[WS(vs, 5) + WS(rs, 3)]), T1W, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 3)]), T1X, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	       {
-		    V T30, T31, T2Y, T2Z;
-		    T2Y = VSUB(T2I, T2P);
-		    T2Z = VBYI(VADD(T2U, T2R));
-		    T30 = BYTWJ(&(W[TWVL * 8]), VSUB(T2Y, T2Z));
-		    T31 = BYTWJ(&(W[TWVL * 4]), VADD(T2Y, T2Z));
-		    ST(&(x[WS(vs, 5) + WS(rs, 5)]), T30, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 5)]), T31, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	       {
-		    V T44, T45, T42, T43;
-		    T42 = VSUB(T3M, T3T);
-		    T43 = VBYI(VADD(T3Y, T3V));
-		    T44 = BYTWJ(&(W[TWVL * 8]), VSUB(T42, T43));
-		    T45 = BYTWJ(&(W[TWVL * 4]), VADD(T42, T43));
-		    ST(&(x[WS(vs, 5) + WS(rs, 7)]), T44, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
-		    ST(&(x[WS(vs, 3) + WS(rs, 7)]), T45, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
-	       }
-	  }
+      INT m;
+      R *x;
+      x = ri;
+      for (m = mb, W = W + (mb * ((TWVL / VL) * 14)); m < me; m = m + VL, x = x + (VL * ms), W = W + (TWVL * 14), MAKE_VOLATILE_STRIDE(16, rs), MAKE_VOLATILE_STRIDE(16, vs)) {
+           V T3, Tu, Tf, Tp, T1E, T25, T1Q, T20, T2b, T2C, T2n, T2x, T3M, T4d, T3Y;
+           V T48, TA, T11, TM, TW, T17, T1y, T1j, T1t, T2I, T39, T2U, T34, T3f, T3G;
+           V T3r, T3B, Ta, Tv, Tc, Ts, T1L, T26, T1N, T23, T2i, T2D, T2k, T2A, T3T;
+           V T4e, T3V, T4b, TH, T12, TJ, TZ, T1e, T1z, T1g, T1w, T2P, T3a, T2R, T37;
+           V T3m, T3H, T3o, T3E, T28, T14;
+           {
+            V T1, T2, Tn, Td, Te, To;
+            T1 = LD(&(x[0]), ms, &(x[0]));
+            T2 = LD(&(x[WS(rs, 4)]), ms, &(x[0]));
+            Tn = VADD(T1, T2);
+            Td = LD(&(x[WS(rs, 2)]), ms, &(x[0]));
+            Te = LD(&(x[WS(rs, 6)]), ms, &(x[0]));
+            To = VADD(Td, Te);
+            T3 = VSUB(T1, T2);
+            Tu = VSUB(Tn, To);
+            Tf = VSUB(Td, Te);
+            Tp = VADD(Tn, To);
+           }
+           {
+            V T1C, T1D, T1Y, T1O, T1P, T1Z;
+            T1C = LD(&(x[WS(vs, 3)]), ms, &(x[WS(vs, 3)]));
+            T1D = LD(&(x[WS(vs, 3) + WS(rs, 4)]), ms, &(x[WS(vs, 3)]));
+            T1Y = VADD(T1C, T1D);
+            T1O = LD(&(x[WS(vs, 3) + WS(rs, 2)]), ms, &(x[WS(vs, 3)]));
+            T1P = LD(&(x[WS(vs, 3) + WS(rs, 6)]), ms, &(x[WS(vs, 3)]));
+            T1Z = VADD(T1O, T1P);
+            T1E = VSUB(T1C, T1D);
+            T25 = VSUB(T1Y, T1Z);
+            T1Q = VSUB(T1O, T1P);
+            T20 = VADD(T1Y, T1Z);
+           }
+           {
+            V T29, T2a, T2v, T2l, T2m, T2w;
+            T29 = LD(&(x[WS(vs, 4)]), ms, &(x[WS(vs, 4)]));
+            T2a = LD(&(x[WS(vs, 4) + WS(rs, 4)]), ms, &(x[WS(vs, 4)]));
+            T2v = VADD(T29, T2a);
+            T2l = LD(&(x[WS(vs, 4) + WS(rs, 2)]), ms, &(x[WS(vs, 4)]));
+            T2m = LD(&(x[WS(vs, 4) + WS(rs, 6)]), ms, &(x[WS(vs, 4)]));
+            T2w = VADD(T2l, T2m);
+            T2b = VSUB(T29, T2a);
+            T2C = VSUB(T2v, T2w);
+            T2n = VSUB(T2l, T2m);
+            T2x = VADD(T2v, T2w);
+           }
+           {
+            V T3K, T3L, T46, T3W, T3X, T47;
+            T3K = LD(&(x[WS(vs, 7)]), ms, &(x[WS(vs, 7)]));
+            T3L = LD(&(x[WS(vs, 7) + WS(rs, 4)]), ms, &(x[WS(vs, 7)]));
+            T46 = VADD(T3K, T3L);
+            T3W = LD(&(x[WS(vs, 7) + WS(rs, 2)]), ms, &(x[WS(vs, 7)]));
+            T3X = LD(&(x[WS(vs, 7) + WS(rs, 6)]), ms, &(x[WS(vs, 7)]));
+            T47 = VADD(T3W, T3X);
+            T3M = VSUB(T3K, T3L);
+            T4d = VSUB(T46, T47);
+            T3Y = VSUB(T3W, T3X);
+            T48 = VADD(T46, T47);
+           }
+           {
+            V Ty, Tz, TU, TK, TL, TV;
+            Ty = LD(&(x[WS(vs, 1)]), ms, &(x[WS(vs, 1)]));
+            Tz = LD(&(x[WS(vs, 1) + WS(rs, 4)]), ms, &(x[WS(vs, 1)]));
+            TU = VADD(Ty, Tz);
+            TK = LD(&(x[WS(vs, 1) + WS(rs, 2)]), ms, &(x[WS(vs, 1)]));
+            TL = LD(&(x[WS(vs, 1) + WS(rs, 6)]), ms, &(x[WS(vs, 1)]));
+            TV = VADD(TK, TL);
+            TA = VSUB(Ty, Tz);
+            T11 = VSUB(TU, TV);
+            TM = VSUB(TK, TL);
+            TW = VADD(TU, TV);
+           }
+           {
+            V T15, T16, T1r, T1h, T1i, T1s;
+            T15 = LD(&(x[WS(vs, 2)]), ms, &(x[WS(vs, 2)]));
+            T16 = LD(&(x[WS(vs, 2) + WS(rs, 4)]), ms, &(x[WS(vs, 2)]));
+            T1r = VADD(T15, T16);
+            T1h = LD(&(x[WS(vs, 2) + WS(rs, 2)]), ms, &(x[WS(vs, 2)]));
+            T1i = LD(&(x[WS(vs, 2) + WS(rs, 6)]), ms, &(x[WS(vs, 2)]));
+            T1s = VADD(T1h, T1i);
+            T17 = VSUB(T15, T16);
+            T1y = VSUB(T1r, T1s);
+            T1j = VSUB(T1h, T1i);
+            T1t = VADD(T1r, T1s);
+           }
+           {
+            V T2G, T2H, T32, T2S, T2T, T33;
+            T2G = LD(&(x[WS(vs, 5)]), ms, &(x[WS(vs, 5)]));
+            T2H = LD(&(x[WS(vs, 5) + WS(rs, 4)]), ms, &(x[WS(vs, 5)]));
+            T32 = VADD(T2G, T2H);
+            T2S = LD(&(x[WS(vs, 5) + WS(rs, 2)]), ms, &(x[WS(vs, 5)]));
+            T2T = LD(&(x[WS(vs, 5) + WS(rs, 6)]), ms, &(x[WS(vs, 5)]));
+            T33 = VADD(T2S, T2T);
+            T2I = VSUB(T2G, T2H);
+            T39 = VSUB(T32, T33);
+            T2U = VSUB(T2S, T2T);
+            T34 = VADD(T32, T33);
+           }
+           {
+            V T3d, T3e, T3z, T3p, T3q, T3A;
+            T3d = LD(&(x[WS(vs, 6)]), ms, &(x[WS(vs, 6)]));
+            T3e = LD(&(x[WS(vs, 6) + WS(rs, 4)]), ms, &(x[WS(vs, 6)]));
+            T3z = VADD(T3d, T3e);
+            T3p = LD(&(x[WS(vs, 6) + WS(rs, 2)]), ms, &(x[WS(vs, 6)]));
+            T3q = LD(&(x[WS(vs, 6) + WS(rs, 6)]), ms, &(x[WS(vs, 6)]));
+            T3A = VADD(T3p, T3q);
+            T3f = VSUB(T3d, T3e);
+            T3G = VSUB(T3z, T3A);
+            T3r = VSUB(T3p, T3q);
+            T3B = VADD(T3z, T3A);
+           }
+           {
+            V T6, Tq, T9, Tr;
+            {
+             V T4, T5, T7, T8;
+             T4 = LD(&(x[WS(rs, 1)]), ms, &(x[WS(rs, 1)]));
+             T5 = LD(&(x[WS(rs, 5)]), ms, &(x[WS(rs, 1)]));
+             T6 = VSUB(T4, T5);
+             Tq = VADD(T4, T5);
+             T7 = LD(&(x[WS(rs, 7)]), ms, &(x[WS(rs, 1)]));
+             T8 = LD(&(x[WS(rs, 3)]), ms, &(x[WS(rs, 1)]));
+             T9 = VSUB(T7, T8);
+             Tr = VADD(T7, T8);
+            }
+            Ta = VMUL(LDK(KP707106781), VADD(T6, T9));
+            Tv = VBYI(VSUB(Tr, Tq));
+            Tc = VMUL(LDK(KP707106781), VSUB(T9, T6));
+            Ts = VADD(Tq, Tr);
+           }
+           {
+            V T1H, T21, T1K, T22;
+            {
+             V T1F, T1G, T1I, T1J;
+             T1F = LD(&(x[WS(vs, 3) + WS(rs, 1)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1G = LD(&(x[WS(vs, 3) + WS(rs, 5)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1H = VSUB(T1F, T1G);
+             T21 = VADD(T1F, T1G);
+             T1I = LD(&(x[WS(vs, 3) + WS(rs, 7)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1J = LD(&(x[WS(vs, 3) + WS(rs, 3)]), ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+             T1K = VSUB(T1I, T1J);
+             T22 = VADD(T1I, T1J);
+            }
+            T1L = VMUL(LDK(KP707106781), VADD(T1H, T1K));
+            T26 = VBYI(VSUB(T22, T21));
+            T1N = VMUL(LDK(KP707106781), VSUB(T1K, T1H));
+            T23 = VADD(T21, T22);
+           }
+           {
+            V T2e, T2y, T2h, T2z;
+            {
+             V T2c, T2d, T2f, T2g;
+             T2c = LD(&(x[WS(vs, 4) + WS(rs, 1)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2d = LD(&(x[WS(vs, 4) + WS(rs, 5)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2e = VSUB(T2c, T2d);
+             T2y = VADD(T2c, T2d);
+             T2f = LD(&(x[WS(vs, 4) + WS(rs, 7)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2g = LD(&(x[WS(vs, 4) + WS(rs, 3)]), ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+             T2h = VSUB(T2f, T2g);
+             T2z = VADD(T2f, T2g);
+            }
+            T2i = VMUL(LDK(KP707106781), VADD(T2e, T2h));
+            T2D = VBYI(VSUB(T2z, T2y));
+            T2k = VMUL(LDK(KP707106781), VSUB(T2h, T2e));
+            T2A = VADD(T2y, T2z);
+           }
+           {
+            V T3P, T49, T3S, T4a;
+            {
+             V T3N, T3O, T3Q, T3R;
+             T3N = LD(&(x[WS(vs, 7) + WS(rs, 1)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3O = LD(&(x[WS(vs, 7) + WS(rs, 5)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3P = VSUB(T3N, T3O);
+             T49 = VADD(T3N, T3O);
+             T3Q = LD(&(x[WS(vs, 7) + WS(rs, 7)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3R = LD(&(x[WS(vs, 7) + WS(rs, 3)]), ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+             T3S = VSUB(T3Q, T3R);
+             T4a = VADD(T3Q, T3R);
+            }
+            T3T = VMUL(LDK(KP707106781), VADD(T3P, T3S));
+            T4e = VBYI(VSUB(T4a, T49));
+            T3V = VMUL(LDK(KP707106781), VSUB(T3S, T3P));
+            T4b = VADD(T49, T4a);
+           }
+           {
+            V TD, TX, TG, TY;
+            {
+             V TB, TC, TE, TF;
+             TB = LD(&(x[WS(vs, 1) + WS(rs, 1)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TC = LD(&(x[WS(vs, 1) + WS(rs, 5)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TD = VSUB(TB, TC);
+             TX = VADD(TB, TC);
+             TE = LD(&(x[WS(vs, 1) + WS(rs, 7)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TF = LD(&(x[WS(vs, 1) + WS(rs, 3)]), ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+             TG = VSUB(TE, TF);
+             TY = VADD(TE, TF);
+            }
+            TH = VMUL(LDK(KP707106781), VADD(TD, TG));
+            T12 = VBYI(VSUB(TY, TX));
+            TJ = VMUL(LDK(KP707106781), VSUB(TG, TD));
+            TZ = VADD(TX, TY);
+           }
+           {
+            V T1a, T1u, T1d, T1v;
+            {
+             V T18, T19, T1b, T1c;
+             T18 = LD(&(x[WS(vs, 2) + WS(rs, 1)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T19 = LD(&(x[WS(vs, 2) + WS(rs, 5)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T1a = VSUB(T18, T19);
+             T1u = VADD(T18, T19);
+             T1b = LD(&(x[WS(vs, 2) + WS(rs, 7)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T1c = LD(&(x[WS(vs, 2) + WS(rs, 3)]), ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+             T1d = VSUB(T1b, T1c);
+             T1v = VADD(T1b, T1c);
+            }
+            T1e = VMUL(LDK(KP707106781), VADD(T1a, T1d));
+            T1z = VBYI(VSUB(T1v, T1u));
+            T1g = VMUL(LDK(KP707106781), VSUB(T1d, T1a));
+            T1w = VADD(T1u, T1v);
+           }
+           {
+            V T2L, T35, T2O, T36;
+            {
+             V T2J, T2K, T2M, T2N;
+             T2J = LD(&(x[WS(vs, 5) + WS(rs, 1)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2K = LD(&(x[WS(vs, 5) + WS(rs, 5)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2L = VSUB(T2J, T2K);
+             T35 = VADD(T2J, T2K);
+             T2M = LD(&(x[WS(vs, 5) + WS(rs, 7)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2N = LD(&(x[WS(vs, 5) + WS(rs, 3)]), ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+             T2O = VSUB(T2M, T2N);
+             T36 = VADD(T2M, T2N);
+            }
+            T2P = VMUL(LDK(KP707106781), VADD(T2L, T2O));
+            T3a = VBYI(VSUB(T36, T35));
+            T2R = VMUL(LDK(KP707106781), VSUB(T2O, T2L));
+            T37 = VADD(T35, T36);
+           }
+           {
+            V T3i, T3C, T3l, T3D;
+            {
+             V T3g, T3h, T3j, T3k;
+             T3g = LD(&(x[WS(vs, 6) + WS(rs, 1)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3h = LD(&(x[WS(vs, 6) + WS(rs, 5)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3i = VSUB(T3g, T3h);
+             T3C = VADD(T3g, T3h);
+             T3j = LD(&(x[WS(vs, 6) + WS(rs, 7)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3k = LD(&(x[WS(vs, 6) + WS(rs, 3)]), ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+             T3l = VSUB(T3j, T3k);
+             T3D = VADD(T3j, T3k);
+            }
+            T3m = VMUL(LDK(KP707106781), VADD(T3i, T3l));
+            T3H = VBYI(VSUB(T3D, T3C));
+            T3o = VMUL(LDK(KP707106781), VSUB(T3l, T3i));
+            T3E = VADD(T3C, T3D);
+           }
+           ////////////////////////////////////////////////////////
+#if (defined(FFTW_SINGLE) && defined(AMD_OPT_KERNEL_REARRANGE_WRITE_V1)) || (!defined(AMD_OPT_KERNEL_NEW_IMPLEMENTATION) && defined(AMD_OPT_KERNEL_REARRANGE_WRITE_V1))
+           {//0th row, 0th row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            Tb = VADD(T3, Ta);
+            Tg = VBYI(VSUB(Tc, Tf));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[0]), VADD(Tp, Ts), ms, &(x[0]));
+            ST(&(x[WS(vs, 1)]), Ti, ms, &(x[WS(vs, 1)]));
+#else
+            Tw = VADD(Tp, Ts);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[0]), Tx, ms, &(x[0]));
+            STA(&(x[0])+ms, Tw, ms, &(x[WS(vs, 1)]));
+#endif
+            Tb = VSUB(T3, Ta);
+            Tg = VBYI(VADD(Tf, Tc));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(Tu, Tv));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2)]), Tx, ms, &(x[WS(vs, 2)]));            
+            ST(&(x[WS(vs, 3)]), Ti, ms, &(x[WS(vs, 3)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2)]), Tw, ms, &(x[WS(vs, 2)]));
+            STA(&(x[WS(vs, 2)])+ms, Tx, ms, &(x[WS(vs, 3)]));
+#endif
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(Tp, Ts));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4)]), Tw, ms, &(x[WS(vs, 4)]));
+            ST(&(x[WS(vs, 5)]), Tx, ms, &(x[WS(vs, 5)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4)]), Tb, ms, &(x[WS(vs, 4)]));
+            STA(&(x[WS(vs, 4)])+ms, Tx, ms, &(x[WS(vs, 5)]));
+#endif            
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(Tu, Tv));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6)]), Tw, ms, &(x[WS(vs, 6)]));
+            ST(&(x[WS(vs, 7)]), Th, ms, &(x[WS(vs, 7)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6)]), Tx, ms, &(x[WS(vs, 6)]));
+            STA(&(x[WS(vs, 6)])+ms, Th, ms, &(x[WS(vs, 7)]));
+#endif             
+           }
+           {//1st row, 1st row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            Tb = VADD(TA, TH);
+            Tg = VBYI(VSUB(TJ, TM));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[WS(rs, 1)]), VADD(TW, TZ), ms, &(x[WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 1)]), Ti, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#else
+            Tw = VADD(TW, TZ);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[WS(rs, 1)]), Tx, ms, &(x[WS(rs, 1)]));
+            STA(&(x[WS(rs, 1)])+ms, Tw, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#endif            
+            Tb = VSUB(TA, TH);
+            Tg = VBYI(VADD(TM, TJ));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(T11, T12));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2) + WS(rs, 1)]), Tx, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 1)]), Ti, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2) + WS(rs, 1)]), Tw, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 2) + WS(rs, 1)])+ms, Tx, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#endif            
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(TW, TZ));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4) + WS(rs, 1)]), Tw, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 5) + WS(rs, 1)]), Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4) + WS(rs, 1)]), Tb, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 4) + WS(rs, 1)])+ms, Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#endif             
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(T11, T12));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6) + WS(rs, 1)]), Tw, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 1)]), Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6) + WS(rs, 1)]), Tx, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 6) + WS(rs, 1)])+ms, Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#endif            
+           }
+           {//2nd row, 2nd row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            
+            Tb = VADD(T17, T1e);
+            Tg = VBYI(VSUB(T1g, T1j));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[WS(rs, 2)]), VADD(T1t, T1w), ms, &(x[0]));
+            ST(&(x[WS(vs, 1) + WS(rs, 2)]), Ti, ms, &(x[WS(vs, 1)]));
+#else
+            Tw = VADD(T1t, T1w);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[WS(rs, 2)]), Tx, ms, &(x[0]));
+            STA(&(x[WS(rs, 2)])+ms, Tw, ms, &(x[WS(vs, 1)]));
+#endif             
+            Tb = VSUB(T17, T1e);
+            Tg = VBYI(VADD(T1j, T1g));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(T1y, T1z));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2) + WS(rs, 2)]), Tx, ms, &(x[WS(vs, 2)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 2)]), Ti, ms, &(x[WS(vs, 3)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2) + WS(rs, 2)]), Tw, ms, &(x[WS(vs, 2)]));
+            STA(&(x[WS(vs, 2) + WS(rs, 2)])+ms, Tx, ms, &(x[WS(vs, 3)]));
+#endif              
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(T1t, T1w));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4) + WS(rs, 2)]), Tw, ms, &(x[WS(vs, 4)]));
+            ST(&(x[WS(vs, 5) + WS(rs, 2)]), Tx, ms, &(x[WS(vs, 5)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4) + WS(rs, 2)]), Tb, ms, &(x[WS(vs, 4)]));
+            STA(&(x[WS(vs, 4) + WS(rs, 2)])+ms, Tx, ms, &(x[WS(vs, 5)]));
+#endif              
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(T1y, T1z));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6) + WS(rs, 2)]), Tw, ms, &(x[WS(vs, 6)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 2)]), Th, ms, &(x[WS(vs, 7)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6) + WS(rs, 2)]), Tx, ms, &(x[WS(vs, 6)]));
+            STA(&(x[WS(vs, 6) + WS(rs, 2)])+ms, Th, ms, &(x[WS(vs, 7)]));
+#endif             
+           }
+           {//3rd row, 3rd row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            Tb = VADD(T1E, T1L);
+            Tg = VBYI(VSUB(T1N, T1Q));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[WS(rs, 3)]), VADD(T20, T23), ms, &(x[WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 3)]), Ti, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#else
+            Tw = VADD(T20, T23);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[WS(rs, 3)]), Tx, ms, &(x[WS(rs, 1)]));
+            STA(&(x[WS(rs, 3)])+ms, Tw, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#endif             
+            Tb = VSUB(T1E, T1L);
+            Tg = VBYI(VADD(T1Q, T1N));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(T25, T26));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2) + WS(rs, 3)]), Tx, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 3)]), Ti, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2) + WS(rs, 3)]), Tw, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 2) + WS(rs, 3)])+ms, Tx, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#endif              
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(T20, T23));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4) + WS(rs, 3)]), Tw, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 5) + WS(rs, 3)]), Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4) + WS(rs, 3)]), Tb, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 4) + WS(rs, 3)])+ms, Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#endif                
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(T25, T26));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6) + WS(rs, 3)]), Tw, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 3)]), Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6) + WS(rs, 3)]), Tx, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 6) + WS(rs, 3)])+ms, Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#endif              
+           }
+           {//4rth row, 4rth row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            Tb = VADD(T2b, T2i);
+            Tg = VBYI(VSUB(T2k, T2n));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[WS(rs, 4)]), VADD(T2x, T2A), ms, &(x[0]));
+            ST(&(x[WS(vs, 1) + WS(rs, 4)]), Ti, ms, &(x[WS(vs, 1)]));
+#else
+            Tw = VADD(T2x, T2A);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[WS(rs, 4)]), Tx, ms, &(x[0]));
+            STA(&(x[WS(rs, 4)])+ms, Tw, ms, &(x[WS(vs, 1)]));
+#endif             
+            Tb = VSUB(T2b, T2i);
+            Tg = VBYI(VADD(T2n, T2k));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(T2C, T2D));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2) + WS(rs, 4)]), Tx, ms, &(x[WS(vs, 2)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 4)]), Ti, ms, &(x[WS(vs, 3)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2) + WS(rs, 4)]), Tw, ms, &(x[WS(vs, 2)]));
+            STA(&(x[WS(vs, 2) + WS(rs, 4)])+ms, Tx, ms, &(x[WS(vs, 3)]));
+#endif             
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(T2x, T2A));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4) + WS(rs, 4)]), Tw, ms, &(x[WS(vs, 4)]));
+            ST(&(x[WS(vs, 5) + WS(rs, 4)]), Tx, ms, &(x[WS(vs, 5)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4) + WS(rs, 4)]), Tb, ms, &(x[WS(vs, 4)]));
+            STA(&(x[WS(vs, 4) + WS(rs, 4)])+ms, Tx, ms, &(x[WS(vs, 5)]));
+#endif             
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(T2C, T2D));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6) + WS(rs, 4)]), Tw, ms, &(x[WS(vs, 6)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 4)]), Th, ms, &(x[WS(vs, 7)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6) + WS(rs, 4)]), Tx, ms, &(x[WS(vs, 6)]));
+            STA(&(x[WS(vs, 6) + WS(rs, 4)])+ms, Th, ms, &(x[WS(vs, 7)]));
+#endif            
+           }
+           {//5th row, 5th row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            Tb = VADD(T2I, T2P);
+            Tg = VBYI(VSUB(T2R, T2U));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[WS(rs, 5)]), VADD(T34, T37), ms, &(x[WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 5)]), Ti, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#else
+            Tw = VADD(T34, T37);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[WS(rs, 5)]), Tx, ms, &(x[WS(rs, 1)]));
+            STA(&(x[WS(rs, 5)])+ms, Tw, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#endif            
+            Tb = VSUB(T2I, T2P);
+            Tg = VBYI(VADD(T2U, T2R));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(T39, T3a));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2) + WS(rs, 5)]), Tx, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 5)]), Ti, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2) + WS(rs, 5)]), Tw, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 2) + WS(rs, 5)])+ms, Tx, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#endif            
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(T34, T37));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4) + WS(rs, 5)]), Tw, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 5) + WS(rs, 5)]), Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4) + WS(rs, 5)]), Tb, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 4) + WS(rs, 5)])+ms, Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#endif            
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(T39, T3a));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6) + WS(rs, 5)]), Tw, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 5)]), Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6) + WS(rs, 5)]), Tx, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 6) + WS(rs, 5)])+ms, Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#endif             
+           }
+           {//6th row, 6th row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            Tb = VADD(T3f, T3m);
+            Tg = VBYI(VSUB(T3o, T3r));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[WS(rs, 6)]), VADD(T3B, T3E), ms, &(x[0]));
+            ST(&(x[WS(vs, 1) + WS(rs, 6)]), Ti, ms, &(x[WS(vs, 1)]));
+#else
+            Tw = VADD(T3B, T3E);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[WS(rs, 6)]), Tx, ms, &(x[0]));
+            STA(&(x[WS(rs, 6)])+ms, Tw, ms, &(x[WS(vs, 1)]));
+#endif            
+            Tb = VSUB(T3f, T3m);
+            Tg = VBYI(VADD(T3r, T3o));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(T3G, T3H));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2) + WS(rs, 6)]), Tx, ms, &(x[WS(vs, 2)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 6)]), Ti, ms, &(x[WS(vs, 3)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2) + WS(rs, 6)]), Tw, ms, &(x[WS(vs, 2)]));
+            STA(&(x[WS(vs, 2) + WS(rs, 6)])+ms, Tx, ms, &(x[WS(vs, 3)]));
+#endif            
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(T3B, T3E));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4) + WS(rs, 6)]), Tw, ms, &(x[WS(vs, 4)]));
+            ST(&(x[WS(vs, 5) + WS(rs, 6)]), Tx, ms, &(x[WS(vs, 5)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4) + WS(rs, 6)]), Tb, ms, &(x[WS(vs, 4)]));
+            STA(&(x[WS(vs, 4) + WS(rs, 6)])+ms, Tx, ms, &(x[WS(vs, 5)]));
+#endif             
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(T3G, T3H));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6) + WS(rs, 6)]), Tw, ms, &(x[WS(vs, 6)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 6)]), Th, ms, &(x[WS(vs, 7)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6) + WS(rs, 6)]), Tx, ms, &(x[WS(vs, 6)]));
+            STA(&(x[WS(vs, 6) + WS(rs, 6)])+ms, Th, ms, &(x[WS(vs, 7)]));
+#endif             
+           }
+           {//7th row, 7th row + ms
+            V Th, Ti, Tb, Tg, Tx, Tw;
+            Tb = VADD(T3M, T3T);
+            Tg = VBYI(VSUB(T3V, T3Y));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+#if MEM_256 == 0
+            ST(&(x[WS(rs, 7)]), VADD(T48, T4b), ms, &(x[WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 7)]), Ti, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#else
+            Tw = VADD(T48, T4b);
+            Tx = SHUF_CROSS_LANE_1(Tw, Ti);
+            Tw = SHUF_CROSS_LANE_2(Tw, Ti);
+            STA(&(x[WS(rs, 7)]), Tx, ms, &(x[WS(rs, 1)]));
+            STA(&(x[WS(rs, 7)])+ms, Tw, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+#endif            
+            Tb = VSUB(T3M, T3T);
+            Tg = VBYI(VADD(T3Y, T3V));
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(T4d, T4e));
+            Ti = BYTWJ(&(W[TWVL * 4]), VADD(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 2) + WS(rs, 7)]), Tx, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 7)]), Ti, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#else
+            Tw = SHUF_CROSS_LANE_1(Tx, Ti);
+            Tx = SHUF_CROSS_LANE_2(Tx, Ti);
+            STA(&(x[WS(vs, 2) + WS(rs, 7)]), Tw, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 2) + WS(rs, 7)])+ms, Tx, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+#endif             
+            Tw = BYTWJ(&(W[TWVL * 6]), VSUB(T48, T4b));
+            Tx = BYTWJ(&(W[TWVL * 8]), VSUB(Tb, Tg));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 4) + WS(rs, 7)]), Tw, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 5) + WS(rs, 7)]), Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#else
+            Tb = SHUF_CROSS_LANE_1(Tw, Tx);
+            Tx = SHUF_CROSS_LANE_2(Tw, Tx);
+            STA(&(x[WS(vs, 4) + WS(rs, 7)]), Tb, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 4) + WS(rs, 7)])+ms, Tx, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+#endif            
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(T4d, T4e));
+#if MEM_256 == 0            
+            ST(&(x[WS(vs, 6) + WS(rs, 7)]), Tw, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 7) + WS(rs, 7)]), Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#else
+            Tx = SHUF_CROSS_LANE_1(Tw, Th);
+            Th = SHUF_CROSS_LANE_2(Tw, Th);
+            STA(&(x[WS(vs, 6) + WS(rs, 7)]), Tx, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            STA(&(x[WS(vs, 6) + WS(rs, 7)])+ms, Th, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+#endif            
+           }
+#else
+#if (!defined(FFTW_SINGLE) && !defined(AMD_OPT_KERNEL_NEW_IMPLEMENTATION)) || defined(FFTW_SINGLE)
+           ST(&(x[0]), VADD(Tp, Ts), ms, &(x[0]));
+           ST(&(x[WS(rs, 2)]), VADD(T1t, T1w), ms, &(x[0]));
+           ST(&(x[WS(rs, 5)]), VADD(T34, T37), ms, &(x[WS(rs, 1)]));
+           ST(&(x[WS(rs, 7)]), VADD(T48, T4b), ms, &(x[WS(rs, 1)]));
+           ST(&(x[WS(rs, 6)]), VADD(T3B, T3E), ms, &(x[0]));
+           ST(&(x[WS(rs, 4)]), VADD(T2x, T2A), ms, &(x[0]));
+           {
+            V Tt, T4c, T2B, T24;
+            ST(&(x[WS(rs, 3)]), VADD(T20, T23), ms, &(x[WS(rs, 1)]));
+            ST(&(x[WS(rs, 1)]), VADD(TW, TZ), ms, &(x[WS(rs, 1)]));
+            Tt = BYTWJ(&(W[TWVL * 6]), VSUB(Tp, Ts));
+            ST(&(x[WS(vs, 4)]), Tt, ms, &(x[WS(vs, 4)]));
+            T4c = BYTWJ(&(W[TWVL * 6]), VSUB(T48, T4b));
+            ST(&(x[WS(vs, 4) + WS(rs, 7)]), T4c, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            T2B = BYTWJ(&(W[TWVL * 6]), VSUB(T2x, T2A));
+            ST(&(x[WS(vs, 4) + WS(rs, 4)]), T2B, ms, &(x[WS(vs, 4)]));
+            T24 = BYTWJ(&(W[TWVL * 6]), VSUB(T20, T23));
+            ST(&(x[WS(vs, 4) + WS(rs, 3)]), T24, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+           }
+           {
+            V T10, T1x, T3F, T38, T1A, Tw;
+            T10 = BYTWJ(&(W[TWVL * 6]), VSUB(TW, TZ));
+            ST(&(x[WS(vs, 4) + WS(rs, 1)]), T10, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            T1x = BYTWJ(&(W[TWVL * 6]), VSUB(T1t, T1w));
+            ST(&(x[WS(vs, 4) + WS(rs, 2)]), T1x, ms, &(x[WS(vs, 4)]));
+            T3F = BYTWJ(&(W[TWVL * 6]), VSUB(T3B, T3E));
+            ST(&(x[WS(vs, 4) + WS(rs, 6)]), T3F, ms, &(x[WS(vs, 4)]));
+            T38 = BYTWJ(&(W[TWVL * 6]), VSUB(T34, T37));
+            ST(&(x[WS(vs, 4) + WS(rs, 5)]), T38, ms, &(x[WS(vs, 4) + WS(rs, 1)]));
+            T1A = BYTWJ(&(W[TWVL * 10]), VSUB(T1y, T1z));
+            ST(&(x[WS(vs, 6) + WS(rs, 2)]), T1A, ms, &(x[WS(vs, 6)]));
+            Tw = BYTWJ(&(W[TWVL * 10]), VSUB(Tu, Tv));
+            ST(&(x[WS(vs, 6)]), Tw, ms, &(x[WS(vs, 6)]));
+           }
+           {
+            V T2E, T3I, T13, T27, T3b, T4f;
+            T2E = BYTWJ(&(W[TWVL * 10]), VSUB(T2C, T2D));
+            ST(&(x[WS(vs, 6) + WS(rs, 4)]), T2E, ms, &(x[WS(vs, 6)]));
+            T3I = BYTWJ(&(W[TWVL * 10]), VSUB(T3G, T3H));
+            ST(&(x[WS(vs, 6) + WS(rs, 6)]), T3I, ms, &(x[WS(vs, 6)]));
+            T13 = BYTWJ(&(W[TWVL * 10]), VSUB(T11, T12));
+            ST(&(x[WS(vs, 6) + WS(rs, 1)]), T13, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            T27 = BYTWJ(&(W[TWVL * 10]), VSUB(T25, T26));
+            ST(&(x[WS(vs, 6) + WS(rs, 3)]), T27, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            T3b = BYTWJ(&(W[TWVL * 10]), VSUB(T39, T3a));
+            ST(&(x[WS(vs, 6) + WS(rs, 5)]), T3b, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+            T4f = BYTWJ(&(W[TWVL * 10]), VSUB(T4d, T4e));
+            ST(&(x[WS(vs, 6) + WS(rs, 7)]), T4f, ms, &(x[WS(vs, 6) + WS(rs, 1)]));
+           }
+           {
+            V Tx, T1B, T3c, T4g, T3J, T2F;
+            Tx = BYTWJ(&(W[TWVL * 2]), VADD(Tu, Tv));
+            ST(&(x[WS(vs, 2)]), Tx, ms, &(x[WS(vs, 2)]));
+            T1B = BYTWJ(&(W[TWVL * 2]), VADD(T1y, T1z));
+            ST(&(x[WS(vs, 2) + WS(rs, 2)]), T1B, ms, &(x[WS(vs, 2)]));
+            T3c = BYTWJ(&(W[TWVL * 2]), VADD(T39, T3a));
+            ST(&(x[WS(vs, 2) + WS(rs, 5)]), T3c, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            T4g = BYTWJ(&(W[TWVL * 2]), VADD(T4d, T4e));
+            ST(&(x[WS(vs, 2) + WS(rs, 7)]), T4g, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+            T3J = BYTWJ(&(W[TWVL * 2]), VADD(T3G, T3H));
+            ST(&(x[WS(vs, 2) + WS(rs, 6)]), T3J, ms, &(x[WS(vs, 2)]));
+            T2F = BYTWJ(&(W[TWVL * 2]), VADD(T2C, T2D));
+            ST(&(x[WS(vs, 2) + WS(rs, 4)]), T2F, ms, &(x[WS(vs, 2)]));
+           }
+           T28 = BYTWJ(&(W[TWVL * 2]), VADD(T25, T26));
+           ST(&(x[WS(vs, 2) + WS(rs, 3)]), T28, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+           T14 = BYTWJ(&(W[TWVL * 2]), VADD(T11, T12));
+           ST(&(x[WS(vs, 2) + WS(rs, 1)]), T14, ms, &(x[WS(vs, 2) + WS(rs, 1)]));
+           {
+            V Th, Ti, Tb, Tg;
+            Tb = VADD(T3, Ta);
+            Tg = VBYI(VSUB(Tc, Tf));
+            Th = BYTWJ(&(W[TWVL * 12]), VSUB(Tb, Tg));
+            Ti = BYTWJ(&(W[0]), VADD(Tb, Tg));
+            ST(&(x[WS(vs, 7)]), Th, ms, &(x[WS(vs, 7)]));
+            ST(&(x[WS(vs, 1)]), Ti, ms, &(x[WS(vs, 1)]));
+           }
+           {
+            V T40, T41, T3U, T3Z;
+            T3U = VADD(T3M, T3T);
+            T3Z = VBYI(VSUB(T3V, T3Y));
+            T40 = BYTWJ(&(W[TWVL * 12]), VSUB(T3U, T3Z));
+            T41 = BYTWJ(&(W[0]), VADD(T3U, T3Z));
+            ST(&(x[WS(vs, 7) + WS(rs, 7)]), T40, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 7)]), T41, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+           }
+           {
+            V T2p, T2q, T2j, T2o;
+            T2j = VADD(T2b, T2i);
+            T2o = VBYI(VSUB(T2k, T2n));
+            T2p = BYTWJ(&(W[TWVL * 12]), VSUB(T2j, T2o));
+            T2q = BYTWJ(&(W[0]), VADD(T2j, T2o));
+            ST(&(x[WS(vs, 7) + WS(rs, 4)]), T2p, ms, &(x[WS(vs, 7)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 4)]), T2q, ms, &(x[WS(vs, 1)]));
+           }
+           {
+            V T1S, T1T, T1M, T1R;
+            T1M = VADD(T1E, T1L);
+            T1R = VBYI(VSUB(T1N, T1Q));
+            T1S = BYTWJ(&(W[TWVL * 12]), VSUB(T1M, T1R));
+            T1T = BYTWJ(&(W[0]), VADD(T1M, T1R));
+            ST(&(x[WS(vs, 7) + WS(rs, 3)]), T1S, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 3)]), T1T, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+           }
+           {
+            V TO, TP, TI, TN;
+            TI = VADD(TA, TH);
+            TN = VBYI(VSUB(TJ, TM));
+            TO = BYTWJ(&(W[TWVL * 12]), VSUB(TI, TN));
+            TP = BYTWJ(&(W[0]), VADD(TI, TN));
+            ST(&(x[WS(vs, 7) + WS(rs, 1)]), TO, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 1)]), TP, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+           }
+           {
+            V T1l, T1m, T1f, T1k;
+            T1f = VADD(T17, T1e);
+            T1k = VBYI(VSUB(T1g, T1j));
+            T1l = BYTWJ(&(W[TWVL * 12]), VSUB(T1f, T1k));
+            T1m = BYTWJ(&(W[0]), VADD(T1f, T1k));
+            ST(&(x[WS(vs, 7) + WS(rs, 2)]), T1l, ms, &(x[WS(vs, 7)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 2)]), T1m, ms, &(x[WS(vs, 1)]));
+           }
+           {
+            V T3t, T3u, T3n, T3s;
+            T3n = VADD(T3f, T3m);
+            T3s = VBYI(VSUB(T3o, T3r));
+            T3t = BYTWJ(&(W[TWVL * 12]), VSUB(T3n, T3s));
+            T3u = BYTWJ(&(W[0]), VADD(T3n, T3s));
+            ST(&(x[WS(vs, 7) + WS(rs, 6)]), T3t, ms, &(x[WS(vs, 7)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 6)]), T3u, ms, &(x[WS(vs, 1)]));
+           }
+           {
+            V T2W, T2X, T2Q, T2V;
+            T2Q = VADD(T2I, T2P);
+            T2V = VBYI(VSUB(T2R, T2U));
+            T2W = BYTWJ(&(W[TWVL * 12]), VSUB(T2Q, T2V));
+            T2X = BYTWJ(&(W[0]), VADD(T2Q, T2V));
+            ST(&(x[WS(vs, 7) + WS(rs, 5)]), T2W, ms, &(x[WS(vs, 7) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 1) + WS(rs, 5)]), T2X, ms, &(x[WS(vs, 1) + WS(rs, 1)]));
+           }
+           {
+            V T1p, T1q, T1n, T1o;
+            T1n = VSUB(T17, T1e);
+            T1o = VBYI(VADD(T1j, T1g));
+            T1p = BYTWJ(&(W[TWVL * 8]), VSUB(T1n, T1o));
+            T1q = BYTWJ(&(W[TWVL * 4]), VADD(T1n, T1o));
+            ST(&(x[WS(vs, 5) + WS(rs, 2)]), T1p, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 2)]), T1q, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V Tl, Tm, Tj, Tk;
+            Tj = VSUB(T3, Ta);
+            Tk = VBYI(VADD(Tf, Tc));
+            Tl = BYTWJ(&(W[TWVL * 8]), VSUB(Tj, Tk));
+            Tm = BYTWJ(&(W[TWVL * 4]), VADD(Tj, Tk));
+            ST(&(x[WS(vs, 5)]), Tl, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3)]), Tm, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V T2t, T2u, T2r, T2s;
+            T2r = VSUB(T2b, T2i);
+            T2s = VBYI(VADD(T2n, T2k));
+            T2t = BYTWJ(&(W[TWVL * 8]), VSUB(T2r, T2s));
+            T2u = BYTWJ(&(W[TWVL * 4]), VADD(T2r, T2s));
+            ST(&(x[WS(vs, 5) + WS(rs, 4)]), T2t, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 4)]), T2u, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V T3x, T3y, T3v, T3w;
+            T3v = VSUB(T3f, T3m);
+            T3w = VBYI(VADD(T3r, T3o));
+            T3x = BYTWJ(&(W[TWVL * 8]), VSUB(T3v, T3w));
+            T3y = BYTWJ(&(W[TWVL * 4]), VADD(T3v, T3w));
+            ST(&(x[WS(vs, 5) + WS(rs, 6)]), T3x, ms, &(x[WS(vs, 5)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 6)]), T3y, ms, &(x[WS(vs, 3)]));
+           }
+           {
+            V TS, TT, TQ, TR;
+            TQ = VSUB(TA, TH);
+            TR = VBYI(VADD(TM, TJ));
+            TS = BYTWJ(&(W[TWVL * 8]), VSUB(TQ, TR));
+            TT = BYTWJ(&(W[TWVL * 4]), VADD(TQ, TR));
+            ST(&(x[WS(vs, 5) + WS(rs, 1)]), TS, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 1)]), TT, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+           {
+            V T1W, T1X, T1U, T1V;
+            T1U = VSUB(T1E, T1L);
+            T1V = VBYI(VADD(T1Q, T1N));
+            T1W = BYTWJ(&(W[TWVL * 8]), VSUB(T1U, T1V));
+            T1X = BYTWJ(&(W[TWVL * 4]), VADD(T1U, T1V));
+            ST(&(x[WS(vs, 5) + WS(rs, 3)]), T1W, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 3)]), T1X, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+           {
+            V T30, T31, T2Y, T2Z;
+            T2Y = VSUB(T2I, T2P);
+            T2Z = VBYI(VADD(T2U, T2R));
+            T30 = BYTWJ(&(W[TWVL * 8]), VSUB(T2Y, T2Z));
+            T31 = BYTWJ(&(W[TWVL * 4]), VADD(T2Y, T2Z));
+            ST(&(x[WS(vs, 5) + WS(rs, 5)]), T30, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 5)]), T31, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+           {
+            V T44, T45, T42, T43;
+            T42 = VSUB(T3M, T3T);
+            T43 = VBYI(VADD(T3Y, T3V));
+            T44 = BYTWJ(&(W[TWVL * 8]), VSUB(T42, T43));
+            T45 = BYTWJ(&(W[TWVL * 4]), VADD(T42, T43));
+            ST(&(x[WS(vs, 5) + WS(rs, 7)]), T44, ms, &(x[WS(vs, 5) + WS(rs, 1)]));
+            ST(&(x[WS(vs, 3) + WS(rs, 7)]), T45, ms, &(x[WS(vs, 3) + WS(rs, 1)]));
+           }
+#endif           
+#endif                      
+      }
      }
      VLEAVE();
 }
+#endif
 
 static const tw_instr twinstr[] = {
      VTW(0, 1),
diff -up fftw-3.3.9/kernel/cpy2d.c.2~ fftw-3.3.9/kernel/cpy2d.c
--- fftw-3.3.9/kernel/cpy2d.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/kernel/cpy2d.c	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -21,6 +22,10 @@
 /* out of place 2D copy routines */
 #include "kernel/ifftw.h"
 
+#ifdef AMD_OPT_ALL
+#include "immintrin.h"
+#endif
+
 #if defined(__x86_64__) || defined(_M_X64) || defined(_M_AMD64)
 #  ifdef HAVE_XMMINTRIN_H
 #    include <xmmintrin.h>
@@ -33,6 +38,894 @@
 #  define WIDE_TYPE double
 #endif
 
+#if defined(AMD_OPT_ALL) && (!defined(FFTW_LDOUBLE) && !defined(FFTW_QUAD)) //AMD optimized routines
+
+#ifdef FFTW_SINGLE//SINGLE PRECISION CPY2d starts
+#ifdef AMD_OPT_IN_PLACE_1D_CPY2D_STABLE_INTRIN//SIMD optimized function
+void X(cpy2d)(R *I, R *O,
+	      INT n0, INT is0, INT os0,
+	      INT n1, INT is1, INT os1,
+	      INT vl)
+{
+     INT i0, i1, v;
+     switch (vl) {
+	 case 1:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0) {
+			R x0 = I[i0 * is0 + i1 * is1];
+			O[i0 * os0 + i1 * os1] = x0;
+		   }
+	      break;
+	 case 2:
+	      {
+	      __m256 in1, in2, in3, in4;
+	      __m256 out1, out2, out3, out4;
+	      INT t0, t1, t2, t3;
+	      INT n0_rem = n0&0x3, n1_rem = n1&0x3;
+	      INT n0Many = n0>3;
+	      INT n1Many = n1>3;
+	      t0 = (is0==2) & n0Many;
+	      t1 = (os0==2) & n0Many;
+	      t2 = (is1==2) & n1Many;
+	      t3 = (os1==2) & n1Many;
+
+	      switch(t0 | (t1 << 1) | (t2 << 2) | (t3 << 3))
+	      {
+		      case 6://os0=2 and is1=2. Both 256-bit read and 256-bit write possible
+			  n0 = n0 - n0_rem;
+			  n1 = n1 - n1_rem;
+			  for (i1 = 0; i1 < n1; i1+=4)
+			  {
+			      for (i0 = 0; i0 < n0; i0+=4) {
+				  in1 = _mm256_loadu_ps((float const *)&I[i0 * is0 + i1 * is1]);
+				  in2 = _mm256_loadu_ps((float const *)&I[(i0+1) * is0 + i1 * is1]);
+				  in3 = _mm256_loadu_ps((float const *)&I[(i0+2) * is0 + i1 * is1]);
+				  in4 = _mm256_loadu_ps((float const *)&I[(i0+3) * is0 + i1 * is1]);
+
+				  out2 = _mm256_shuffle_ps(in1, in2, 0x44);
+				  out4 = _mm256_shuffle_ps(in3, in4, 0x44);
+				  out1 = _mm256_permute2f128_ps(out2, out4, 0x20);
+				  out3 = _mm256_permute2f128_ps(out2, out4, 0x31);
+				  in1 = _mm256_shuffle_ps(in1, in2, 0xEE);
+				  in3 = _mm256_shuffle_ps(in3, in4, 0xEE);
+				  out2 = _mm256_permute2f128_ps(in1, in3, 0x20);
+				  out4 = _mm256_permute2f128_ps(in1, in3, 0x31);
+
+				  _mm256_storeu_ps((float *)&O[i0 * os0 + i1 * os1], out1);
+				  _mm256_storeu_ps((float *)&O[i0 * os0 + (i1+1) * os1], out2);
+				  _mm256_storeu_ps((float *)&O[i0 * os0 + (i1+2) * os1], out3);
+				  _mm256_storeu_ps((float *)&O[i0 * os0 + (i1+3) * os1], out4);
+			      }
+			      for (; i0 < (n0+n0_rem); i0++) {
+				  R x0 = I[i0 * is0 + i1 * is1];
+				  R x1 = I[i0 * is0 + i1 * is1 + 1];
+				  R x2 = I[i0 * is0 + (i1+1) * is1];
+				  R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+				  R x4 = I[i0 * is0 + (i1+2) * is1];
+				  R x5 = I[i0 * is0 + (i1+2) * is1 + 1];
+				  R x6 = I[i0 * is0 + (i1+3) * is1];
+				  R x7 = I[i0 * is0 + (i1+3) * is1 + 1];
+				  O[i0 * os0 + i1 * os1] = x0;
+				  O[i0 * os0 + i1 * os1 + 1] = x1;
+				  O[i0 * os0 + (i1+1) * os1] = x2;
+				  O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+				  O[i0 * os0 + (i1+2) * os1] = x4;
+				  O[i0 * os0 + (i1+2) * os1 + 1] = x5;
+				  O[i0 * os0 + (i1+3) * os1] = x6;
+				  O[i0 * os0 + (i1+3) * os1 + 1] = x7;
+			      }
+			  }
+			  n0 += n0_rem;
+			  for (; i1 < (n1+n1_rem); ++i1) {
+			      for (i0 = 0; i0 < n0; ++i0) {
+				  R x0 = I[i0 * is0 + i1 * is1];
+				  R x1 = I[i0 * is0 + i1 * is1 + 1];
+				  O[i0 * os0 + i1 * os1] = x0;
+				  O[i0 * os0 + i1 * os1 + 1] = x1;
+			      }
+			  }
+			  break;
+
+		      case 9://is0=2 and os1=2. Both 256-bit read and 256-bit write possible
+			  n0 = n0 - n0_rem;
+			  n1 = n1 - n1_rem;
+			  for (i1 = 0; i1 < n1; i1+=4)
+			  {
+			      for (i0 = 0; i0 < n0; i0+=4) {
+				  in1 = _mm256_loadu_ps((float const *)&I[i0 * is0 + i1 * is1]);
+				  in2 = _mm256_loadu_ps((float const *)&I[i0 * is0 + (i1+1) * is1]);
+				  in3 = _mm256_loadu_ps((float const *)&I[i0 * is0 + (i1+2) * is1]);
+				  in4 = _mm256_loadu_ps((float const *)&I[i0 * is0 + (i1+3) * is1]);
+
+				  out2 = _mm256_shuffle_ps(in1, in2, 0x44);
+				  out4 = _mm256_shuffle_ps(in3, in4, 0x44);
+				  out1 = _mm256_permute2f128_ps(out2, out4, 0x20);
+				  out3 = _mm256_permute2f128_ps(out2, out4, 0x31);
+				  in1 = _mm256_shuffle_ps(in1, in2, 0xEE);
+				  in3 = _mm256_shuffle_ps(in3, in4, 0xEE);
+				  out2 = _mm256_permute2f128_ps(in1, in3, 0x20);
+				  out4 = _mm256_permute2f128_ps(in1, in3, 0x31);
+
+				  _mm256_storeu_ps((float *)&O[i0 * os0 + i1 * os1], out1);
+				  _mm256_storeu_ps((float *)&O[(i0+1) * os0 + i1 * os1], out2);
+				  _mm256_storeu_ps((float *)&O[(i0+2) * os0 + i1 * os1], out3);
+				  _mm256_storeu_ps((float *)&O[(i0+3) * os0 + i1 * os1], out4);
+			      }
+			      for (; i0 < (n0+n0_rem); i0++) {
+				  R x0 = I[i0 * is0 + i1 * is1];
+				  R x1 = I[i0 * is0 + i1 * is1 + 1];
+				  R x2 = I[i0 * is0 + (i1+1) * is1];
+				  R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+				  R x4 = I[i0 * is0 + (i1+2) * is1];
+				  R x5 = I[i0 * is0 + (i1+2) * is1 + 1];
+				  R x6 = I[i0 * is0 + (i1+3) * is1];
+				  R x7 = I[i0 * is0 + (i1+3) * is1 + 1];
+				  O[i0 * os0 + i1 * os1] = x0;
+				  O[i0 * os0 + i1 * os1 + 1] = x1;
+				  O[i0 * os0 + (i1+1) * os1] = x2;
+				  O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+				  O[i0 * os0 + (i1+2) * os1] = x4;
+				  O[i0 * os0 + (i1+2) * os1 + 1] = x5;
+				  O[i0 * os0 + (i1+3) * os1] = x6;
+				  O[i0 * os0 + (i1+3) * os1 + 1] = x7;
+			      }
+			  }
+			  n0 += n0_rem;
+			  for (; i1 < (n1+n1_rem); ++i1) {
+			      for (i0 = 0; i0 < n0; ++i0) {
+				  R x0 = I[i0 * is0 + i1 * is1];
+				  R x1 = I[i0 * is0 + i1 * is1 + 1];
+				  O[i0 * os0 + i1 * os1] = x0;
+				  O[i0 * os0 + i1 * os1 + 1] = x1;
+			      }
+			  }
+			  break;
+
+		      case 3://is0=2 and os0=2. Both 256-bit read and 256-bit write possible
+		      case 7://is0=2 and os0=2. Also is1=2. Both 256-bit read and 256-bit write possible
+		      case 11://is0=2 and os0=2. Also os1=2. Both 256-bit read and 256-bit write possible
+		      case 15://is0=2 and os0=2. Also is1=2, os1=2. Both 256-bit read and 256-bit write possible
+			  n0 = n0 - n0_rem;
+			  for (i1 = 0; i1 < n1; ++i1)
+			  {
+			      for (i0 = 0; i0 < n0; i0+=4) {
+				  in1 = _mm256_loadu_ps((float const *)&I[i0 * is0 + i1 * is1]);
+				  _mm256_storeu_ps((float *)&O[i0 * os0 + i1 * os1], in1);
+			      }
+			      for (; i0 < (n0+n0_rem); i0++) {
+				  R x0 = I[i0 * is0 + i1 * is1];
+				  R x1 = I[i0 * is0 + i1 * is1 + 1];
+				  O[i0 * os0 + i1 * os1] = x0;
+				  O[i0 * os0 + i1 * os1 + 1] = x1;
+			      }
+			  }
+			  break;
+
+		      default:
+			  if (1
+				  && (2 * sizeof(R) == sizeof(double))
+				  && (((size_t)I) % sizeof(double) == 0)
+				  && (((size_t)O) % sizeof(double) == 0)
+				  && ((is0 & 1) == 0)
+				  && ((is1 & 1) == 0)
+				  && ((os0 & 1) == 0)
+				  && ((os1 & 1) == 0)) {
+			      /* copy R[2] as double if double is large enough to
+				 hold R[2], and if the input is properly aligned.
+				 This case applies when R==float */
+			      for (i1 = 0; i1 < n1; ++i1)
+				  for (i0 = 0; i0 < n0; ++i0) {
+				      *(double *)&O[i0 * os0 + i1 * os1] =
+					  *(double *)&I[i0 * is0 + i1 * is1];
+				  }
+			  }
+			  else
+			  {
+			      for (i1 = 0; i1 < n1; ++i1)
+				  for (i0 = 0; i0 < n0; ++i0) {
+				      R x0 = I[i0 * is0 + i1 * is1];
+				      R x1 = I[i0 * is0 + i1 * is1 + 1];
+				      O[i0 * os0 + i1 * os1] = x0;
+				      O[i0 * os0 + i1 * os1 + 1] = x1;
+				  }
+			  }
+			  break;
+	      }//switch(t0 | (t1 << 1) | (t2 << 2) | (t3 << 3))
+     	 }             
+	 break;
+                      
+	 default:
+	      for (i1 = 0; i1 < n1; ++i1)
+		      for (i0 = 0; i0 < n0; ++i0)
+			      for (v = 0; v < vl; ++v) {
+				      R x0 = I[i0 * is0 + i1 * is1 + v];
+				      O[i0 * os0 + i1 * os1 + v] = x0;
+			      }
+	      break;
+         }//switch (vl)
+}
+#else //Default CPY2D function
+void X(cpy2d)(R *I, R *O,
+	      INT n0, INT is0, INT os0,
+	      INT n1, INT is1, INT os1,
+	      INT vl)
+{
+     INT i0, i1, v;
+
+     switch (vl) {
+	 case 1:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0) {
+			R x0 = I[i0 * is0 + i1 * is1];
+			O[i0 * os0 + i1 * os1] = x0;
+		   }
+	      break;
+	 case 2:
+	      if (1
+		  && (2 * sizeof(R) == sizeof(WIDE_TYPE))
+		  && (sizeof(WIDE_TYPE) > sizeof(double))
+		  && (((size_t)I) % sizeof(WIDE_TYPE) == 0)
+		  && (((size_t)O) % sizeof(WIDE_TYPE) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as WIDE_TYPE if WIDE_TYPE is large
+		      enough to hold R[2], and if the input is
+		      properly aligned.  This is a win when R==double
+		      and WIDE_TYPE is 128 bits. */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(WIDE_TYPE *)&O[i0 * os0 + i1 * os1] =
+				  *(WIDE_TYPE *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else if (1
+		  && (2 * sizeof(R) == sizeof(double))
+		  && (((size_t)I) % sizeof(double) == 0)
+		  && (((size_t)O) % sizeof(double) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as double if double is large enough to
+		      hold R[2], and if the input is properly aligned.
+		      This case applies when R==float */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(double *)&O[i0 * os0 + i1 * os1] =
+				  *(double *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else {
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     R x0 = I[i0 * is0 + i1 * is1];
+			     R x1 = I[i0 * is0 + i1 * is1 + 1];
+			     O[i0 * os0 + i1 * os1] = x0;
+ 			     O[i0 * os0 + i1 * os1 + 1] = x1;
+			}
+	      }
+	      break;
+	 default:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0)
+			for (v = 0; v < vl; ++v) {
+			     R x0 = I[i0 * is0 + i1 * is1 + v];
+			     O[i0 * os0 + i1 * os1 + v] = x0;
+			}
+	      break;
+     }
+}
+#endif//SINGLE PRECISION CPY2d ends
+
+#else//DOUBLE-PRECISION CPY2D starts
+
+#ifdef AMD_OPT_IN_PLACE_1D_CPY2D_STABLE_C//C optimized function
+void X(cpy2d)(R *I, R *O,
+	      INT n0, INT is0, INT os0,
+	      INT n1, INT is1, INT os1,
+	      INT vl)
+{
+     INT i0, i1, v;
+
+     switch (vl) {
+	 case 1:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0) {
+			R x0 = I[i0 * is0 + i1 * is1];
+			O[i0 * os0 + i1 * os1] = x0;
+		   }
+	      break;
+	 case 2:
+	      if (1
+		  && (2 * sizeof(R) == sizeof(WIDE_TYPE))
+		  && (sizeof(WIDE_TYPE) > sizeof(double))
+		  && (((size_t)I) % sizeof(WIDE_TYPE) == 0)
+		  && (((size_t)O) % sizeof(WIDE_TYPE) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as WIDE_TYPE if WIDE_TYPE is large
+		      enough to hold R[2], and if the input is
+		      properly aligned.  This is a win when R==double
+		      and WIDE_TYPE is 128 bits. */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(WIDE_TYPE *)&O[i0 * os0 + i1 * os1] =
+				  *(WIDE_TYPE *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else if (1
+		  && (2 * sizeof(R) == sizeof(double))
+		  && (((size_t)I) % sizeof(double) == 0)
+		  && (((size_t)O) % sizeof(double) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as double if double is large enough to
+		      hold R[2], and if the input is properly aligned.
+		      This case applies when R==float */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(double *)&O[i0 * os0 + i1 * os1] =
+				  *(double *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else {
+		      INT t0, t1, t2, t3;
+		      INT n0_rem = n0&0x1, n1_rem = n1&0x1;
+		      t0 = (is0==2);
+		      t1 = (os0==2);
+		      t2 = (is1==2);
+		      t3 = (os1==2);
+		      
+		      switch(t0 | (t1 << 1) | (t2 << 2) | (t3 << 3))
+		      {
+			      case 1://only is0 is 2. 256-bit contiguous read possible
+			      n0 = n0 - n0_rem;
+			      for (i1 = 0; i1 < n1; ++i1) {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[i0 * is0 + i1 * is1 + 2];
+					      R x3 = I[i0 * is0 + i1 * is1 + 3];
+
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[(i0+1) * os0 + i1 * os1] = x2;
+					      O[(i0+1) * os0 + i1 * os1 + 1] = x3;
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 2://only os0 is 2. 256-bit contiguous write possible
+			      n0 = n0 - n0_rem;
+			      for (i1 = 0; i1 < n1; ++i1) {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[(i0+1) * is0 + i1 * is1];
+					      R x3 = I[(i0+1) * is0 + i1 * is1 + 1];
+
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[i0 * os0 + i1 * os1 + 2] = x2;
+					      O[i0 * os0 + i1 * os1 + 3] = x3;
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 6://os0=2 and is1=2. Both 256-bit read and 256-bit write possible
+			      n0 = n0 - n0_rem;
+			      n1 = n1 - n1_rem;
+			      for (i1 = 0; i1 < n1; i1+=2)
+			      {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[i0 * is0 + (i1+1) * is1];
+					      R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+					      R x4 = I[(i0+1) * is0 + i1 * is1];
+					      R x5 = I[(i0+1) * is0 + i1 * is1 + 1];
+					      R x6 = I[(i0+1) * is0 + (i1+1) * is1];
+					      R x7 = I[(i0+1) * is0 + (i1+1) * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[(i0+1) * os0 + i1 * os1] = x4;
+					      O[(i0+1) * os0 + i1 * os1 + 1] = x5;
+					      O[i0 * os0 + (i1+1) * os1] = x2;
+					      O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+					      O[(i0+1) * os0 + (i1+1) * os1] = x6;
+					      O[(i0+1) * os0 + (i1+1) * os1 + 1] = x7;
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[i0 * is0 + (i1+1) * is1];
+					      R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[i0 * os0 + (i1+1) * os1] = x2;
+					      O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+				      }
+			      }
+			      n0 += n0_rem;
+			      if (n1_rem)
+			      {
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 8://only os1 is 2. 256-bit contiguous write possible
+			      n1 = n1 - n1_rem;
+			      for (i1 = 0; i1 < n1; i1+=2) {
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[i0 * is0 + (i1+1) * is1];
+					      R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[i0 * os0 + i1 * os1 + 2] = x2;
+					      O[i0 * os0 + i1 * os1 + 3] = x3;
+				      }
+			      }
+			      if (n1_rem)
+			      {
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+			      
+			      case 9://is0=2 and os1=2. Both 256-bit read and 256-bit write possible
+			      n0 = n0 - n0_rem;
+			      n1 = n1 - n1_rem;
+			      for (i1 = 0; i1 < n1; i1+=2)
+			      {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[(i0+1) * is0 + i1 * is1];
+					      R x3 = I[(i0+1) * is0 + i1 * is1 + 1];
+					      R x4 = I[i0 * is0 + (i1+1) * is1];
+					      R x5 = I[i0 * is0 + (i1+1) * is1 + 1];
+					      R x6 = I[(i0+1) * is0 + (i1+1) * is1];
+					      R x7 = I[(i0+1) * is0 + (i1+1) * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[i0 * os0 + (i1+1) * os1] = x4;
+					      O[i0 * os0 + (i1+1) * os1 + 1] = x5;
+					      O[(i0+1) * os0 + i1 * os1] = x2;
+					      O[(i0+1) * os0 + i1 * os1 + 1] = x3;
+					      O[(i0+1) * os0 + (i1+1) * os1] = x6;
+					      O[(i0+1) * os0 + (i1+1) * os1 + 1] = x7;
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[i0 * is0 + (i1+1) * is1];
+					      R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[i0 * os0 + (i1+1) * os1] = x2;
+					      O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+				      }
+			      }
+			      if (n1_rem)
+			      {
+				      n0 += n0_rem;
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 3://is0=2 and os0=2. Both 256-bit read and 256-bit write possible
+			      case 7://is0=2 and os0=2. Also is1=2. Both 256-bit read and 256-bit write possible
+			      case 11://is0=2 and os0=2. Also os1=2. Both 256-bit read and 256-bit write possible
+			      case 15://is0=2 and os0=2. Also is1=2, os1=2. Both 256-bit read and 256-bit write possible
+			      n0 = n0 - n0_rem;
+			      for (i1 = 0; i1 < n1; ++i1)
+			      {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[(i0+1) * is0 + i1 * is1];
+					      R x3 = I[(i0+1) * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[(i0+1) * os0 + i1 * os1] = x2;
+					      O[(i0+1) * os0 + i1 * os1 + 1] = x3;
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+			      
+			      default:
+			      for (i1 = 0; i1 < n1; ++i1)
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      break;
+		      }
+	      }
+	      break;
+	 default:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0)
+			for (v = 0; v < vl; ++v) {
+			     R x0 = I[i0 * is0 + i1 * is1 + v];
+			     O[i0 * os0 + i1 * os1 + v] = x0;
+			}
+	      break;
+     }
+}
+#elif defined(AMD_OPT_IN_PLACE_1D_CPY2D_STABLE_INTRIN)//SIMD optimized function
+void X(cpy2d)(R *I, R *O,
+	      INT n0, INT is0, INT os0,
+	      INT n1, INT is1, INT os1,
+	      INT vl)
+{
+     INT i0, i1, v;
+     
+     switch (vl) {
+	 case 1:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0) {
+			R x0 = I[i0 * is0 + i1 * is1];
+			O[i0 * os0 + i1 * os1] = x0;
+		   }
+	      break;
+	 case 2:
+	      if (1
+		  && (2 * sizeof(R) == sizeof(WIDE_TYPE))
+		  && (sizeof(WIDE_TYPE) > sizeof(double))
+		  && (((size_t)I) % sizeof(WIDE_TYPE) == 0)
+		  && (((size_t)O) % sizeof(WIDE_TYPE) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as WIDE_TYPE if WIDE_TYPE is large
+		      enough to hold R[2], and if the input is
+		      properly aligned.  This is a win when R==double
+		      and WIDE_TYPE is 128 bits. */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(WIDE_TYPE *)&O[i0 * os0 + i1 * os1] =
+				  *(WIDE_TYPE *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else if (1
+		  && (2 * sizeof(R) == sizeof(double))
+		  && (((size_t)I) % sizeof(double) == 0)
+		  && (((size_t)O) % sizeof(double) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as double if double is large enough to
+		      hold R[2], and if the input is properly aligned.
+		      This case applies when R==float */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(double *)&O[i0 * os0 + i1 * os1] =
+				  *(double *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else {
+		      __m256d in1, in2, in3, in4;
+		      __m256d out1, out2;
+		      __m128d in1_128, in2_128;
+		      INT t0, t1, t2, t3;
+		      INT n0_rem = n0&0x1, n1_rem = n1&0x1;
+		      t0 = (is0==2);
+		      t1 = (os0==2);
+		      t2 = (is1==2);
+		      t3 = (os1==2);
+		      
+		      switch(t0 | (t1 << 1) | (t2 << 2) | (t3 << 3))
+		      {
+			      case 1://only is0 is 2. 256-bit contiguous read possible
+			      n0 = n0 - n0_rem;
+			      for (i1 = 0; i1 < n1; ++i1) {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      in1 = _mm256_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+					      in1_128 = _mm256_castpd256_pd128(in1);
+					      in2_128 = _mm256_extractf128_pd(in1, 0x1);
+					      _mm_storeu_pd((double *)&O[i0 * os0 + i1 * os1], in1_128);
+					      _mm_storeu_pd((double *)&O[(i0+1) * os0 + i1 * os1], in2_128);
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 2://only os0 is 2. 256-bit contiguous write possible
+			      n0 = n0 - n0_rem;
+			      for (i1 = 0; i1 < n1; ++i1) {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      in1_128 = _mm_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+					      in2_128 = _mm_loadu_pd((double const *)&I[(i0+1) * is0 + i1 * is1]);
+					      in1 = _mm256_castpd128_pd256(in1_128);
+					      //in2 = _mm256_castpd128_pd256(in2_128);
+					      //out1 = _mm256_permute2f128_pd(in1, in2, 0x20);
+					      out1 = _mm256_insertf128_pd(in1, in2_128, 1);
+					      _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 6://os0=2 and is1=2. Both 256-bit read and 256-bit write possible
+			      n0 = n0 - n0_rem;
+			      n1 = n1 - n1_rem;
+			      for (i1 = 0; i1 < n1; i1+=2)
+			      {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      in1 = _mm256_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+					      in2 = _mm256_loadu_pd((double const *)&I[(i0+1) * is0 + i1 * is1]);
+
+					      //out1 = _mm256_shuffle_pd(in1, in2, 0x33);
+					      //out2 = _mm256_shuffle_pd(in1, in2, 0x11);
+					      out1 = _mm256_permute2f128_pd(in1, in2, 0x20);
+					      out2 = _mm256_permute2f128_pd(in1, in2, 0x31);
+					      _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+					      _mm256_storeu_pd((double *)&O[i0 * os0 + (i1+1) * os1], out2);
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[i0 * is0 + (i1+1) * is1];
+					      R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[i0 * os0 + (i1+1) * os1] = x2;
+					      O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+				      }
+			      }
+			      n0 += n0_rem;
+			      if (n1_rem)
+			      {
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 8://only os1 is 2. 256-bit contiguous write possible
+			      n1 = n1 - n1_rem;
+			      for (i1 = 0; i1 < n1; i1+=2) {
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      in1_128 = _mm_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+					      in2_128 = _mm_loadu_pd((double const *)&I[i0 * is0 + (i1+1) * is1]);
+					      in1 = _mm256_castpd128_pd256(in1_128);
+					      //in2 = _mm256_castpd128_pd256(in2_128);
+					      //out1 = _mm256_permute2f128_pd(in1, in2, 0x20);
+					      out1 = _mm256_insertf128_pd(in1, in2_128, 1);
+					      _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+				      }
+			      }
+			      if (n1_rem)
+			      {
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+			      
+			      case 9://is0=2 and os1=2. Both 256-bit read and 256-bit write possible
+			      n0 = n0 - n0_rem;
+			      n1 = n1 - n1_rem;
+			      for (i1 = 0; i1 < n1; i1+=2)
+			      {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      in1 = _mm256_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+					      in2 = _mm256_loadu_pd((double const *)&I[i0 * is0 + (i1+1) * is1]);
+
+					      //out1 = _mm256_shuffle_pd(in1, in2, 0x33);
+					      //out2 = _mm256_shuffle_pd(in1, in2, 0x11);
+					      out1 = _mm256_permute2f128_pd(in1, in2, 0x20);
+					      out2 = _mm256_permute2f128_pd(in1, in2, 0x31);
+					      _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+					      _mm256_storeu_pd((double *)&O[(i0+1) * os0 + i1 * os1], out2);
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      R x2 = I[i0 * is0 + (i1+1) * is1];
+					      R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+					      O[i0 * os0 + (i1+1) * os1] = x2;
+					      O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+				      }
+			      }
+			      if (n1_rem)
+			      {
+				      n0 += n0_rem;
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+
+			      case 3://is0=2 and os0=2. Both 256-bit read and 256-bit write possible
+			      case 7://is0=2 and os0=2. Also is1=2. Both 256-bit read and 256-bit write possible
+			      case 11://is0=2 and os0=2. Also os1=2. Both 256-bit read and 256-bit write possible
+			      case 15://is0=2 and os0=2. Also is1=2, os1=2. Both 256-bit read and 256-bit write possible
+			      n0 = n0 - n0_rem;
+			      for (i1 = 0; i1 < n1; ++i1)
+			      {
+				      for (i0 = 0; i0 < n0; i0+=2) {
+					      in1 = _mm256_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+					      _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], in1);
+				      }
+				      if (n0_rem)
+				      {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      }
+			      break;
+			      
+			      default:
+			      for (i1 = 0; i1 < n1; ++i1)
+				      for (i0 = 0; i0 < n0; ++i0) {
+					      R x0 = I[i0 * is0 + i1 * is1];
+					      R x1 = I[i0 * is0 + i1 * is1 + 1];
+					      O[i0 * os0 + i1 * os1] = x0;
+					      O[i0 * os0 + i1 * os1 + 1] = x1;
+				      }
+			      break;
+		      }
+	      }
+	      break;
+	 default:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0)
+			for (v = 0; v < vl; ++v) {
+			     R x0 = I[i0 * is0 + i1 * is1 + v];
+			     O[i0 * os0 + i1 * os1 + v] = x0;
+			}
+	      break;
+     }
+}
+#else//Default CPY2D function
+void X(cpy2d)(R *I, R *O,
+	      INT n0, INT is0, INT os0,
+	      INT n1, INT is1, INT os1,
+	      INT vl)
+{
+     INT i0, i1, v;
+
+     switch (vl) {
+	 case 1:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0) {
+			R x0 = I[i0 * is0 + i1 * is1];
+			O[i0 * os0 + i1 * os1] = x0;
+		   }
+	      break;
+	 case 2:
+	      if (1
+		  && (2 * sizeof(R) == sizeof(WIDE_TYPE))
+		  && (sizeof(WIDE_TYPE) > sizeof(double))
+		  && (((size_t)I) % sizeof(WIDE_TYPE) == 0)
+		  && (((size_t)O) % sizeof(WIDE_TYPE) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as WIDE_TYPE if WIDE_TYPE is large
+		      enough to hold R[2], and if the input is
+		      properly aligned.  This is a win when R==double
+		      and WIDE_TYPE is 128 bits. */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(WIDE_TYPE *)&O[i0 * os0 + i1 * os1] =
+				  *(WIDE_TYPE *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else if (1
+		  && (2 * sizeof(R) == sizeof(double))
+		  && (((size_t)I) % sizeof(double) == 0)
+		  && (((size_t)O) % sizeof(double) == 0)
+		  && ((is0 & 1) == 0)
+		  && ((is1 & 1) == 0)
+		  && ((os0 & 1) == 0)
+		  && ((os1 & 1) == 0)) {
+		   /* copy R[2] as double if double is large enough to
+		      hold R[2], and if the input is properly aligned.
+		      This case applies when R==float */
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     *(double *)&O[i0 * os0 + i1 * os1] =
+				  *(double *)&I[i0 * is0 + i1 * is1];
+			}
+	      } else {
+		   for (i1 = 0; i1 < n1; ++i1)
+			for (i0 = 0; i0 < n0; ++i0) {
+			     R x0 = I[i0 * is0 + i1 * is1];
+			     R x1 = I[i0 * is0 + i1 * is1 + 1];
+			     O[i0 * os0 + i1 * os1] = x0;
+ 			     O[i0 * os0 + i1 * os1 + 1] = x1;
+			}
+	      }
+	      break;
+	 default:
+	      for (i1 = 0; i1 < n1; ++i1)
+		   for (i0 = 0; i0 < n0; ++i0)
+			for (v = 0; v < vl; ++v) {
+			     R x0 = I[i0 * is0 + i1 * is1 + v];
+			     O[i0 * os0 + i1 * os1 + v] = x0;
+			}
+	      break;
+     }
+}
+#endif
+#endif//DOUBLE PRECISION CPY2d ends
+
+#else //Default(original) cpy2d routine
+
 void X(cpy2d)(R *I, R *O,
 	      INT n0, INT is0, INT os0,
 	      INT n1, INT is1, INT os1,
@@ -103,6 +996,7 @@ void X(cpy2d)(R *I, R *O,
 	      break;
      }
 }
+#endif
 
 /* like cpy2d, but read input contiguously if possible */
 void X(cpy2d_ci)(R *I, R *O,
diff -up fftw-3.3.9/kernel/cpy2d-pair.c.2~ fftw-3.3.9/kernel/cpy2d-pair.c
--- fftw-3.3.9/kernel/cpy2d-pair.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/kernel/cpy2d-pair.c	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -21,20 +22,522 @@
 /* out of place copy routines for pairs of isomorphic 2D arrays */
 #include "kernel/ifftw.h"
 
+#ifdef AMD_OPT_ALL
+#include "immintrin.h"
+#endif
+
+#if defined(AMD_OPT_ALL) && (!defined(FFTW_LDOUBLE) && !defined(FFTW_QUAD)) //AMD optimized routines
+
+#ifdef FFTW_SINGLE//SINGLE PRECISION
+#if defined(AMD_OPT_IN_PLACE_1D_CPY2D_STABLE_INTRIN)//SIMD optimized function
+void X(cpy2d_pair)(R *I0, R *I1, R *O0, R *O1,
+           INT n0, INT is0, INT os0,
+           INT n1, INT is1, INT os1)
+{
+     INT i0, i1, v;
+     R *I = I0, *O = O0;
+     __m256 in1, in2, in3, in4;
+     __m256 out1, out2, out3, out4;
+     INT t0, t1, t2, t3;
+     INT n0_rem, n1_rem;
+     INT n0Many;
+     INT n1Many;
+     
+     switch (((I1 - I0) == 1) & ((O1 - O0) == 1))
+     {
+        case 1:
+        {
+            n0_rem = n0&0x3, n1_rem = n1&0x3;
+            n0Many = n0>3;
+            n1Many = n1>3;
+            t0 = (is0==2) & n0Many;
+            t1 = (os0==2) & n0Many;
+            t2 = (is1==2) & n1Many;
+            t3 = (os1==2) & n1Many;
+            
+            switch(t0 | (t1 << 1) | (t2 << 2) | (t3 << 3))
+            {
+              case 6://os0=2 and is1=2. Both 256-bit read and 256-bit write possible
+              n0 = n0 - n0_rem;
+              n1 = n1 - n1_rem;
+              for (i1 = 0; i1 < n1; i1+=4)
+              {
+                  for (i0 = 0; i0 < n0; i0+=4) 
+                  {
+                    in1 = _mm256_loadu_ps((float const *)&I[i0 * is0 + i1 * is1]);
+                    in2 = _mm256_loadu_ps((float const *)&I[(i0+1) * is0 + i1 * is1]);
+                    in3 = _mm256_loadu_ps((float const *)&I[(i0+2) * is0 + i1 * is1]);
+                    in4 = _mm256_loadu_ps((float const *)&I[(i0+3) * is0 + i1 * is1]);
+    
+                    out2 = _mm256_shuffle_ps(in1, in2, 0x44);
+                    out4 = _mm256_shuffle_ps(in3, in4, 0x44);
+                    out1 = _mm256_permute2f128_ps(out2, out4, 0x20);
+                    out3 = _mm256_permute2f128_ps(out2, out4, 0x31);
+                    in1 = _mm256_shuffle_ps(in1, in2, 0xEE);
+                    in3 = _mm256_shuffle_ps(in3, in4, 0xEE);
+                    out2 = _mm256_permute2f128_ps(in1, in3, 0x20);
+                    out4 = _mm256_permute2f128_ps(in1, in3, 0x31);
+    
+                    _mm256_storeu_ps((float *)&O[i0 * os0 + i1 * os1], out1);
+                    _mm256_storeu_ps((float *)&O[i0 * os0 + (i1+1) * os1], out2);
+                    _mm256_storeu_ps((float *)&O[i0 * os0 + (i1+2) * os1], out3);
+                    _mm256_storeu_ps((float *)&O[i0 * os0 + (i1+3) * os1], out4);
+                  }
+                  for (; i0 < (n0+n0_rem); i0++) 
+                  {
+                    R x0 = I[i0 * is0 + i1 * is1];
+                    R x1 = I[i0 * is0 + i1 * is1 + 1];
+                    R x2 = I[i0 * is0 + (i1+1) * is1];
+                    R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+                    R x4 = I[i0 * is0 + (i1+2) * is1];
+                    R x5 = I[i0 * is0 + (i1+2) * is1 + 1];
+                    R x6 = I[i0 * is0 + (i1+3) * is1];
+                    R x7 = I[i0 * is0 + (i1+3) * is1 + 1];
+                    O[i0 * os0 + i1 * os1] = x0;
+                    O[i0 * os0 + i1 * os1 + 1] = x1;
+                    O[i0 * os0 + (i1+1) * os1] = x2;
+                    O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+                    O[i0 * os0 + (i1+2) * os1] = x4;
+                    O[i0 * os0 + (i1+2) * os1 + 1] = x5;
+                    O[i0 * os0 + (i1+3) * os1] = x6;
+                    O[i0 * os0 + (i1+3) * os1 + 1] = x7;
+                  }
+              }
+              n0 += n0_rem;
+              for (; i1 < (n1+n1_rem); ++i1) 
+              {
+                  for (i0 = 0; i0 < n0; ++i0) 
+                  {
+                    R x0 = I[i0 * is0 + i1 * is1];
+                    R x1 = I[i0 * is0 + i1 * is1 + 1];
+                    O[i0 * os0 + i1 * os1] = x0;
+                    O[i0 * os0 + i1 * os1 + 1] = x1;
+                  }
+              }
+              break;
+
+              case 9://is0=2 and os1=2. Both 256-bit read and 256-bit write possible
+              n0 = n0 - n0_rem;
+              n1 = n1 - n1_rem;
+              for (i1 = 0; i1 < n1; i1+=4)
+              {
+                  for (i0 = 0; i0 < n0; i0+=4) 
+                  {
+                    in1 = _mm256_loadu_ps((float const *)&I[i0 * is0 + i1 * is1]);
+                    in2 = _mm256_loadu_ps((float const *)&I[i0 * is0 + (i1+1) * is1]);
+                    in3 = _mm256_loadu_ps((float const *)&I[i0 * is0 + (i1+2) * is1]);
+                    in4 = _mm256_loadu_ps((float const *)&I[i0 * is0 + (i1+3) * is1]);
+    
+                    out2 = _mm256_shuffle_ps(in1, in2, 0x44);
+                    out4 = _mm256_shuffle_ps(in3, in4, 0x44);
+                    out1 = _mm256_permute2f128_ps(out2, out4, 0x20);
+                    out3 = _mm256_permute2f128_ps(out2, out4, 0x31);
+                    in1 = _mm256_shuffle_ps(in1, in2, 0xEE);
+                    in3 = _mm256_shuffle_ps(in3, in4, 0xEE);
+                    out2 = _mm256_permute2f128_ps(in1, in3, 0x20);
+                    out4 = _mm256_permute2f128_ps(in1, in3, 0x31);
+    
+                    _mm256_storeu_ps((float *)&O[i0 * os0 + i1 * os1], out1);
+                    _mm256_storeu_ps((float *)&O[(i0+1) * os0 + i1 * os1], out2);
+                    _mm256_storeu_ps((float *)&O[(i0+2) * os0 + i1 * os1], out3);
+                    _mm256_storeu_ps((float *)&O[(i0+3) * os0 + i1 * os1], out4);
+                  }
+                  for (; i0 < (n0+n0_rem); i0++) 
+                  {
+                    R x0 = I[i0 * is0 + i1 * is1];
+                    R x1 = I[i0 * is0 + i1 * is1 + 1];
+                    R x2 = I[i0 * is0 + (i1+1) * is1];
+                    R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+                    R x4 = I[i0 * is0 + (i1+2) * is1];
+                    R x5 = I[i0 * is0 + (i1+2) * is1 + 1];
+                    R x6 = I[i0 * is0 + (i1+3) * is1];
+                    R x7 = I[i0 * is0 + (i1+3) * is1 + 1];
+                    O[i0 * os0 + i1 * os1] = x0;
+                    O[i0 * os0 + i1 * os1 + 1] = x1;
+                    O[i0 * os0 + (i1+1) * os1] = x2;
+                    O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+                    O[i0 * os0 + (i1+2) * os1] = x4;
+                    O[i0 * os0 + (i1+2) * os1 + 1] = x5;
+                    O[i0 * os0 + (i1+3) * os1] = x6;
+                    O[i0 * os0 + (i1+3) * os1 + 1] = x7;
+                  }
+              }
+              n0 += n0_rem;
+              for (; i1 < (n1+n1_rem); ++i1) 
+              {
+                  for (i0 = 0; i0 < n0; ++i0) 
+                  {
+                    R x0 = I[i0 * is0 + i1 * is1];
+                    R x1 = I[i0 * is0 + i1 * is1 + 1];
+                    O[i0 * os0 + i1 * os1] = x0;
+                    O[i0 * os0 + i1 * os1 + 1] = x1;
+                  }
+              }
+              break;
+
+              case 3://is0=2 and os0=2. Both 256-bit read and 256-bit write possible
+              case 7://is0=2 and os0=2. Also is1=2. Both 256-bit read and 256-bit write possible
+              case 11://is0=2 and os0=2. Also os1=2. Both 256-bit read and 256-bit write possible
+              case 15://is0=2 and os0=2. Also is1=2, os1=2. Both 256-bit read and 256-bit write possible
+              n0 = n0 - n0_rem;
+              for (i1 = 0; i1 < n1; ++i1)
+              {
+                  for (i0 = 0; i0 < n0; i0+=4) 
+                  {
+                    in1 = _mm256_loadu_ps((float const *)&I[i0 * is0 + i1 * is1]);
+                    _mm256_storeu_ps((float *)&O[i0 * os0 + i1 * os1], in1);
+                  }
+                  for (; i0 < (n0+n0_rem); i0++) 
+                  {
+                    R x0 = I[i0 * is0 + i1 * is1];
+                    R x1 = I[i0 * is0 + i1 * is1 + 1];
+                    O[i0 * os0 + i1 * os1] = x0;
+                    O[i0 * os0 + i1 * os1 + 1] = x1;
+                  }
+              }
+              break;
+
+              default:
+              for (i1 = 0; i1 < n1; ++i1)
+              {
+                for (i0 = 0; i0 < n0; ++i0)
+                {
+                    R x0 = I0[i0 * is0 + i1 * is1];
+                    R x1 = I1[i0 * is0 + i1 * is1];
+                    O0[i0 * os0 + i1 * os1] = x0;
+                    O1[i0 * os0 + i1 * os1] = x1;
+                }
+              }
+              break;
+            }
+        }
+        break;
+
+        default:
+            for (i1 = 0; i1 < n1; ++i1)
+            {
+                for (i0 = 0; i0 < n0; ++i0)
+                {
+                    R x0 = I0[i0 * is0 + i1 * is1];
+                    R x1 = I1[i0 * is0 + i1 * is1];
+                    O0[i0 * os0 + i1 * os1] = x0;
+                    O1[i0 * os0 + i1 * os1] = x1;
+                }
+            }
+        break;
+     }
+}
+#else//Default C function
 void X(cpy2d_pair)(R *I0, R *I1, R *O0, R *O1,
-		   INT n0, INT is0, INT os0,
-		   INT n1, INT is1, INT os1)
+           INT n0, INT is0, INT os0,
+           INT n1, INT is1, INT os1)
 {
      INT i0, i1;
+     for (i1 = 0; i1 < n1; ++i1)
+      for (i0 = 0; i0 < n0; ++i0) {
+           R x0 = I0[i0 * is0 + i1 * is1];
+           R x1 = I1[i0 * is0 + i1 * is1];
+           O0[i0 * os0 + i1 * os1] = x0;
+           O1[i0 * os0 + i1 * os1] = x1;
+      }
+}
+#endif//ends
 
+#else//DOUBLE-PRECISION
+#if defined(AMD_OPT_IN_PLACE_1D_CPY2D_STABLE_INTRIN)//SIMD optimized function
+void X(cpy2d_pair)(R *I0, R *I1, R *O0, R *O1,
+           INT n0, INT is0, INT os0,
+           INT n1, INT is1, INT os1)
+{
+     INT i0, i1, v;
+     R *I = I0, *O = O0;
+     __m256d in1, in2, in3, in4;
+     __m256d out1, out2;
+     __m128d in1_128, in2_128;
+     INT t0, t1, t2, t3;
+     INT n0_rem, n1_rem;
+     
+     switch (((I1 - I0) == 1) & ((O1 - O0) == 1))
+     {
+        case 1:
+        {
+            n0_rem = n0&0x1, n1_rem = n1&0x1;
+            t0 = (is0==2);
+            t1 = (os0==2);
+            t2 = (is1==2);
+            t3 = (os1==2);
+            switch(t0 | (t1 << 1) | (t2 << 2) | (t3 << 3))
+            {
+            case 1://only is0 is 2. 256-bit contiguous read possible
+                n0 = n0 - n0_rem;
+                for (i1 = 0; i1 < n1; ++i1) 
+                {
+                    for (i0 = 0; i0 < n0; i0+=2) 
+                    {
+                        in1 = _mm256_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+                        in1_128 = _mm256_castpd256_pd128(in1);
+                        in2_128 = _mm256_extractf128_pd(in1, 0x1);
+                        _mm_storeu_pd((double *)&O[i0 * os0 + i1 * os1], in1_128);
+                        _mm_storeu_pd((double *)&O[(i0+1) * os0 + i1 * os1], in2_128);
+                    }
+                    if (n0_rem)
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                    }
+                }
+                break;
+    
+            case 2://only os0 is 2. 256-bit contiguous write possible
+                n0 = n0 - n0_rem;
+                for (i1 = 0; i1 < n1; ++i1) 
+                {
+                    for (i0 = 0; i0 < n0; i0+=2) 
+                    {
+                        in1_128 = _mm_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+                        in2_128 = _mm_loadu_pd((double const *)&I[(i0+1) * is0 + i1 * is1]);
+                        in1 = _mm256_castpd128_pd256(in1_128);
+                        out1 = _mm256_insertf128_pd(in1, in2_128, 1);
+                        _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+                    }
+                    if (n0_rem)
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                    }
+                }
+                break;
+    
+            case 6://os0=2 and is1=2. Both 256-bit read and 256-bit write possible
+                n0 = n0 - n0_rem;
+                n1 = n1 - n1_rem;
+                for (i1 = 0; i1 < n1; i1+=2)
+                {
+                    for (i0 = 0; i0 < n0; i0+=2) 
+                    {
+                        in1 = _mm256_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+                        in2 = _mm256_loadu_pd((double const *)&I[(i0+1) * is0 + i1 * is1]);
+                        out1 = _mm256_permute2f128_pd(in1, in2, 0x20);
+                        out2 = _mm256_permute2f128_pd(in1, in2, 0x31);
+                        _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+                        _mm256_storeu_pd((double *)&O[i0 * os0 + (i1+1) * os1], out2);
+                    }
+                    if (n0_rem)
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        R x2 = I[i0 * is0 + (i1+1) * is1];
+                        R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                        O[i0 * os0 + (i1+1) * os1] = x2;
+                        O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+                    }
+                }
+                n0 += n0_rem;
+                if (n1_rem)
+                {
+                    for (i0 = 0; i0 < n0; ++i0) 
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                    }
+                }
+                break;
+    
+            case 8://only os1 is 2. 256-bit contiguous write possible
+                n1 = n1 - n1_rem;
+                for (i1 = 0; i1 < n1; i1+=2) 
+                {
+                    for (i0 = 0; i0 < n0; ++i0) 
+                    {
+                        in1_128 = _mm_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+                        in2_128 = _mm_loadu_pd((double const *)&I[i0 * is0 + (i1+1) * is1]);
+                        in1 = _mm256_castpd128_pd256(in1_128);
+                        out1 = _mm256_insertf128_pd(in1, in2_128, 1);
+                        _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+                    }
+                }
+                if (n1_rem)
+                {
+                    for (i0 = 0; i0 < n0; ++i0) 
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                    }
+                }
+                break;
+    
+            case 9://is0=2 and os1=2. Both 256-bit read and 256-bit write possible
+                n0 = n0 - n0_rem;
+                n1 = n1 - n1_rem;
+                for (i1 = 0; i1 < n1; i1+=2)
+                {
+                    for (i0 = 0; i0 < n0; i0+=2) 
+                    {
+                        in1 = _mm256_loadu_pd((double const *)&I[i0 * is0 + i1 * is1]);
+                        in2 = _mm256_loadu_pd((double const *)&I[i0 * is0 + (i1+1) * is1]);
+                        out1 = _mm256_permute2f128_pd(in1, in2, 0x20);
+                        out2 = _mm256_permute2f128_pd(in1, in2, 0x31);
+                        _mm256_storeu_pd((double *)&O[i0 * os0 + i1 * os1], out1);
+                        _mm256_storeu_pd((double *)&O[(i0+1) * os0 + i1 * os1], out2);
+                    }
+                    if (n0_rem)
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        R x2 = I[i0 * is0 + (i1+1) * is1];
+                        R x3 = I[i0 * is0 + (i1+1) * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                        O[i0 * os0 + (i1+1) * os1] = x2;
+                        O[i0 * os0 + (i1+1) * os1 + 1] = x3;
+                    }
+                }
+                if (n1_rem)
+                {
+                    n0 += n0_rem;
+                    for (i0 = 0; i0 < n0; ++i0) 
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                    }
+                }
+                break;
+    
+            case 3://is0=2 and os0=2. Both 256-bit read and 256-bit write possible
+            case 7://is0=2 and os0=2. Also is1=2. Both 256-bit read and 256-bit write possible
+            case 11://is0=2 and os0=2. Also os1=2. Both 256-bit read and 256-bit write possible
+            case 15://is0=2 and os0=2. Also is1=2, os1=2. Both 256-bit read and 256-bit write possible
+                t1 = n0&0x7;//remainder of 8 in total n0
+                t2 = (n0-t1);//multiple of 8
+                t3 = (t1)&0x3;//remainder of 4 in remainder of 8
+                t1 = t1 - t3;//presence of 4
+                t3 = t3 - n0_rem;///presence of 2
+                for (i1 = 0; i1 < n1; ++i1)
+                {
+                    for (i0 = 0; i0 < t2; i0+=8) 
+                    {
+                        t0 = i0 * is0 + i1 * is1;
+                        in1 = _mm256_loadu_pd((double const *)&I[t0]);
+                        t0 += (is0 << 1);
+                        in2 = _mm256_loadu_pd((double const *)&I[t0]);
+                        t0 += (is0 << 1);
+                        in3 = _mm256_loadu_pd((double const *)&I[t0]);
+                        t0 += (is0 << 1);
+                        in4 = _mm256_loadu_pd((double const *)&I[t0]);
+                        t0 = i0 * os0 + i1 * os1;
+                        _mm256_storeu_pd((double *)&O[t0], in1);
+                        t0 += (os0 << 1);
+                        _mm256_storeu_pd((double *)&O[t0], in2);
+                        t0 += (os0 << 1);
+                        _mm256_storeu_pd((double *)&O[t0], in3);
+                        t0 += (os0 << 1);
+                        _mm256_storeu_pd((double *)&O[t0], in4);
+                    }
+                    if (t1) 
+                    {
+                        t0 = i0 * is0 + i1 * is1;
+                        in1 = _mm256_loadu_pd((double const *)&I[t0]);
+                        t0 += (is0 << 1);
+                        in2 = _mm256_loadu_pd((double const *)&I[t0]);
+                        t0 = i0 * os0 + i1 * os1;
+                        _mm256_storeu_pd((double *)&O[t0], in1);
+                        t0 += (os0 << 1);
+                        _mm256_storeu_pd((double *)&O[t0], in2);
+                        i0+=4;
+                    }
+                    if (t3) 
+                    {
+                        t0 = i0 * is0 + i1 * is1;
+                        in1 = _mm256_loadu_pd((double const *)&I[t0]);
+                        t0 = i0 * os0 + i1 * os1;
+                        _mm256_storeu_pd((double *)&O[t0], in1);
+                        i0+=2;
+                    }
+                    if (n0_rem)
+                    {
+                        R x0 = I[i0 * is0 + i1 * is1];
+                        R x1 = I[i0 * is0 + i1 * is1 + 1];
+                        O[i0 * os0 + i1 * os1] = x0;
+                        O[i0 * os0 + i1 * os1 + 1] = x1;
+                    }
+                }
+                break;
+    
+            default:
+                for (i1 = 0; i1 < n1; ++i1)
+                {
+                    for (i0 = 0; i0 < n0; ++i0)
+                    {
+                        R x0 = I0[i0 * is0 + i1 * is1];
+                        R x1 = I1[i0 * is0 + i1 * is1];
+                        O0[i0 * os0 + i1 * os1] = x0;
+                        O1[i0 * os0 + i1 * os1] = x1;
+                    }
+                }
+                break;
+            }
+          }
+        break;
+
+        default:
+            for (i1 = 0; i1 < n1; ++i1)
+            {
+                for (i0 = 0; i0 < n0; ++i0)
+                {
+                    R x0 = I0[i0 * is0 + i1 * is1];
+                    R x1 = I1[i0 * is0 + i1 * is1];
+                    O0[i0 * os0 + i1 * os1] = x0;
+                    O1[i0 * os0 + i1 * os1] = x1;
+                }
+            }
+        break;
+     }
+}
+#else//Default C function
+void X(cpy2d_pair)(R *I0, R *I1, R *O0, R *O1,
+           INT n0, INT is0, INT os0,
+           INT n1, INT is1, INT os1)
+{
+     INT i0, i1;
+     for (i1 = 0; i1 < n1; ++i1)
+      for (i0 = 0; i0 < n0; ++i0) {
+           R x0 = I0[i0 * is0 + i1 * is1];
+           R x1 = I1[i0 * is0 + i1 * is1];
+           O0[i0 * os0 + i1 * os1] = x0;
+           O1[i0 * os0 + i1 * os1] = x1;
+      }
+}
+#endif//ends
+#endif//DOUBLE PRECISION ends
+
+#else //Default(original)
+
+void X(cpy2d_pair)(R *I0, R *I1, R *O0, R *O1,
+           INT n0, INT is0, INT os0,
+           INT n1, INT is1, INT os1)
+{
+     INT i0, i1;
      for (i1 = 0; i1 < n1; ++i1)
-	  for (i0 = 0; i0 < n0; ++i0) {
-	       R x0 = I0[i0 * is0 + i1 * is1];
-	       R x1 = I1[i0 * is0 + i1 * is1];
-	       O0[i0 * os0 + i1 * os1] = x0;
-	       O1[i0 * os0 + i1 * os1] = x1;
-	  }
+      for (i0 = 0; i0 < n0; ++i0) {
+           R x0 = I0[i0 * is0 + i1 * is1];
+           R x1 = I1[i0 * is0 + i1 * is1];
+           O0[i0 * os0 + i1 * os1] = x0;
+           O1[i0 * os0 + i1 * os1] = x1;
+      }
 }
+#endif
 
 void X(zero1d_pair)(R *O0, R *O1, INT n0, INT os0)
 {
@@ -47,22 +550,22 @@ void X(zero1d_pair)(R *O0, R *O1, INT n0
 
 /* like cpy2d_pair, but read input contiguously if possible */
 void X(cpy2d_pair_ci)(R *I0, R *I1, R *O0, R *O1,
-		      INT n0, INT is0, INT os0,
-		      INT n1, INT is1, INT os1)
+              INT n0, INT is0, INT os0,
+              INT n1, INT is1, INT os1)
 {
-     if (IABS(is0) < IABS(is1))	/* inner loop is for n0 */
-	  X(cpy2d_pair) (I0, I1, O0, O1, n0, is0, os0, n1, is1, os1);
+     if (IABS(is0) < IABS(is1)) /* inner loop is for n0 */
+      X(cpy2d_pair) (I0, I1, O0, O1, n0, is0, os0, n1, is1, os1);
      else
-	  X(cpy2d_pair) (I0, I1, O0, O1, n1, is1, os1, n0, is0, os0);
+      X(cpy2d_pair) (I0, I1, O0, O1, n1, is1, os1, n0, is0, os0);
 }
 
 /* like cpy2d_pair, but write output contiguously if possible */
 void X(cpy2d_pair_co)(R *I0, R *I1, R *O0, R *O1,
-		      INT n0, INT is0, INT os0,
-		      INT n1, INT is1, INT os1)
+              INT n0, INT is0, INT os0,
+              INT n1, INT is1, INT os1)
 {
-     if (IABS(os0) < IABS(os1))	/* inner loop is for n0 */
-	  X(cpy2d_pair) (I0, I1, O0, O1, n0, is0, os0, n1, is1, os1);
+     if (IABS(os0) < IABS(os1)) /* inner loop is for n0 */
+      X(cpy2d_pair) (I0, I1, O0, O1, n0, is0, os0, n1, is1, os1);
      else
-	  X(cpy2d_pair) (I0, I1, O0, O1, n1, is1, os1, n0, is0, os0);
+      X(cpy2d_pair) (I0, I1, O0, O1, n1, is1, os1, n0, is0, os0);
 }
diff -up fftw-3.3.9/kernel/ifftw.h.2~ fftw-3.3.9/kernel/ifftw.h
--- fftw-3.3.9/kernel/ifftw.h.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/kernel/ifftw.h	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -77,6 +78,48 @@ extern "C"
 # define X(name) CONCAT(fftw_, name)
 #endif
 
+//AMD OPTIMIZATIONS :- start
+//============================================================
+//Main optimization switch that enables or disables all AMD specific optimizations in FFTW
+//AMD_OPT_ALL is defined through config.h using configure script run-time feature arg --enable-amd-opt
+
+#ifdef AMD_OPT_ALL
+
+//--------------------------------
+//disables 128-bit AVX2 versions of kernels and prefers only 256-bit AVX2 kernels support
+#define AMD_OPT_PREFER_256BIT_FPU
+#define AMD_OPT_128BIT_KERNELS_THRESHOLD 1024//Below this SIZE, 128-bit AVX2 kernels allowed
+//--------------------------------
+//CPY2d related optimizations :- enable Either (i)C switch Or (ii)INTRIN switch
+//#define AMD_OPT_IN_PLACE_1D_CPY2D_STABLE_C
+#define AMD_OPT_IN_PLACE_1D_CPY2D_STABLE_INTRIN
+//--------------------------------
+//In-place Transpose related optimization switches :-
+//The below switches are defined through config.h using configure script run-time feature arg --enable-amd-trans
+//AMD_OPT_TRANS is currently tested and supported only for single-threaded, so undefining when MPI or openMP used
+#if defined(HAVE_MPI) || defined(HAVE_OPENMP)
+#undef AMD_OPT_TRANS
+#endif
+#ifdef AMD_OPT_TRANS
+#define AMD_OPT_AUTO_TUNED_TRANS_BLK_SIZE
+#define AMD_OPT_AUTO_TUNED_RASTER_TILED_TRANS_METHOD
+#endif
+//Here they are again provided for manual override to enable them.
+//(i) enables auto-tuned block sized tiling as per CPU's L1D cache size (applicable for both original 
+//    FFTW's transpose and the new auto-tuned cache-efficient raster order tiled transpose
+//#define AMD_OPT_AUTO_TUNED_TRANS_BLK_SIZE
+//(ii) enables new auto-tuned cache-efficient raster order tiled transpose for squared sized matrix
+//     (for this optimization switch, AMD_OPT_AUTO_TUNED_TRANS_BLK_SIZE should also be enabled)
+//#define AMD_OPT_AUTO_TUNED_RASTER_TILED_TRANS_METHOD
+//--------------------------------
+//Kernel new implementations and optimization enable/disable switch by AMD_OPT_KERNEL_256SIMD_PERF
+#define AMD_OPT_KERNEL_256SIMD_PERF
+//--------------------------------
+
+#endif//#ifdef AMD_OPT_ALL
+//============================================================
+//AMD OPTIMIZATIONS :- end
+
 /*
   integral type large enough to contain a stride (what ``int'' should
   have been in the first place.
@@ -119,6 +162,7 @@ extern int X(have_simd_avx512)(void);
 extern int X(have_simd_altivec)(void);
 extern int X(have_simd_vsx)(void);
 extern int X(have_simd_neon)(void);
+extern void X(enquire_L1DcacheSize) (void);
 
 /* forward declarations */
 typedef struct problem_s problem;
@@ -753,7 +797,9 @@ struct planner_s {
      double timelimit; /* elapsed_since(start_time) at which to bail out */
      int timed_out; /* whether most recent search timed out */
      int need_timeout_check;
-
+#ifdef AMD_OPT_PREFER_256BIT_FPU
+     int size;
+#endif
      /* various statistics */
      int nplan;    /* number of plans evaluated */
      double pcost, epcost; /* total pcost of measured/estimated plans */
@@ -957,8 +1003,16 @@ void X(rader_tl_delete)(R *W, rader_tl *
 /*-----------------------------------------------------------------------*/
 /* copy/transposition routines */
 
+#if defined(AMD_OPT_AUTO_TUNED_TRANS_BLK_SIZE) || defined(AMD_OPT_AUTO_TUNED_RASTER_TILED_TRANS_METHOD)
+/* upper bound to the cache size based on latest CPU architectures, for AMD optimized tiled routines */
+#define CACHESIZE 32768
+#define BLK_SIZE 32
+unsigned int L1D_blk_size;// = CACHESIZE;
+unsigned int L1Dsize;// = BLK_SIZE;
+#else
 /* lower bound to the cache size, for tiled routines */
 #define CACHESIZE 8192
+#endif
 
 INT X(compute_tilesz)(INT vl, int how_many_tiles_in_cache);
 
diff -up fftw-3.3.9/kernel/tile2d.c.2~ fftw-3.3.9/kernel/tile2d.c
--- fftw-3.3.9/kernel/tile2d.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/kernel/tile2d.c	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -48,6 +49,11 @@ void X(tile2d)(INT n0l, INT n0u, INT n1l
 
 INT X(compute_tilesz)(INT vl, int how_many_tiles_in_cache)
 {
+#ifndef AMD_OPT_AUTO_TUNED_TRANS_BLK_SIZE
      return X(isqrt)(CACHESIZE / 
 		     (((INT)sizeof(R)) * vl * (INT)how_many_tiles_in_cache));
+#else
+     return X(isqrt)(L1Dsize / 
+		     (((INT)sizeof(R)) * vl * (INT)how_many_tiles_in_cache));
+#endif
 }
diff -up fftw-3.3.9/kernel/transpose.c.2~ fftw-3.3.9/kernel/transpose.c
--- fftw-3.3.9/kernel/transpose.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/kernel/transpose.c	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -169,9 +170,83 @@ void X(transpose_tiled)(R *I, INT n, INT
      transpose_rec(I, n, dotile, &k);
 }
 
+#ifdef AMD_OPT_AUTO_TUNED_RASTER_TILED_TRANS_METHOD
+void trans_autoTuned_tiled_Rorder(R *in, int n, struct transpose_closure *pk)
+{
+    int i, j, k, l;
+    //static arrays of size equal to upper bound of L1D cache size, but it's utilized only as per current CPU's actual L1D size
+#ifdef FFTW_SINGLE
+    float buf1[2*BLK_SIZE*BLK_SIZE];
+    float buf2[2*BLK_SIZE*BLK_SIZE];
+#else
+    double buf1[BLK_SIZE*BLK_SIZE];
+    double buf2[BLK_SIZE*BLK_SIZE];
+#endif
+    int atc_blk_size = L1D_blk_size;//auto-tuned L1D cache block size
+    //(vl >> 1) supports both double-precision and single-precision
+    int num_ele = L1D_blk_size >> (pk->vl >> 1);//number of complex numbers is half for double-precision and one-fourth for single-precision.
+    int m = pk->s0;
+ 
+    //case when n is less than block size is handled by calling original fftw transpose function
+
+    for (i = 0; i < n; i += num_ele)
+    {
+#ifdef FFTW_SINGLE
+	float tmp1, tmp2;
+#else
+        double tmp1, tmp2;
+#endif
+	j = i;
+	k = i<<1;
+	//diagonal block: can be directly copied like ramModel
+     	X(cpy2d_ci)(in + i * m + k,
+		 buf1,
+		 num_ele, pk->vl, pk->vl,
+		 num_ele, pk->s0, atc_blk_size,
+		 pk->vl);
+     	X(cpy2d_co)(buf1,
+		 in + i * m + k,
+		 num_ele, atc_blk_size, pk->vl,
+		 num_ele, pk->vl, pk->s0,
+		 pk->vl);
+	j += num_ele;
+
+	//Next block in the raster scan order for current value of i
+	for (; j < n; j += num_ele)
+	{
+		k = j << 1;
+		l = i << 1;
+		X(cpy2d_ci)(in + i * m + k,
+				buf1,
+				num_ele, pk->vl, pk->vl,
+				num_ele, pk->s0, atc_blk_size,
+				pk->vl);
+		X(cpy2d_ci)(in + j * m + l,
+				buf2,
+				num_ele, pk->vl, pk->vl,
+				num_ele, pk->s0, atc_blk_size,
+				pk->vl);
+		X(cpy2d_co)(buf2,
+				in + i * m + k,
+				num_ele, atc_blk_size, pk->vl,
+				num_ele, pk->vl, pk->s0,
+				pk->vl);
+		X(cpy2d_co)(buf1,
+				in + j * m + l,
+				num_ele, atc_blk_size, pk->vl,
+				num_ele, pk->vl, pk->s0,
+				pk->vl);
+	}
+    }
+}
+#endif
+
 void X(transpose_tiledbuf)(R *I, INT n, INT s0, INT s1, INT vl) 
 {
      struct transpose_closure k;
+#ifdef AMD_OPT_AUTO_TUNED_RASTER_TILED_TRANS_METHOD
+     int blkSize = L1D_blk_size >> (vl >> 1);// (vl >> 1) supports both double and single precision
+#endif
      /* Assume that the the rows of I conflict into the same cache
         lines, and therefore we don't need to reserve cache space for
         the input.  If the rows don't conflict, there is no reason
@@ -186,6 +261,20 @@ void X(transpose_tiledbuf)(R *I, INT n,
      k.buf1 = buf1;
      A(k.tilesz * k.tilesz * vl * sizeof(R) <= sizeof(buf0));
      A(k.tilesz * k.tilesz * vl * sizeof(R) <= sizeof(buf1));
+
+#ifndef AMD_OPT_AUTO_TUNED_RASTER_TILED_TRANS_METHOD
      transpose_rec(I, n, dotile_buf, &k);
+#else
+     //Call original cache-oblivious transpose for cases:-
+     //(i) when matrix size is smaller than block size
+     //(ii) when matrix size is not multiple of block size
+     //(iii) when vector length is 1. It is assumed that real input data will have vl=1 and enter here.
+     if ((n < blkSize) || (n & (blkSize-1)) || (vl == 1))
+     {
+     	transpose_rec(I, n, dotile_buf, &k);
+	return;
+     }
+     trans_autoTuned_tiled_Rorder(I, n, &k);
+#endif
 }
 
diff -up fftw-3.3.9/libbench2/bench.h.2~ fftw-3.3.9/libbench2/bench.h
--- fftw-3.3.9/libbench2/bench.h.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/libbench2/bench.h	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2001 Matteo Frigo
  * Copyright (c) 2001 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -24,7 +25,9 @@
 
 extern double time_min;
 extern int time_repeat;
-
+//#ifdef AMD_WISDOM_MULTI_NAMED_FILE
+//extern bench_problem *p;
+//#endif
 extern void timer_init(double tmin, int repeat);
 
 /* report functions */
diff -up fftw-3.3.9/libbench2/bench-main.c.2~ fftw-3.3.9/libbench2/bench-main.c
--- fftw-3.3.9/libbench2/bench-main.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/libbench2/bench-main.c	2021-05-10 22:57:51.176040043 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2001 Matteo Frigo
  * Copyright (c) 2001 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -190,6 +191,10 @@ int bench_main(int argc, char *argv[])
 	  speed(argv[my_optind++], 0);
      }
 
+//#ifndef AMD_WISDOM_MULTI_NAMED_FILE
      cleanup();
+//#else
+//     cleanup_ex(p);
+//#endif
      return 0;
 }
diff -up fftw-3.3.9/libbench2/bench-user.h.2~ fftw-3.3.9/libbench2/bench-user.h
--- fftw-3.3.9/libbench2/bench-user.h.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/libbench2/bench-user.h	2021-05-10 22:57:51.177040058 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2001 Matteo Frigo
  * Copyright (c) 2001 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -141,6 +142,13 @@ extern int always_pad_real;
 #define LIBBENCH_TIMER 0
 #define USER_TIMER 1
 #define BENCH_NTIMERS 2
+
+//AMD enabled Wisdom related change to write separate wisdom file for each input with a distinct name.
+//#define AMD_WISDOM_MULTI_NAMED_FILE
+//AMD enabled Wisdom related change to read wisdom files without updating them every-time.
+//AMD_WISDOM_MULTI_NAMED_FILE should also be defined for this macro to work.
+//#define AMD_WISDOM_MULTI_NAMED_FILE_READ_ONLY
+
 extern void timer_start(int which_timer);
 extern double timer_stop(int which_timer);
 
@@ -150,6 +158,9 @@ extern void doit(int iter, bench_problem
 extern void done(bench_problem *p);
 extern void main_init(int *argc, char ***argv);
 extern void cleanup(void);
+//#ifdef AMD_WISDOM_MULTI_NAMED_FILE
+//extern void cleanup_ex(bench_problem *p);
+//#endif
 extern void verify(const char *param, int rounds, double tol);
 extern void useropt(const char *arg);
 
diff -up fftw-3.3.9/libbench2/speed.c.2~ fftw-3.3.9/libbench2/speed.c
--- fftw-3.3.9/libbench2/speed.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/libbench2/speed.c	2021-05-10 22:57:51.177040058 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2001 Matteo Frigo
  * Copyright (c) 2001 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -20,14 +21,18 @@
 
 
 #include "libbench2/bench.h"
-
+//#ifdef AMD_WISDOM_MULTI_NAMED_FILE
+//bench_problem *p;
+//#endif
 int no_speed_allocation = 0; /* 1 to not allocate array data in speed() */
 
 void speed(const char *param, int setup_only)
 {
      double *t;
      int iter = 0, k;
+//#ifndef AMD_WISDOM_MULTI_NAMED_FILE
      bench_problem *p;
+//#endif
      double tmin, y;
 
      t = (double *) bench_malloc(time_repeat * sizeof(double));
@@ -88,7 +93,9 @@ void speed(const char *param, int setup_
      report(p, t, time_repeat);
 
      if (!no_speed_allocation)
-	  problem_destroy(p);
+//#ifndef AMD_WISDOM_MULTI_NAMED_FILE
+	problem_destroy(p);
+//#endif
      bench_free(t);
      return;
 }
diff -up fftw-3.3.9/README_AMD.md.2~ fftw-3.3.9/README_AMD.md
--- fftw-3.3.9/README_AMD.md.2~	2021-05-10 22:57:51.177040058 +0200
+++ fftw-3.3.9/README_AMD.md	2021-05-10 22:57:51.177040058 +0200
@@ -0,0 +1,69 @@
+AMD OPTIMIZED FFTW
+------------------
+
+AMD Optimized FFTW is the optimized FFTW implementation targeted for 
+AMD EPYC CPUs. It is developed on top of FFTW (version fftw-3.3.8).
+All known features and functionalities of FFTW are retained and supported
+as it is with this AMD optimized FFTW library.
+
+AMD Optimized FFTW achieves higher performance than the FFTW 3.3.8 due to
+various optimizations involving improved SIMD Kernel functions, improved copy
+functions (cpy2d and cpy2d_pair used in rank-0 transform and buffering plan),
+improved 256-bit kernels selection by Planner and an optional in-place 
+transpose for large problem sizes.
+
+FFTW is a free collection of fast C routines for computing the
+Discrete Fourier Transform and various special cases thereof in one or more
+dimensions. It includes complex, real, symmetric, and parallel transforms, 
+and can handle arbitrary array sizes efficiently.
+
+The doc/ directory contains the manual in texinfo, PDF, info, and HTML
+formats.  Frequently asked questions and answers can be found in the
+doc/FAQ/ directory in ASCII and HTML.
+
+For a quick introduction to calling FFTW, see the "Tutorial" section
+of the manual.
+
+INSTALLATION
+------------
+
+INSTALLATION FROM AMD Optimized FFTW GIT REPOSITORY:
+
+After downloading the latest stable release from the git repository,
+https://github.com/amd/amd-fftw, follow the below steps to configure and
+build it for AMD EPYC processor based on Naples, Rome and future 
+generation architectures.
+
+     ./configure --enable-sse2 --enable-avx --enable-avx2 
+                 --enable-mpi --enable-openmp --enable-shared 
+                 --enable-amd-opt 
+                 --prefix=<your-install-dir>
+     make
+     make install
+
+The configure option "--enable-amd-opt" enables all the improvements and 
+optimizations targeted for AMD EPYC CPUs.
+When enabling configure option "--enable-amd-opt", do not use the 
+configure option "--enable-generic-simd128" or "--enable-generic-simd256".
+
+An optional configure option "--enable-amd-trans" is provided that may benefit
+the performance of transpose operations in case of very large FFT problem sizes.
+This is by default not enabled and provided as an experimental optional switch. 
+
+By default, configure script enables double-precision mode. User should pass
+appropriate configure options to enable the single-precision or quad-precision
+or long-double mode.
+
+CONTACTS
+--------
+
+AMD Optimized FFTW is developed and maintained by AMD.
+You can contact us on the email-id aoclsupport@amd.com.
+You can also raise any issue/suggestion on the git-hub repository at
+https://github.com/amd/amd-fftw/issues
+
+ACKNOWLEDGEMENTS
+----------------
+
+FFTW was developed by Matteo Frigo and Steven G. Johnson. We thank Matteo Frigo
+for his support provided to us.
diff -up fftw-3.3.9/README.md.2~ fftw-3.3.9/README.md
--- fftw-3.3.9/README.md.2~	2021-05-10 22:57:51.177040058 +0200
+++ fftw-3.3.9/README.md	2021-05-10 22:57:51.177040058 +0200
@@ -0,0 +1,69 @@
+AMD OPTIMIZED FFTW
+------------------
+
+AMD Optimized FFTW is the optimized FFTW implementation targeted for 
+AMD EPYC CPUs. It is developed on top of FFTW (version fftw-3.3.8).
+All known features and functionalities of FFTW are retained and supported
+as it is with this AMD optimized FFTW library.
+
+AMD Optimized FFTW achieves higher performance than the FFTW 3.3.8 due to
+various optimizations involving improved SIMD Kernel functions, improved copy
+functions (cpy2d and cpy2d_pair used in rank-0 transform and buffering plan),
+improved 256-bit kernels selection by Planner and an optional in-place 
+transpose for large problem sizes.
+
+FFTW is a free collection of fast C routines for computing the
+Discrete Fourier Transform and various special cases thereof in one or more
+dimensions. It includes complex, real, symmetric, and parallel transforms, 
+and can handle arbitrary array sizes efficiently.
+
+The doc/ directory contains the manual in texinfo, PDF, info, and HTML
+formats.  Frequently asked questions and answers can be found in the
+doc/FAQ/ directory in ASCII and HTML.
+
+For a quick introduction to calling FFTW, see the "Tutorial" section
+of the manual.
+
+INSTALLATION
+------------
+
+INSTALLATION FROM AMD Optimized FFTW GIT REPOSITORY:
+
+After downloading the latest stable release from the git repository,
+https://github.com/amd/amd-fftw, follow the below steps to configure and
+build it for AMD EPYC processor based on Naples, Rome and future 
+generation architectures.
+
+     ./configure --enable-sse2 --enable-avx --enable-avx2 
+                 --enable-mpi --enable-openmp --enable-shared 
+                 --enable-amd-opt 
+                 --prefix=<your-install-dir>
+     make
+     make install
+
+The configure option "--enable-amd-opt" enables all the improvements and 
+optimizations targeted for AMD EPYC CPUs.
+When enabling configure option "--enable-amd-opt", do not use the 
+configure option "--enable-generic-simd128" or "--enable-generic-simd256".
+
+An optional configure option "--enable-amd-trans" is provided that may benefit
+the performance of transpose operations in case of very large FFT problem sizes.
+This is by default not enabled and provided as an experimental optional switch. 
+
+By default, configure script enables double-precision mode. User should pass
+appropriate configure options to enable the single-precision or quad-precision
+or long-double mode.
+
+CONTACTS
+--------
+
+AMD Optimized FFTW is developed and maintained by AMD.
+You can contact us on the email-id aoclsupport@amd.com.
+You can also raise any issue/suggestion on the git-hub repository at
+https://github.com/amd/amd-fftw/issues
+
+ACKNOWLEDGEMENTS
+----------------
+
+FFTW was developed by Matteo Frigo and Steven G. Johnson. We thank Matteo Frigo
+for his support provided to us.
diff -up fftw-3.3.9/README_MIT.2~ fftw-3.3.9/README_MIT
--- fftw-3.3.9/README_MIT.2~	2021-05-10 22:57:51.177040058 +0200
+++ fftw-3.3.9/README_MIT	2021-05-10 22:57:51.177040058 +0200
@@ -0,0 +1,61 @@
+FFTW is a free collection of fast C routines for computing the
+Discrete Fourier Transform in one or more dimensions.  It includes
+complex, real, symmetric, and parallel transforms, and can handle
+arbitrary array sizes efficiently.  FFTW is typically faster than
+other publically-available FFT implementations, and is even
+competitive with vendor-tuned libraries.  (See our web page
+http://fftw.org/ for extensive benchmarks.)  To achieve this
+performance, FFTW uses novel code-generation and runtime
+self-optimization techniques (along with many other tricks).
+
+The doc/ directory contains the manual in texinfo, PDF, info, and HTML
+formats.  Frequently asked questions and answers can be found in the
+doc/FAQ/ directory in ASCII and HTML.
+
+For a quick introduction to calling FFTW, see the "Tutorial" section
+of the manual.
+
+INSTALLATION
+------------
+
+INSTALLATION FROM AN OFFICIAL RELEASE:
+
+Please read chapter 10 "Installation and Customization" of the manual.
+In short:
+
+     ./configure
+     make
+     make install
+
+INSTALLATION FROM THE GIT REPOSITORY:
+
+First, install these programs:
+
+  ocaml, ocamlbuild, autoconf, automake, indent, and libtool,
+
+Then, execute
+
+    sh bootstrap.sh
+    make
+    
+The bootstrap.sh script runs configure directly, but if you need to
+re-run configure, you must pass the --enable-maintainer-mode flag:
+
+    ./configure --enable-maintainer-mode [OTHER CONFIGURE FLAGS]
+
+Alternatively, you can run
+
+    sh mkdist.sh
+
+which will run the entire bootstrapping process and generate
+.tar.gz files similar to those for official releases.
+
+CONTACTS
+--------
+
+FFTW was written by Matteo Frigo and Steven G. Johnson.  You can
+contact them at fftw@fftw.org.  The latest version of FFTW,
+benchmarks, links, and other information can be found at the FFTW home
+page (http://www.fftw.org).  You can also sign up to the fftw-announce
+Google group to receive (infrequent) updates and information about new
+releases.
diff -up fftw-3.3.9/simd-support/avx.c.2~ fftw-3.3.9/simd-support/avx.c
--- fftw-3.3.9/simd-support/avx.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/simd-support/avx.c	2021-05-10 22:57:51.177040058 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -52,3 +53,13 @@ int X(have_simd_avx)(void)
 
 #endif
 
+#ifdef AMD_OPT_AUTO_TUNED_TRANS_BLK_SIZE
+void X(enquire_L1DcacheSize) (void)
+{
+	int eax, ebx, ecx, edx;
+	cpuid_all(0x80000005,0,&eax,&ebx,&ecx,&edx);
+	L1Dsize = ((ecx >> 24) & 0xFF)*1024;
+	L1D_blk_size = X(isqrt)((L1Dsize/(2*8))); //where 2 is no. of tiles and 8 is double data type (may be use (INT)sizeof(R))
+	L1D_blk_size = L1D_blk_size&0xFF0; //block size is chosen that is multiple of 16/8, currently chosen that is multiple of 16.
+}
+#endif
diff -up fftw-3.3.9/simd-support/simd-avx2.h.2~ fftw-3.3.9/simd-support/simd-avx2.h
--- fftw-3.3.9/simd-support/simd-avx2.h.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/simd-support/simd-avx2.h	2021-05-10 22:57:51.177040058 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * Modifications by Romain Dolbeau & Erik Lindahl, derived from simd-avx.h
  * Romain Dolbeau hereby places his modifications in the public domain.
@@ -51,6 +52,18 @@
 
 #include <immintrin.h>
 
+//--------------------------------
+//Under AMD_OPT_KERNEL_256SIMD_PERF switch, enable the required sub-switches:-
+//(i)   Rearranges data and writes 256-bit in original kernel
+//(iii) Use a new implementation of more efficient kernel available for whichever kernel
+// MEM_256 must be 0 in case of FFTW_SINGLE
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+#define MEM_256 0
+#define AMD_OPT_KERNEL_REARRANGE_WRITE_V1
+//#define AMD_OPT_KERNEL_NEW_IMPLEMENTATION
+#endif
+//--------------------------------
+
 typedef DS(__m256d, __m256) V;
 #define VADD SUFF(_mm256_add_p)
 #define VSUB SUFF(_mm256_sub_p)
@@ -59,6 +72,13 @@ typedef DS(__m256d, __m256) V;
 #define VSHUF SUFF(_mm256_shuffle_p)
 #define VPERM1 SUFF(_mm256_permute_p)
 
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+#define VBROADCASTS SUFF(_mm256_broadcast_s)
+#define VBROADCASTP SUFF(_mm256_broadcast_p)
+#define VUNPACKLO SUFF(_mm256_unpacklo_p)
+#define VUNPACKHI SUFF(_mm256_unpackhi_p)
+#endif
+
 #define SHUFVALD(fp0,fp1) \
    (((fp1) << 3) | ((fp0) << 2) | ((fp1) << 1) | ((fp0)))
 #define SHUFVALS(fp0,fp1,fp2,fp3) \
@@ -71,6 +91,16 @@ typedef DS(__m256d, __m256) V;
 #define DVK(var, val) V var = VLIT(val, val)
 #define LDK(x) x
 
+/* FMA support */
+#define VFMA    SUFF(_mm256_fmadd_p)
+#define VFNMS   SUFF(_mm256_fnmadd_p)
+#define VFMS    SUFF(_mm256_fmsub_p)
+#define VFMAI(b, c) SUFF(_mm256_addsub_p)(c, FLIP_RI(b)) /* VADD(c, VBYI(b)) */
+#define VFNMSI(b, c)   VSUB(c, VBYI(b))
+#define VFMACONJ(b,c)  VADD(VCONJ(b),c)
+#define VFMSCONJ(b,c)  VSUB(VCONJ(b),c)
+#define VFNMSCONJ(b,c) SUFF(_mm256_addsub_p)(c, b)  /* VSUB(c, VCONJ(b)) */
+
 static inline V LDA(const R *x, INT ivs, const R *aligned_like)
 {
      (void)aligned_like; /* UNUSED */
@@ -85,6 +115,55 @@ static inline void STA(R *x, V v, INT ov
      SUFF(_mm256_storeu_p)(x, v);
 }
 
+static inline V FLIP_RI(V x)
+{
+     return VPERM1(x, DS(SHUFVALD(1, 0), SHUFVALS(1, 0, 3, 2)));
+}
+
+static inline V VCONJ(V x)
+{
+     /* Produce a SIMD vector[VL] of (0 + -0i). 
+
+        We really want to write this:
+
+           V pmpm = VLIT(-0.0, 0.0);
+
+        but historically some compilers have ignored the distiction
+        between +0 and -0.  It looks like 'gcc-8 -fast-math' treats -0
+        as 0 too.
+      */
+     union uvec {
+          unsigned u[8];
+          V v;
+     };
+     static const union uvec pmpm = {
+#ifdef FFTW_SINGLE
+          { 0x00000000, 0x80000000, 0x00000000, 0x80000000,
+            0x00000000, 0x80000000, 0x00000000, 0x80000000 }
+#else
+          { 0x00000000, 0x00000000, 0x00000000, 0x80000000,
+            0x00000000, 0x00000000, 0x00000000, 0x80000000 }
+#endif
+     };
+     return VXOR(pmpm.v, x);
+}
+
+static inline V VBYI(V x)
+{
+     return FLIP_RI(VCONJ(x));
+}
+
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+static inline V VZMULJB(V tx, V sr)
+{
+     V tr = VUNPACKLO(tx, tx);
+     V ti = VUNPACKHI(tx, tx);
+     tr = VMUL(sr, tr);
+     sr = VBYI(sr);
+     return VFNMS(ti, sr, tr);
+}
+#endif
+
 #if FFTW_SINGLE
 
 #  ifdef _MSC_VER
@@ -186,8 +265,14 @@ static inline void STN2(R *x, V v0, V v1
      *(__m128 *)(x + 3 * ovs) = _mm256_castps256_ps128(yyy3);	\
      *(__m128 *)(x + 7 * ovs) = _mm256_extractf128_ps(yyy3, 1);	\
 }
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+static inline V BYTWJB(const R *t, V sr)
+{
+     return VZMULJB( VBROADCASTP((__m128 const *)t), sr);
+}
+#endif
 
-#else
+#else //DOUBLE PRECISION Starts
 static inline __m128d VMOVAPD_LD(const R *x)
 {
      /* gcc-4.6 miscompiles the combination _mm256_castpd128_pd256(VMOVAPD_LD(x))
@@ -224,6 +309,20 @@ static inline void ST(R *x, V v, INT ovs
 }
 
 
+static inline V SHUF_CROSS_LANE_1(V v1, V v2)
+{
+     V var;
+     var = _mm256_insertf128_pd(v1, _mm256_castpd256_pd128(v2), 1);
+     return var;
+}
+
+static inline V SHUF_CROSS_LANE_2(V v1, V v2)
+{
+     V var;
+     var = _mm256_insertf128_pd(v2, (_mm256_extractf128_pd(v1, 1)), 0);
+     return var;
+}
+
 #define STM2 ST
 #define STN2(x, v0, v1, ovs) /* nop */
 #define STM4(x, v, ovs, aligned_like) /* no-op */
@@ -245,55 +344,13 @@ static inline void ST(R *x, V v, INT ovs
      STA(x + 2 * ovs, _mm256_permute2f128_pd(xxx0, xxx2, 0x31), 0, 0); \
      STA(x + 3 * ovs, _mm256_permute2f128_pd(xxx1, xxx3, 0x31), 0, 0); \
 }
-#endif
-
-static inline V FLIP_RI(V x)
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+static inline V BYTWJB(const R *t, V sr)
 {
-     return VPERM1(x, DS(SHUFVALD(1, 0), SHUFVALS(1, 0, 3, 2)));
+     return VZMULJB( VBROADCASTP((__m128d const *)t), sr);
 }
-
-static inline V VCONJ(V x)
-{
-     /* Produce a SIMD vector[VL] of (0 + -0i). 
-
-        We really want to write this:
-
-           V pmpm = VLIT(-0.0, 0.0);
-
-        but historically some compilers have ignored the distiction
-        between +0 and -0.  It looks like 'gcc-8 -fast-math' treats -0
-        as 0 too.
-      */
-     union uvec {
-          unsigned u[8];
-          V v;
-     };
-     static const union uvec pmpm = {
-#ifdef FFTW_SINGLE
-          { 0x00000000, 0x80000000, 0x00000000, 0x80000000,
-            0x00000000, 0x80000000, 0x00000000, 0x80000000 }
-#else
-          { 0x00000000, 0x00000000, 0x00000000, 0x80000000,
-            0x00000000, 0x00000000, 0x00000000, 0x80000000 }
 #endif
-     };
-     return VXOR(pmpm.v, x);
-}
-
-static inline V VBYI(V x)
-{
-     return FLIP_RI(VCONJ(x));
-}
-
-/* FMA support */
-#define VFMA    SUFF(_mm256_fmadd_p)
-#define VFNMS   SUFF(_mm256_fnmadd_p)
-#define VFMS    SUFF(_mm256_fmsub_p)
-#define VFMAI(b, c) SUFF(_mm256_addsub_p)(c, FLIP_RI(b)) /* VADD(c, VBYI(b)) */
-#define VFNMSI(b, c)   VSUB(c, VBYI(b))
-#define VFMACONJ(b,c)  VADD(VCONJ(b),c)
-#define VFMSCONJ(b,c)  VSUB(VCONJ(b),c)
-#define VFNMSCONJ(b,c) SUFF(_mm256_addsub_p)(c, b)  /* VSUB(c, VCONJ(b)) */
+#endif
 
 static inline V VZMUL(V tx, V sr)
 {
@@ -332,7 +389,6 @@ static inline V VZMULI(V tx, V sr)
      * VMUL(FLIP_RI(sr), VDUPL(tx)));
     */
 }
-
 static inline V VZMULIJ(V tx, V sr)
 {
      /* V tr = VDUPL(tx); */
@@ -360,7 +416,6 @@ static inline V BYTWJ1(const R *t, V sr)
 {
      return VZMULJ(LDA(t, 2, t), sr);
 }
-
 /* twiddle storage #2: twice the space, faster (when in cache) */
 #ifdef FFTW_SINGLE
 # define VTW2(v,x)							\
diff -up fftw-3.3.9/simd-support/simd-avx.h.2~ fftw-3.3.9/simd-support/simd-avx.h
--- fftw-3.3.9/simd-support/simd-avx.h.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/simd-support/simd-avx.h	2021-05-10 22:57:51.177040058 +0200
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2003, 2007-14 Matteo Frigo
  * Copyright (c) 2003, 2007-14 Massachusetts Institute of Technology
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
@@ -47,6 +48,18 @@
 
 #include <immintrin.h>
 
+//--------------------------------
+//Under AMD_OPT_KERNEL_256SIMD_PERF switch, enable the required sub-switches:-
+//(i)   Rearranges data and writes 256-bit in original kernel
+//(iii) Use a new implementation of more efficient kernel available for whichever kernel
+// MEM_256 must be 0 in case of FFTW_SINGLE
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+#define MEM_256 0
+#define AMD_OPT_KERNEL_REARRANGE_WRITE_V1
+//#define AMD_OPT_KERNEL_NEW_IMPLEMENTATION
+#endif
+//--------------------------------
+
 typedef DS(__m256d, __m256) V;
 #define VADD SUFF(_mm256_add_p)
 #define VSUB SUFF(_mm256_sub_p)
@@ -54,6 +67,13 @@ typedef DS(__m256d, __m256) V;
 #define VXOR SUFF(_mm256_xor_p)
 #define VSHUF SUFF(_mm256_shuffle_p)
 
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+#define VBROADCASTS SUFF(_mm256_broadcast_s)
+#define VBROADCASTP SUFF(_mm256_broadcast_p)
+#define VUNPACKLO SUFF(_mm256_unpacklo_p)
+#define VUNPACKHI SUFF(_mm256_unpackhi_p)
+#endif
+
 #define SHUFVALD(fp0,fp1) \
    (((fp1) << 3) | ((fp0) << 2) | ((fp1) << 1) | ((fp0)))
 #define SHUFVALS(fp0,fp1,fp2,fp3) \
@@ -66,6 +86,27 @@ typedef DS(__m256d, __m256) V;
 #define DVK(var, val) V var = VLIT(val, val)
 #define LDK(x) x
 
+/* FMA support */
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+#define VFMA 	SUFF(_mm256_fmadd_p)
+#define VFNMS 	SUFF(_mm256_fnmadd_p)
+#define VFMS 	SUFF(_mm256_fmsub_p)
+#define VFMAI(b, c) VADD(c, VBYI(b))
+#define VFNMSI(b, c) VSUB(c, VBYI(b))
+#define VFMACONJ(b,c)  VADD(VCONJ(b),c)
+#define VFMSCONJ(b,c)  VSUB(VCONJ(b),c)
+#define VFNMSCONJ(b,c) VSUB(c, VCONJ(b))
+#else
+#define VFMA(a, b, c) VADD(c, VMUL(a, b))
+#define VFNMS(a, b, c) VSUB(c, VMUL(a, b))
+#define VFMS(a, b, c) VSUB(VMUL(a, b), c)
+#define VFMAI(b, c) VADD(c, VBYI(b))
+#define VFNMSI(b, c) VSUB(c, VBYI(b))
+#define VFMACONJ(b,c)  VADD(VCONJ(b),c)
+#define VFMSCONJ(b,c)  VSUB(VCONJ(b),c)
+#define VFNMSCONJ(b,c) VSUB(c, VCONJ(b))
+#endif
+
 static inline V LDA(const R *x, INT ivs, const R *aligned_like)
 {
      (void)aligned_like; /* UNUSED */
@@ -80,6 +121,57 @@ static inline void STA(R *x, V v, INT ov
      SUFF(_mm256_storeu_p)(x, v);
 }
 
+static inline V FLIP_RI(V x)
+{
+     return VSHUF(x, x,
+		  DS(SHUFVALD(1, 0), 
+		     SHUFVALS(1, 0, 3, 2)));
+}
+
+static inline V VCONJ(V x)
+{
+     /* Produce a SIMD vector[VL] of (0 + -0i). 
+
+        We really want to write this:
+
+           V pmpm = VLIT(-0.0, 0.0);
+
+        but historically some compilers have ignored the distiction
+        between +0 and -0.  It looks like 'gcc-8 -fast-math' treats -0
+        as 0 too.
+      */
+     union uvec {
+          unsigned u[8];
+          V v;
+     };
+     static const union uvec pmpm = {
+#ifdef FFTW_SINGLE
+          { 0x00000000, 0x80000000, 0x00000000, 0x80000000,
+            0x00000000, 0x80000000, 0x00000000, 0x80000000 }
+#else
+          { 0x00000000, 0x00000000, 0x00000000, 0x80000000,
+            0x00000000, 0x00000000, 0x00000000, 0x80000000 }
+#endif
+     };
+     return VXOR(pmpm.v, x);
+}
+
+static inline V VBYI(V x)
+{
+     return FLIP_RI(VCONJ(x));
+}
+
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+static inline V VZMULJB(V tx, V sr)
+{
+     V tr = VUNPACKLO(tx, tx);
+     V ti = VUNPACKHI(tx, tx);
+     tr = VMUL(sr, tr);
+     sr = VBYI(sr);
+     return VFNMS(ti, sr, tr);
+}
+#endif
+
 #if FFTW_SINGLE
 
 #  ifdef _MSC_VER
@@ -182,8 +274,14 @@ static inline void STN2(R *x, V v0, V v1
      *(__m128 *)(x + 3 * ovs) = _mm256_castps256_ps128(yyy3);	\
      *(__m128 *)(x + 7 * ovs) = _mm256_extractf128_ps(yyy3, 1);	\
 }
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+static inline V BYTWJB(const R *t, V sr)
+{
+     return VZMULJB( VBROADCASTP((__m128 const *)t), sr);
+}
+#endif
 
-#else
+#else //DOUBLE PRECISION STARTS
 static inline __m128d VMOVAPD_LD(const R *x)
 {
      /* gcc-4.6 miscompiles the combination _mm256_castpd128_pd256(VMOVAPD_LD(x))
@@ -219,6 +317,19 @@ static inline void ST(R *x, V v, INT ovs
      *(__m128d *)x = _mm256_castpd256_pd128(v);
 }
 
+static inline V SHUF_CROSS_LANE_1(V v1, V v2)
+{
+     V var;
+     var = _mm256_insertf128_pd(v1, _mm256_castpd256_pd128(v2), 1);
+     return var;
+}
+
+static inline V SHUF_CROSS_LANE_2(V v1, V v2)
+{
+     V var;
+     var = _mm256_insertf128_pd(v2, (_mm256_extractf128_pd(v1, 1)), 0);
+     return var;
+}
 
 #define STM2 ST
 #define STN2(x, v0, v1, ovs) /* nop */
@@ -241,57 +352,13 @@ static inline void ST(R *x, V v, INT ovs
      STA(x + 2 * ovs, _mm256_permute2f128_pd(xxx0, xxx2, 0x31), 0, 0); \
      STA(x + 3 * ovs, _mm256_permute2f128_pd(xxx1, xxx3, 0x31), 0, 0); \
 }
-#endif
-
-static inline V FLIP_RI(V x)
+#ifdef AMD_OPT_KERNEL_256SIMD_PERF
+static inline V BYTWJB(const R *t, V sr)
 {
-     return VSHUF(x, x,
-		  DS(SHUFVALD(1, 0), 
-		     SHUFVALS(1, 0, 3, 2)));
+     return VZMULJB( VBROADCASTP((__m128d const *)t), sr);
 }
-
-static inline V VCONJ(V x)
-{
-     /* Produce a SIMD vector[VL] of (0 + -0i). 
-
-        We really want to write this:
-
-           V pmpm = VLIT(-0.0, 0.0);
-
-        but historically some compilers have ignored the distiction
-        between +0 and -0.  It looks like 'gcc-8 -fast-math' treats -0
-        as 0 too.
-      */
-     union uvec {
-          unsigned u[8];
-          V v;
-     };
-     static const union uvec pmpm = {
-#ifdef FFTW_SINGLE
-          { 0x00000000, 0x80000000, 0x00000000, 0x80000000,
-            0x00000000, 0x80000000, 0x00000000, 0x80000000 }
-#else
-          { 0x00000000, 0x00000000, 0x00000000, 0x80000000,
-            0x00000000, 0x00000000, 0x00000000, 0x80000000 }
 #endif
-     };
-     return VXOR(pmpm.v, x);
-}
-
-static inline V VBYI(V x)
-{
-     return FLIP_RI(VCONJ(x));
-}
-
-/* FMA support */
-#define VFMA(a, b, c) VADD(c, VMUL(a, b))
-#define VFNMS(a, b, c) VSUB(c, VMUL(a, b))
-#define VFMS(a, b, c) VSUB(VMUL(a, b), c)
-#define VFMAI(b, c) VADD(c, VBYI(b))
-#define VFNMSI(b, c) VSUB(c, VBYI(b))
-#define VFMACONJ(b,c)  VADD(VCONJ(b),c)
-#define VFMSCONJ(b,c)  VSUB(VCONJ(b),c)
-#define VFNMSCONJ(b,c) VSUB(c, VCONJ(b))
+#endif
 
 static inline V VZMUL(V tx, V sr)
 {
@@ -301,7 +368,6 @@ static inline V VZMUL(V tx, V sr)
      sr = VBYI(sr);
      return VFMA(ti, sr, tr);
 }
-
 static inline V VZMULJ(V tx, V sr)
 {
      V tr = VDUPL(tx);
@@ -346,7 +412,6 @@ static inline V BYTWJ1(const R *t, V sr)
 {
      return VZMULJ(LDA(t, 2, t), sr);
 }
-
 /* twiddle storage #2: twice the space, faster (when in cache) */
 #ifdef FFTW_SINGLE
 # define VTW2(v,x)							\
diff -up fftw-3.3.9/tests/fftw-bench.c.2~ fftw-3.3.9/tests/fftw-bench.c
--- fftw-3.3.9/tests/fftw-bench.c.2~	2020-12-10 13:02:44.000000000 +0100
+++ fftw-3.3.9/tests/fftw-bench.c	2021-05-10 22:57:51.177040058 +0200
@@ -1,3 +1,8 @@
+/*
+ * Copyright (C) 2019, Advanced Micro Devices, Inc. All Rights Reserved.
+ *
+ */
+
 /* See bench.c.  We keep a few common subroutines in this file so
    that they can be re-used in the MPI test program. */
 
@@ -27,7 +32,12 @@ int threads_ok = 1;
 
 FFTW(plan) the_plan = 0;
 
+#ifdef AMD_WISDOM_MULTI_NAMED_FILE
+static char wisdat[32];
+#else
 static const char *wisdat = "wis.dat";
+#endif
+
 unsigned the_flags = 0;
 int paranoid = 0;
 int usewisdom = 0;
@@ -142,7 +152,7 @@ void rdwisdom(void)
      timer_start(USER_TIMER);
      if ((f = fopen(wisdat, "r"))) {
 	  if (!import_wisdom(f))
-	       fprintf(stderr, "bench: ERROR reading wisdom\n");
+	       fprintf(stderr, "bench: ERROR reading wisdom %s\n", wisdat);
 	  else
 	       success = 1;
 	  fclose(f);
@@ -164,6 +174,12 @@ void wrwisdom(void)
 {
      FILE *f;
      double tim;
+#ifdef AMD_WISDOM_MULTI_NAMED_FILE_READ_ONLY
+     /* in order to skip writing wisdom file. for the case when
+      * already generated wisdom file is used and not written after use.
+      */
+     return;
+#endif
      if (!havewisdom) return;
 
      timer_start(USER_TIMER);
@@ -199,6 +215,11 @@ int can_do(bench_problem *p)
 
      if (verbose > 2 && p->pstring)
 	  printf("Planning %s...\n", p->pstring);
+#ifdef AMD_WISDOM_MULTI_NAMED_FILE
+     memset(wisdat, 0x0, 32);
+     strncpy(wisdat, p->pstring, strlen(p->pstring));
+     strcat(wisdat, ".dat");
+#endif
      rdwisdom();
 
      timer_start(USER_TIMER);
@@ -232,6 +253,11 @@ void setup(bench_problem *p)
           FFTW(free(ptr));
      }
 
+#ifdef AMD_WISDOM_MULTI_NAMED_FILE
+     memset(wisdat, 0x0, 32);
+     strncpy(wisdat, p->pstring, strlen(p->pstring));
+     strcat(wisdat, ".dat");
+#endif     
      rdwisdom();
      install_hook();
 
@@ -301,3 +327,26 @@ void cleanup(void)
 
      final_cleanup();
 }
+
+/*void cleanup_ex(bench_problem *p)
+{
+	initial_cleanup();
+	//wisdat already contains wisdom file name string due to strcpy done in can_do/setup
+	wrwisdom();
+
+#ifdef HAVE_SMP
+	FFTW(cleanup_threads)();
+#else
+	FFTW(cleanup)();
+#endif
+
+#    ifdef FFTW_DEBUG_MALLOC
+	{
+		// undocumented memory checker 
+		FFTW_EXTERN void FFTW(malloc_print_minfo)(int v);
+		FFTW(malloc_print_minfo)(verbose);
+	}
+#    endif
+
+	final_cleanup();
+}*/
